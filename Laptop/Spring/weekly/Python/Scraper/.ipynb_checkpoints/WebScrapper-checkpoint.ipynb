{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Using cached https://files.pythonhosted.org/packages/5d/12/a6197eaf97385e96fd8ec56627749a6229a9b3178ad73866a0b1fb377379/Scrapy-1.5.1-py2.py3-none-any.whl\n",
      "Collecting parsel>=1.1 (from scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/fd/1a/9642a5ea68763d5e7c419df0873073e54bb23d0a8897d3c78e146dd6f355/parsel-1.5.0-py2.py3-none-any.whl\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/37/94/40c93ad0cadac0f8cb729e1668823c71532fd4a7361b141aec535acb68e3/w3lib-1.19.0-py2.py3-none-any.whl\n",
      "Collecting Twisted>=13.1.0 (from scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/50/4c315ce5d119f67189d1819629cae7908ca0b0a6c572980df5cc6942bc22/Twisted-18.7.0.tar.bz2\n",
      "Collecting queuelib (from scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cssselect>=0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from scrapy) (1.0.3)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scrapy) (1.11.0)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from scrapy) (4.2.1)\n",
      "Collecting service-identity (from scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/29/fa/995e364220979e577e7ca232440961db0bf996b6edaf586a7d1bd14d81f1/service_identity-17.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: pyOpenSSL in c:\\programdata\\anaconda3\\lib\\site-packages (from scrapy) (18.0.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (4.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (0.7.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (18.0.0)\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (1.9.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (18.1.0)\n",
      "Collecting pyasn1 (from service-identity->scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/a1/7790cc85db38daa874f6a2e6308131b9953feb1367f2ae2d1123bb93a9f5/pyasn1-0.4.4-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules (from service-identity->scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/19/02/fa63f7ba30a0d7b925ca29d034510fc1ffde53264b71b4155022ddf3ab5d/pyasn1_modules-0.2.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cryptography>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyOpenSSL->scrapy) (2.2.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->scrapy) (39.1.0)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=13.1.0->scrapy) (2.6)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (0.24.0)\n",
      "Requirement already satisfied: cffi>=1.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (1.11.5)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy) (2.18)\n",
      "Building wheels for collected packages: Twisted\n",
      "  Running setup.py bdist_wheel for Twisted: started\n",
      "  Running setup.py bdist_wheel for Twisted: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\anuryadav\\AppData\\Local\\pip\\Cache\\wheels\\a9\\85\\24\\fc82998fb686cb31e65a26c027a20120fd1219c9f1e925913a\n",
      "Successfully built Twisted\n",
      "Installing collected packages: w3lib, parsel, Twisted, queuelib, pyasn1, pyasn1-modules, service-identity, scrapy\n",
      "Successfully installed Twisted-18.7.0 parsel-1.5.0 pyasn1-0.4.4 pyasn1-modules-0.2.2 queuelib-1.5.0 scrapy-1.5.1 service-identity-17.0.0 w3lib-1.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'first_scraper', using template directory 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project', created in:\n",
      "    C:\\Users\\anuryadav\\Desktop\\Python\\Untitled Folder\\first_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd first_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject first_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 1.5.1 - no active project\n",
      "\n",
      "Unknown command: crawl\n",
      "\n",
      "Use \"scrapy\" to see available commands\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd first_scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 1.5.1 - no active project\n",
      "\n",
      "Unknown command: crawl\n",
      "\n",
      "Use \"scrapy\" to see available commands\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'second_tutorial', using template directory 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project', created in:\n",
      "    C:\\Users\\anuryadav\\Desktop\\Python\\Scraper\\second_tutorial\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd second_tutorial\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject second_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-13 16:15:35 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: second_tutorial)\n",
      "2018-09-13 16:15:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.15063-SP0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\spiderloader.py\", line 69, in load\n",
      "    return self._spiders[spider_name]\n",
      "KeyError: 'quotes'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 9, in <module>\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 150, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 90, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 157, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 57, in run\n",
      "    self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 170, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 198, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 202, in _create_crawler\n",
      "    spidercls = self.spider_loader.load(spidercls)\n",
      "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\spiderloader.py\", line 71, in load\n",
      "    raise KeyError(\"Spider not found: {}\".format(spider_name))\n",
      "KeyError: 'Spider not found: quotes'\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anuryadav\\Desktop\\Python\\Scraper\\second_tutorial\n"
     ]
    }
   ],
   "source": [
    "cd second_tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'tutorial', using template directory 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project', created in:\n",
      "    C:\\Users\\anuryadav\\Desktop\\Python\\Scraper\\second_tutorial\\tutorial\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd tutorial\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
