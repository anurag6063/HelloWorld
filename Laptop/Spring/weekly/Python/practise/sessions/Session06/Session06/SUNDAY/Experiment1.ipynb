{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint\n",
    "#### To be done in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is to understand how to implement MLP using PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will be using MNIST database. The MNIST database is a dataset of handwritten digits. It has 60,000 training samples, and 10,000 test samples. Each image is represented by 28 x 28 pixels, each containing a value 0 - 255 with its gray scale value.\n",
    "\n",
    "It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch\n",
    "\n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "1. A replacement for NumPy to use the power of GPUs\n",
    "\n",
    "2. a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "For more information refer the following url :\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment will be implementing MLP using Pytorch. We are going to do this step-by-step:\n",
    "\n",
    "    1. Load MNIST dataset, and visualize\n",
    "    2. Define the Neural Network\n",
    "    3. Define loss and optimizer\n",
    "    4. Train the model\n",
    "    5. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:18.553597Z",
     "start_time": "2018-07-27T13:43:18.550425Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:18.574340Z",
     "start_time": "2018-07-27T13:43:18.568654Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:18.596297Z",
     "start_time": "2018-07-27T13:43:18.592788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the MNIST data. First time we may have to download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:18.684225Z",
     "start_time": "2018-07-27T13:43:18.622167Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading the train set file\n",
    "train_dataset = dsets.MNIST(root='../data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "#Loading the test set file\n",
    "test_dataset = dsets.MNIST(root='../data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset using \"Dataloader\" - this dataloader will return batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:18.693465Z",
     "start_time": "2018-07-27T13:43:18.687134Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# loading the test dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. Loading X and Y train values from the loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:18.711812Z",
     "start_time": "2018-07-27T13:43:18.697209Z"
    }
   },
   "outputs": [],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting first 10 training digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:19.149885Z",
     "start_time": "2018-07-27T13:43:18.718916Z"
    }
   },
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Defining the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the network as a Python class. This Python class inherits functions from _nn.module_.\n",
    "\n",
    "There are three convenient functions that are defined in this class:\n",
    "\n",
    "- ### **\\__init__()**:\n",
    "In this function, we shall declare all the layers of our neural network, including the number of neurons, non-linear activations, etc.\n",
    "\n",
    "- ### **forward()**:\n",
    "This is the function that is used to compute forward pass of the network. Here, we shall connect the different layers we had defined in \\__init__, according to the network architecture we want to make. In this case, $x -> fc1 -> relu -> fc2 -> out$.\n",
    "\n",
    "\"forward\" can be called by calling the object of this class directly. For example:\n",
    "\n",
    "```\n",
    "net = Network()\n",
    "out = net(x)\n",
    "```\n",
    "\n",
    "- ### **backward()**:\n",
    "This function is used to compute gradients across the entire network, and is called from the loss function at the end of the network.\n",
    "\n",
    "```\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "We have to write the **__init__()** and **forward()** methods, and PyTorch will automatically generate a **backward()** method for computing the gradients for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:48:23.366462Z",
     "start_time": "2018-07-27T13:48:23.360900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a neural network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:48:25.088814Z",
     "start_time": "2018-07-27T13:48:25.078708Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loss and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use the Cross Entropy Loss function as the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:19.179383Z",
     "start_time": "2018-07-27T13:43:19.174743Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use SGD as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:43:19.188313Z",
     "start_time": "2018-07-27T13:43:19.182647Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:46:55.364083Z",
     "start_time": "2018-07-27T13:43:43.973048Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    net.train(True)\n",
    "    # For each batch of images in train set\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        # Initialize gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (this calls the \"forward\" function within Net)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Find the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Find the gradients of all weights using the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights using the optimizer\n",
    "        # For e.g.: w = w - (delta_w)*lr\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T13:48:37.325709Z",
     "start_time": "2018-07-27T13:48:35.958775Z"
    }
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# For each batch of images in test set\n",
    "for images, labels in test_loader:\n",
    "    net.eval()\n",
    "    # Get the images\n",
    "    images = images.view(-1, 28*28)\n",
    "    \n",
    "    # Find the output by doing a forward pass through the network\n",
    "    outputs = net(images)\n",
    "    \n",
    "    # Find the class of each sample by taking a max across the probabilities of each class\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "Play with number of epochs, batch_size, hidden layer size, non-linearity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
