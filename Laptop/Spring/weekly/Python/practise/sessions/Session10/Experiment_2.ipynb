{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sY8HfxCh_CSO"
   },
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcLZbnv4Ba3a"
   },
   "source": [
    "The objective of this experiment is to understand Siamese Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuObTdeFBiGY"
   },
   "source": [
    "Tons of data area available on the web (wikipedia, Google, Twitter, YouTube) that could be used to train an ML model.\n",
    "One such source is Google Images. You enter a text query and Google Images shows thousands of related images based on the query and text that are present on the web page with the related image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5Gw25muBlKU"
   },
   "source": [
    "In this experiment we would crawl images from Google Images and try to use this as data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHi5xGcCBnA-"
   },
   "source": [
    "1. Your task is to search for face images for 'AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha'\n",
    "\n",
    "2. Refine your search to faces (Google Images -> enter query -> Tools -> Type -> Face). You could also use movies', ads' names as additional query (e.g., \"Aamir 3 idiots\", \"Boman Irani Khosla Ka Ghosla\", \"Katrina Slice ad\" etc.). The results are noisy but they are useful, and moreover, they are avaible in abundance and for free!\n",
    "\n",
    "    a. Example: https://www.google.co.in/search?client=firefox-b-ab&dcr=0&biw=1366&bih=628&tbs=itp%3Aface&tbm=isch&sa=1&ei=5gbIWtCjN4n2vgSCoqzYBw&q=biswa+kalyan+rath\n",
    "\n",
    "3. Then use a browser extensions to download all the results into a directory. In this way you, would get around 300-600 images for each class. Overall, you should collect atleast 10000 images.\n",
    "    \n",
    "    a. Firefox: https://addons.mozilla.org/en-US/firefox/addon/google-images-downloader/\n",
    "    \n",
    "    b. Chrome: https://chrome.google.com/webstore/detail/download-all-images/ifipmflagepipjokmbdecpmjbibjnakm/related?hl=en\n",
    "    \n",
    "4. **Without cleaning** use these images as your training data. Test you results on IMFDB test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbKgGvN8BtWG"
   },
   "source": [
    "#### Run the Notebook on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFg6Jf8VBuTw"
   },
   "source": [
    "#### Authenticating Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDvo9vOa_IwQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6e0131dbeb10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Authentication for your google drive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# The below are wrapper functions used to connect to your drive and this needs to be run once (i.e. once every new session or possibily refreshes for every 24 hours)\n",
    "\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "# Authentication for your google drive\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "\n",
    "# Authentication for the wrapper libraries  or possibily refreshes for every 24 hours)\n",
    "\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8782jAB6Y5vC"
   },
   "outputs": [],
   "source": [
    "!mkdir -p MyDrive\n",
    "!google-drive-ocamlfuse MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhoMm0ENB04C"
   },
   "source": [
    "#### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsn9tCx2eeaE"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/0b4txuf4si3659z/One_shot_Face_recognition.zip?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNCSkMclB-jU"
   },
   "source": [
    "#### Unziping the downloaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqxnpOZkjwMT"
   },
   "outputs": [],
   "source": [
    "!unzip -o One_shot_Face_recognition.zip?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_p4CBA5VCEKW"
   },
   "source": [
    "#### Installing Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riLJW9msg5oy"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6bXbVJpCKBq"
   },
   "source": [
    "#### Changing the Working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAhuoS_ioZ9m"
   },
   "outputs": [],
   "source": [
    "%cd One_shot_Face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aq9clcuMCRVy"
   },
   "source": [
    "#### Importing the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APSGlS_s_CSU"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a87921325eb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsun\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSUN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSUNClass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetFolder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCocoCaptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcifar\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCIFAR100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstl10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\lsun.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PILLOW_VERSION'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         raise ImportError(\"The _imaging extension was built for another \"\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# Importing pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "# Importing config.py file\n",
    "import config as cf\n",
    "from utils import *\n",
    "from light_cnn import LightCNN_9Layers #, LightCNN_29Layers, LightCNN_29Layers_v2\n",
    "#from resnet import resnet18\n",
    "from siamese_data_loader import *\n",
    "from contrastive import *   ### implementation of contrastive loss\n",
    "## Importing python packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oia-EoIHCjo0"
   },
   "source": [
    "#### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "LWG1XpBV_CSi",
    "outputId": "938c8bb5-ff07-4dce-acea-59877c41ab87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 1095\n",
      "5000 1095\n"
     ]
    }
   ],
   "source": [
    "img_root = cf.data_dir+'IMFDB_final/'\n",
    "\n",
    "train_list_file = cf.data_dir+'IMFDB_train_sorted.txt'   #### 5000 images for training\n",
    "val_list_file = cf.data_dirimg_root = cf.data_dir+'IMFDB_final/'\n",
    "\n",
    "train_list_file = cf.data_dir+'IMFDB_train_sorted.txt'   #### 5000 images for training\n",
    "val_list_file = cf.data_dir+'IMFDB_test_sorted.txt'      #### 1095 images for validation\n",
    "\n",
    "\n",
    "train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n",
    "val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n",
    "\n",
    "print(len(train_image_list), len(val_image_list))\n",
    "\n",
    "### Notice a new data loader for siamese networks. This gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n",
    "### see siamese_data_loader.py for details\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n",
    "                                                             resize = True, resize_shape=[128,128]), \n",
    "batch_size=32, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=10, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "\n",
    "#classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']+'IMFDB_test_sorted.txt'      #### 1095 images for validation\n",
    "\n",
    "\n",
    "train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n",
    "val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n",
    "\n",
    "print(len(train_image_list), len(val_image_list))\n",
    "\n",
    "### Notice a new data loader for siamese networks. This gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n",
    "### see siamese_data_loader.py for details\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n",
    "                                                             resize = True, resize_shape=[128,128]), \n",
    "batch_size=32, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=10, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "\n",
    "classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.14.3)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (0.4.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (5.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2Aee8StCokK"
   },
   "source": [
    "#### Command to check whether GPU is enabled or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0_YCzNqeLyB"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvlieheJ_CSy"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Intilizaing the loss value as high value\n",
    "best_loss = 99999999\n",
    "\n",
    "num_classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4NzPR-weLyH"
   },
   "outputs": [],
   "source": [
    "from torchvision import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40tFVFoWeLyM"
   },
   "outputs": [],
   "source": [
    "feature_net = LightCNN_9Layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ort2nuVUeLyS"
   },
   "outputs": [],
   "source": [
    "feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUJKM-V5_CS-"
   },
   "outputs": [],
   "source": [
    "feature_net = LightCNN_9Layers()   ### creates an object of this network architecture\n",
    "feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth')\n",
    "\n",
    "\n",
    "layers_to_remove = ['fc2']\n",
    "for layers_ in layers_to_remove:        \n",
    "    del(feature_net._modules[layers_])\n",
    "    \n",
    "classifier = nn.Sequential(nn.Linear(256, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
    "                           nn.Linear(32, num_classes))\n",
    "\n",
    "feature_net.fc2 = nn.Sequential(nn.Linear(256, 16))\n",
    "feature_net = feature_net.to(device)\n",
    "classifier =  classifier.to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dvh9E5z0_CTG"
   },
   "outputs": [],
   "source": [
    "### Intiliazing the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "siamese_loss = contrastive_loss()   ### Notice a new loss. contrastive.py shows how to compute contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFloT0WAeLyw"
   },
   "outputs": [],
   "source": [
    "criterion = criterion.to(device)\n",
    "siamese_loss = siamese_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOmg3Ypb_CTO"
   },
   "source": [
    "#### Lets train the siamese networks. The objective is images from same class (+ pair, label = 0) should have similar feature and images from different classes (- pair, label = 1) should have different features. Instead of having two physical networks sharing the weights, in implementation we have only one network and first pass image_1 (to get its feature) and then pass image_2 (to get its feature) through the same network. We then compute the contrastive loss on these feature pairs from input image pairs. This saves a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isK--8bS_CTS"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    feature_net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 1\n",
    "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(trainloader):\n",
    "        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n",
    "        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n",
    "        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n",
    "        \n",
    "        loss = siamese_loss(features_1, features_2, targets.float())   ### compute the contrastive loss, computes the similarity between the features.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        #print(1)\n",
    "        \n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f '% (train_loss/(batch_idx+1)))\n",
    "        \n",
    "    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n",
    "        #print(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qj7vLS23C_SS"
   },
   "source": [
    "#### Function to test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMwHtczA_CTc"
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_loss\n",
    "    feature_net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 1\n",
    "    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n",
    "        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #inputs_1, inputs_2, targets = Variable(inputs_1), Variable(inputs_2), Variable(targets)\n",
    "        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n",
    "        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2      \n",
    "        \n",
    "        loss = siamese_loss(features_1, features_2, targets.float())\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f '\n",
    "                         % (test_loss/(batch_idx+1)))\n",
    "        \n",
    "    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    losss = test_loss/len(testloader)\n",
    "    if  losss < best_loss:   ### save model with the best loss so far\n",
    "        print('Saving..') \n",
    "        state = {\n",
    "            'net': feature_net\n",
    "        }\n",
    "        if not os.path.isdir(cf.data_dir+'checkpoint'):\n",
    "            os.mkdir(cf.data_dir+'checkpoint')\n",
    "        torch.save(state, cf.data_dir+'checkpoint/siamese_ckpt.t7')\n",
    "        best_loss = losss\n",
    "    \n",
    "    return test_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3k0q1BOVDD86"
   },
   "source": [
    "#### Creating the files to store train and validation data loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhnUSGoC_CTk"
   },
   "outputs": [],
   "source": [
    "experiment = 'siamese_IMFDB/'\n",
    "train_loss_file = open(cf.data_dir+experiment+\"train_loss.txt\", \"w\")\n",
    "val_loss_file = open(cf.data_dir+experiment+\"val_loss.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKSAHBYR_CT0"
   },
   "outputs": [],
   "source": [
    "feature_net = feature_net.to(device)\n",
    "optimizer = optim.Adam(feature_net.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n",
    "for epoch in range(0, 10):\n",
    "    train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    scheduler.step(test_loss)\n",
    "    print(\"Test Loss: \", test_loss)\n",
    "train_loss_file.close()\n",
    "val_loss_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYMrwcsA_CUG"
   },
   "outputs": [],
   "source": [
    "### After training we load the model that performed the best on validation data (avoid picking overfitted model)\n",
    "### we will use the base pre-trained network for feature extraction only. This feature is used to train an MLP classifier.\n",
    "\n",
    "feature_net = torch.load(cf.data_dir+'checkpoint/siamese_ckpt.t7')['net'].eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48su1H5b_CUO"
   },
   "source": [
    "#### Lets see how well does the siamese detect an imposter. We check whether image_2 is same individual as image_1 or an imposter. We do this by computing dissimilarity score between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kz9y72aY1HCv"
   },
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n",
    "\n",
    "lab = ['same', 'imposter']\n",
    "with torch.no_grad():\n",
    "  for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n",
    "      if batch_idx%10 == 0 or int(targets)==0:      ### show every tenth image or if its the same individual\n",
    "\n",
    "          inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n",
    "          features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n",
    "          features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n",
    "\n",
    "          dissimilarity = torch.nn.functional.cosine_similarity(features_1, features_2).item()\n",
    "          img = np.concatenate((inputs_1.data.cpu().numpy()[0][0], inputs_2.data.cpu().numpy()[0][0]), axis = 1)\n",
    "          plt.imshow(img, cmap='gray')\n",
    "          plt.text(100,20,str(dissimilarity), fontsize=24, color='r')     ### similarity score\n",
    "          plt.text(100,40,lab[int(targets.data[0])], fontsize=24, color='r')   ### ground truth\n",
    "          plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9di4sNM_CUc"
   },
   "source": [
    "### Now we use this network for feature extraction and train an MLP classifier. Feature_net is not updated/train/tweak after this. We only train the MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJTaxxb21riY"
   },
   "outputs": [],
   "source": [
    "from google.colab import files \n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FTC3qFNJ_CUe",
    "outputId": "c07bff08-993b-45b3-8f0c-19f3edfed694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 1095\n"
     ]
    }
   ],
   "source": [
    "from data_loader import custom_data_loader\n",
    "\n",
    "train_list_file = cf.data_dir+'IMFDB_train.txt'   #### 5000 images for training\n",
    "val_list_file = cf.data_dir+'IMFDB_test.txt'      #### 1095 images for validation\n",
    "\n",
    "\n",
    "train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n",
    "val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n",
    "\n",
    "print(len(train_image_list), len(val_image_list))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n",
    "                                                             resize = True, resize_shape=[128,128]), \n",
    "                                          batch_size=32, num_workers=16, shuffle = True, pin_memory=False)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                         batch_size=10, num_workers=5, shuffle = False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tfWqQZB_CUm"
   },
   "outputs": [],
   "source": [
    "def train_classifier(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    classifier.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #inputs, targets = Variable(inputs), Variable(targets)\n",
    "        features = feature_net(inputs)[1]      \n",
    "        \n",
    "        \n",
    "        outputs = classifier(features)\n",
    "        size_ = outputs.size()\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        loss = criterion(outputs_, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        \n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srGe92K5_CUu"
   },
   "outputs": [],
   "source": [
    "def test_classifier(epoch):\n",
    "    global best_acc\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        #if device:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        features = feature_net.get_features(inputs).detach()\n",
    "        \n",
    "        outputs = classifier(features)\n",
    "        size_ = outputs.size()\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        loss = criterion(outputs_, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        \n",
    "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': classifier,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(cf.data_dir+'checkpoint'):\n",
    "            os.mkdir(cf.data_dir+'checkpoint')\n",
    "        torch.save(state, cf.data_dir+'checkpoint/checkpoint_ckpt.t7')\n",
    "        best_acc = acc\n",
    "    \n",
    "    return test_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bO-t7kPP_CU0"
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "experiment = 'siamese_IMFDB'\n",
    "train_loss_file = open(cf.data_dir+experiment+\"train_loss.txt\", \"rb\", 0)\n",
    "val_loss_file = open(cf.data_dir+experiment+\"val_loss.txt\", \"rb\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5DdHCUt1XQP"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n",
    "for epoch in range(0, 30):\n",
    "    train_classifier(epoch)\n",
    "    test_loss = test_classifier(epoch)\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "train_loss_file.close()\n",
    "val_loss_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zOf7cwQ_CVG"
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "    feature_net.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n",
    "                                                           resize = True, resize_shape=[128,128]), \n",
    "                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    conf_mat = np.zeros((num_classes, num_classes))\n",
    "    total_ = 1e-12+np.zeros((num_classes))\n",
    "    wrong_predictions = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        #if use_cuda:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        features = feature_net.features(inputs).detach()\n",
    "        features = features.view(1,-1)\n",
    "        #print(features.size())        \n",
    "        outputs = classifier(features)\n",
    "        size_ = outputs.size()\n",
    "        outputs_ = outputs.view(size_[0], num_classes)\n",
    "        _, predicted = torch.max(outputs_.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        prediction = predicted.cpu().numpy()[0]\n",
    "        targets = targets.data.cpu().numpy()[0]\n",
    "        total_[targets] +=1\n",
    "        conf_mat[predicted, targets] +=1\n",
    "        \n",
    "        if prediction != targets:\n",
    "            wrong_predictions += [[inputs, prediction, targets]]\n",
    "        \n",
    "    for k in range(num_classes):\n",
    "        conf_mat[:,k] /= total_[k]\n",
    "    return conf_mat, 100.*correct/total, wrong_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xu5-2sTarNfA"
   },
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(nn.Linear(8192, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
    "                           nn.Linear(32, num_classes))\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1Lvd4Aq1dew"
   },
   "outputs": [],
   "source": [
    "conf, acc, wrong_predictions = eval()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uelo2wrL1gH7"
   },
   "outputs": [],
   "source": [
    "plt.imshow(conf, cmap='jet', vmin=0, vmax = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h19bPE7C1lXK"
   },
   "outputs": [],
   "source": [
    "for w in wrong_predictions[::10]:\n",
    "    print(classes[w[2]], 'confused with', classes[w[1]])\n",
    "    plt.imshow(w[0][0][0].data.cpu().numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Siamese (1).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
