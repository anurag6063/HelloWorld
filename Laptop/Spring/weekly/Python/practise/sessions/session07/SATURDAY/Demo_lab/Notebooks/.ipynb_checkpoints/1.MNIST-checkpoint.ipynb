{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layered Perceptron (MLP) - A quick (re-)introduction usng PyTorch\n",
    "\n",
    "In this notebook, we will train a simple MLP on the EMNIST data set using PyTorch library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as weight_init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from loadFashionMNIST import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting gpu device if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 0.4.1 CUDA: True\n"
     ]
    }
   ],
   "source": [
    "### To test whether GPU instance is present in the system of not.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data set\n",
    "\n",
    "For the demo, we will consider only 10% (or 0.1 fraction) of the original training data. We will keep the entire test set as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionMNIST(split='train', frac=0.1)\n",
    "test_dataset = FashionMNIST(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3702aa1fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].size())\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(train_dataset[0][0].squeeze().numpy(),cmap='gray')\n",
    "print(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "#loading the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# loading the test dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3702aa1940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting 10 samples\n",
    "for (X_train, y_train) in train_loader:\n",
    "    pltsize=1\n",
    "    plt.figure(figsize=(10*pltsize, pltsize))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(1,10,i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple MLP\n",
    "\n",
    "Let's define the network as a Python class. This Python class inherits functions from _nn.module_.\n",
    "\n",
    "There are three convenient functions that are defined in this class:\n",
    "\n",
    "- ### **\\__init__()**:\n",
    "In this function, we shall declare all the layers of our neural network, including the number of neurons, non-linear activations, etc.\n",
    "\n",
    "- ### **forward()**:\n",
    "This is the function that is used to compute forward pass of the network. Here, we shall connect the different layers we had defined in \\__init__, according to the network architecture we want to make. In this case, $x -> fc1 -> relu -> fc2 -> out$.\n",
    "\n",
    "\"forward\" can be called by calling the object of this class directly. For example:\n",
    "\n",
    "```\n",
    "net = Network()\n",
    "out = net(x)\n",
    "```\n",
    "\n",
    "- ### **backward()**:\n",
    "This function is used to compute gradients across the entire network, and is called from the loss function at the end of the network.\n",
    "\n",
    "```\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "We have to write the **__init__()** and **forward()** methods, and PyTorch will automatically generate a **backward()** method for computing the gradients for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        #Weight Initialization\n",
    "        for m in self.modules():\n",
    "          if isinstance(m,nn.Linear):\n",
    "            weight_init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.softmax(self.fc3(out), dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MLP object and transfering it to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)\n",
    "print(net)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss for optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100], Loss: 2.3071\n",
      "Epoch [1/100], Step [200], Loss: 2.3077\n",
      "Epoch [1/100], Step [300], Loss: 2.2973\n",
      "Epoch [1/100], Step [400], Loss: 2.3018\n",
      "Epoch [1/100], Step [500], Loss: 2.3014\n",
      "Epoch [1/100], Step [600], Loss: 2.2893\n",
      "Accuracy for epoch 1 is 20%\n",
      "Epoch [2/100], Step [100], Loss: 2.2940\n",
      "Epoch [2/100], Step [200], Loss: 2.3102\n",
      "Epoch [2/100], Step [300], Loss: 2.2844\n",
      "Epoch [2/100], Step [400], Loss: 2.2724\n",
      "Epoch [2/100], Step [500], Loss: 2.2435\n",
      "Epoch [2/100], Step [600], Loss: 2.3082\n",
      "Accuracy for epoch 2 is 22%\n",
      "Epoch [3/100], Step [100], Loss: 2.2854\n",
      "Epoch [3/100], Step [200], Loss: 2.2725\n",
      "Epoch [3/100], Step [300], Loss: 2.2871\n",
      "Epoch [3/100], Step [400], Loss: 2.2540\n",
      "Epoch [3/100], Step [500], Loss: 2.2527\n",
      "Epoch [3/100], Step [600], Loss: 2.2690\n",
      "Accuracy for epoch 3 is 23%\n",
      "Epoch [4/100], Step [100], Loss: 2.2350\n",
      "Epoch [4/100], Step [200], Loss: 2.2725\n",
      "Epoch [4/100], Step [300], Loss: 2.3086\n",
      "Epoch [4/100], Step [400], Loss: 2.2916\n",
      "Epoch [4/100], Step [500], Loss: 2.2982\n",
      "Epoch [4/100], Step [600], Loss: 2.3021\n",
      "Accuracy for epoch 4 is 22%\n",
      "Epoch [5/100], Step [100], Loss: 2.2926\n",
      "Epoch [5/100], Step [200], Loss: 2.2214\n",
      "Epoch [5/100], Step [300], Loss: 2.2075\n",
      "Epoch [5/100], Step [400], Loss: 2.2339\n",
      "Epoch [5/100], Step [500], Loss: 2.2730\n",
      "Epoch [5/100], Step [600], Loss: 2.2420\n",
      "Accuracy for epoch 5 is 22%\n",
      "Epoch [6/100], Step [100], Loss: 2.2120\n",
      "Epoch [6/100], Step [200], Loss: 2.3110\n",
      "Epoch [6/100], Step [300], Loss: 2.2350\n",
      "Epoch [6/100], Step [400], Loss: 2.2945\n",
      "Epoch [6/100], Step [500], Loss: 2.2716\n",
      "Epoch [6/100], Step [600], Loss: 2.3187\n",
      "Accuracy for epoch 6 is 23%\n",
      "Epoch [7/100], Step [100], Loss: 2.1430\n",
      "Epoch [7/100], Step [200], Loss: 2.2363\n",
      "Epoch [7/100], Step [300], Loss: 2.3131\n",
      "Epoch [7/100], Step [400], Loss: 2.2392\n",
      "Epoch [7/100], Step [500], Loss: 2.2121\n",
      "Epoch [7/100], Step [600], Loss: 2.2859\n",
      "Accuracy for epoch 7 is 25%\n",
      "Epoch [8/100], Step [100], Loss: 2.2670\n",
      "Epoch [8/100], Step [200], Loss: 2.2940\n",
      "Epoch [8/100], Step [300], Loss: 2.2609\n",
      "Epoch [8/100], Step [400], Loss: 2.1837\n",
      "Epoch [8/100], Step [500], Loss: 2.3030\n",
      "Epoch [8/100], Step [600], Loss: 2.1055\n",
      "Accuracy for epoch 8 is 27%\n",
      "Epoch [9/100], Step [100], Loss: 2.2656\n",
      "Epoch [9/100], Step [200], Loss: 2.1892\n",
      "Epoch [9/100], Step [300], Loss: 2.2619\n",
      "Epoch [9/100], Step [400], Loss: 2.2690\n",
      "Epoch [9/100], Step [500], Loss: 2.2463\n",
      "Epoch [9/100], Step [600], Loss: 2.2238\n",
      "Accuracy for epoch 9 is 28%\n",
      "Epoch [10/100], Step [100], Loss: 2.2486\n",
      "Epoch [10/100], Step [200], Loss: 2.2035\n",
      "Epoch [10/100], Step [300], Loss: 2.2385\n",
      "Epoch [10/100], Step [400], Loss: 2.1847\n",
      "Epoch [10/100], Step [500], Loss: 2.1874\n",
      "Epoch [10/100], Step [600], Loss: 2.2197\n",
      "Accuracy for epoch 10 is 30%\n",
      "Epoch [11/100], Step [100], Loss: 2.1006\n",
      "Epoch [11/100], Step [200], Loss: 2.1395\n",
      "Epoch [11/100], Step [300], Loss: 2.1836\n",
      "Epoch [11/100], Step [400], Loss: 2.1778\n",
      "Epoch [11/100], Step [500], Loss: 2.2066\n",
      "Epoch [11/100], Step [600], Loss: 2.2845\n",
      "Accuracy for epoch 11 is 31%\n",
      "Epoch [12/100], Step [100], Loss: 2.2530\n",
      "Epoch [12/100], Step [200], Loss: 2.3062\n",
      "Epoch [12/100], Step [300], Loss: 2.1117\n",
      "Epoch [12/100], Step [400], Loss: 2.1821\n",
      "Epoch [12/100], Step [500], Loss: 2.1071\n",
      "Epoch [12/100], Step [600], Loss: 2.1957\n",
      "Accuracy for epoch 12 is 32%\n",
      "Epoch [13/100], Step [100], Loss: 2.2420\n",
      "Epoch [13/100], Step [200], Loss: 2.1247\n",
      "Epoch [13/100], Step [300], Loss: 2.1853\n",
      "Epoch [13/100], Step [400], Loss: 2.1449\n",
      "Epoch [13/100], Step [500], Loss: 2.0486\n",
      "Epoch [13/100], Step [600], Loss: 2.0220\n",
      "Accuracy for epoch 13 is 35%\n",
      "Epoch [14/100], Step [100], Loss: 2.0149\n",
      "Epoch [14/100], Step [200], Loss: 1.9644\n",
      "Epoch [14/100], Step [300], Loss: 1.9676\n",
      "Epoch [14/100], Step [400], Loss: 2.1294\n",
      "Epoch [14/100], Step [500], Loss: 2.0615\n",
      "Epoch [14/100], Step [600], Loss: 2.1451\n",
      "Accuracy for epoch 14 is 39%\n",
      "Epoch [15/100], Step [100], Loss: 1.9709\n",
      "Epoch [15/100], Step [200], Loss: 2.2447\n",
      "Epoch [15/100], Step [300], Loss: 1.9012\n",
      "Epoch [15/100], Step [400], Loss: 1.8995\n",
      "Epoch [15/100], Step [500], Loss: 2.0956\n",
      "Epoch [15/100], Step [600], Loss: 2.1972\n",
      "Accuracy for epoch 15 is 42%\n",
      "Epoch [16/100], Step [100], Loss: 2.0440\n",
      "Epoch [16/100], Step [200], Loss: 2.1510\n",
      "Epoch [16/100], Step [300], Loss: 2.1317\n",
      "Epoch [16/100], Step [400], Loss: 2.0308\n",
      "Epoch [16/100], Step [500], Loss: 2.1007\n",
      "Epoch [16/100], Step [600], Loss: 1.9531\n",
      "Accuracy for epoch 16 is 46%\n",
      "Epoch [17/100], Step [100], Loss: 1.9886\n",
      "Epoch [17/100], Step [200], Loss: 2.1880\n",
      "Epoch [17/100], Step [300], Loss: 1.9417\n",
      "Epoch [17/100], Step [400], Loss: 1.8311\n",
      "Epoch [17/100], Step [500], Loss: 2.1434\n",
      "Epoch [17/100], Step [600], Loss: 1.9525\n",
      "Accuracy for epoch 17 is 48%\n",
      "Epoch [18/100], Step [100], Loss: 2.0848\n",
      "Epoch [18/100], Step [200], Loss: 1.9504\n",
      "Epoch [18/100], Step [300], Loss: 1.9803\n",
      "Epoch [18/100], Step [400], Loss: 2.2968\n",
      "Epoch [18/100], Step [500], Loss: 1.8682\n",
      "Epoch [18/100], Step [600], Loss: 1.7186\n",
      "Accuracy for epoch 18 is 50%\n",
      "Epoch [19/100], Step [100], Loss: 2.0054\n",
      "Epoch [19/100], Step [200], Loss: 2.0089\n",
      "Epoch [19/100], Step [300], Loss: 1.9822\n",
      "Epoch [19/100], Step [400], Loss: 2.0445\n",
      "Epoch [19/100], Step [500], Loss: 2.0757\n",
      "Epoch [19/100], Step [600], Loss: 1.9946\n",
      "Accuracy for epoch 19 is 52%\n",
      "Epoch [20/100], Step [100], Loss: 2.1047\n",
      "Epoch [20/100], Step [200], Loss: 1.9277\n",
      "Epoch [20/100], Step [300], Loss: 1.9406\n",
      "Epoch [20/100], Step [400], Loss: 1.7852\n",
      "Epoch [20/100], Step [500], Loss: 2.2105\n",
      "Epoch [20/100], Step [600], Loss: 1.8202\n",
      "Accuracy for epoch 20 is 54%\n",
      "Epoch [21/100], Step [100], Loss: 1.7402\n",
      "Epoch [21/100], Step [200], Loss: 1.9182\n",
      "Epoch [21/100], Step [300], Loss: 1.9595\n",
      "Epoch [21/100], Step [400], Loss: 1.9739\n",
      "Epoch [21/100], Step [500], Loss: 2.0576\n",
      "Epoch [21/100], Step [600], Loss: 1.9322\n",
      "Accuracy for epoch 21 is 57%\n",
      "Epoch [22/100], Step [100], Loss: 2.1064\n",
      "Epoch [22/100], Step [200], Loss: 1.9167\n",
      "Epoch [22/100], Step [300], Loss: 2.0336\n",
      "Epoch [22/100], Step [400], Loss: 2.0876\n",
      "Epoch [22/100], Step [500], Loss: 1.7289\n",
      "Epoch [22/100], Step [600], Loss: 1.8980\n",
      "Accuracy for epoch 22 is 61%\n",
      "Epoch [23/100], Step [100], Loss: 1.9385\n",
      "Epoch [23/100], Step [200], Loss: 1.7023\n",
      "Epoch [23/100], Step [300], Loss: 2.0176\n",
      "Epoch [23/100], Step [400], Loss: 1.9696\n",
      "Epoch [23/100], Step [500], Loss: 1.9829\n",
      "Epoch [23/100], Step [600], Loss: 2.0322\n",
      "Accuracy for epoch 23 is 63%\n",
      "Epoch [24/100], Step [100], Loss: 1.7005\n",
      "Epoch [24/100], Step [200], Loss: 1.8444\n",
      "Epoch [24/100], Step [300], Loss: 2.0337\n",
      "Epoch [24/100], Step [400], Loss: 1.7666\n",
      "Epoch [24/100], Step [500], Loss: 1.9961\n",
      "Epoch [24/100], Step [600], Loss: 1.9493\n",
      "Accuracy for epoch 24 is 63%\n",
      "Epoch [25/100], Step [100], Loss: 1.8182\n",
      "Epoch [25/100], Step [200], Loss: 1.9509\n",
      "Epoch [25/100], Step [300], Loss: 2.0881\n",
      "Epoch [25/100], Step [400], Loss: 1.8263\n",
      "Epoch [25/100], Step [500], Loss: 1.7729\n",
      "Epoch [25/100], Step [600], Loss: 2.0598\n",
      "Accuracy for epoch 25 is 63%\n",
      "Epoch [26/100], Step [100], Loss: 1.8667\n",
      "Epoch [26/100], Step [200], Loss: 2.0232\n",
      "Epoch [26/100], Step [300], Loss: 1.8759\n",
      "Epoch [26/100], Step [400], Loss: 1.5606\n",
      "Epoch [26/100], Step [500], Loss: 1.8023\n",
      "Epoch [26/100], Step [600], Loss: 1.8598\n",
      "Accuracy for epoch 26 is 64%\n",
      "Epoch [27/100], Step [100], Loss: 1.7990\n",
      "Epoch [27/100], Step [200], Loss: 1.8615\n",
      "Epoch [27/100], Step [300], Loss: 1.9783\n",
      "Epoch [27/100], Step [400], Loss: 1.9379\n",
      "Epoch [27/100], Step [500], Loss: 1.7816\n",
      "Epoch [27/100], Step [600], Loss: 1.8929\n",
      "Accuracy for epoch 27 is 64%\n",
      "Epoch [28/100], Step [100], Loss: 1.8056\n",
      "Epoch [28/100], Step [200], Loss: 1.8128\n",
      "Epoch [28/100], Step [300], Loss: 1.8368\n",
      "Epoch [28/100], Step [400], Loss: 1.8866\n",
      "Epoch [28/100], Step [500], Loss: 1.9364\n",
      "Epoch [28/100], Step [600], Loss: 1.7771\n",
      "Accuracy for epoch 28 is 65%\n",
      "Epoch [29/100], Step [100], Loss: 1.7608\n",
      "Epoch [29/100], Step [200], Loss: 1.9775\n",
      "Epoch [29/100], Step [300], Loss: 2.0682\n",
      "Epoch [29/100], Step [400], Loss: 1.7546\n",
      "Epoch [29/100], Step [500], Loss: 1.9481\n",
      "Epoch [29/100], Step [600], Loss: 1.8004\n",
      "Accuracy for epoch 29 is 67%\n",
      "Epoch [30/100], Step [100], Loss: 1.8843\n",
      "Epoch [30/100], Step [200], Loss: 1.9128\n",
      "Epoch [30/100], Step [300], Loss: 1.9276\n",
      "Epoch [30/100], Step [400], Loss: 1.8455\n",
      "Epoch [30/100], Step [500], Loss: 1.9424\n",
      "Epoch [30/100], Step [600], Loss: 1.6902\n",
      "Accuracy for epoch 30 is 69%\n",
      "Epoch [31/100], Step [100], Loss: 1.8587\n",
      "Epoch [31/100], Step [200], Loss: 1.7325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Step [300], Loss: 2.0035\n",
      "Epoch [31/100], Step [400], Loss: 1.9110\n",
      "Epoch [31/100], Step [500], Loss: 1.8192\n",
      "Epoch [31/100], Step [600], Loss: 1.7598\n",
      "Accuracy for epoch 31 is 72%\n",
      "Epoch [32/100], Step [100], Loss: 1.6678\n",
      "Epoch [32/100], Step [200], Loss: 1.7297\n",
      "Epoch [32/100], Step [300], Loss: 1.7360\n",
      "Epoch [32/100], Step [400], Loss: 1.7221\n",
      "Epoch [32/100], Step [500], Loss: 1.7898\n",
      "Epoch [32/100], Step [600], Loss: 1.8532\n",
      "Accuracy for epoch 32 is 75%\n",
      "Epoch [33/100], Step [100], Loss: 1.9776\n",
      "Epoch [33/100], Step [200], Loss: 1.7789\n",
      "Epoch [33/100], Step [300], Loss: 1.8561\n",
      "Epoch [33/100], Step [400], Loss: 1.8865\n",
      "Epoch [33/100], Step [500], Loss: 1.8378\n",
      "Epoch [33/100], Step [600], Loss: 1.8630\n",
      "Accuracy for epoch 33 is 76%\n",
      "Epoch [34/100], Step [100], Loss: 1.7489\n",
      "Epoch [34/100], Step [200], Loss: 1.8382\n",
      "Epoch [34/100], Step [300], Loss: 2.0572\n",
      "Epoch [34/100], Step [400], Loss: 2.0423\n",
      "Epoch [34/100], Step [500], Loss: 1.8244\n",
      "Epoch [34/100], Step [600], Loss: 1.7945\n",
      "Accuracy for epoch 34 is 77%\n",
      "Epoch [35/100], Step [100], Loss: 1.8910\n",
      "Epoch [35/100], Step [200], Loss: 1.7311\n",
      "Epoch [35/100], Step [300], Loss: 1.8208\n",
      "Epoch [35/100], Step [400], Loss: 1.6871\n",
      "Epoch [35/100], Step [500], Loss: 1.6516\n",
      "Epoch [35/100], Step [600], Loss: 1.9685\n",
      "Accuracy for epoch 35 is 78%\n",
      "Epoch [36/100], Step [100], Loss: 1.7206\n",
      "Epoch [36/100], Step [200], Loss: 1.9462\n",
      "Epoch [36/100], Step [300], Loss: 1.6830\n",
      "Epoch [36/100], Step [400], Loss: 1.7605\n",
      "Epoch [36/100], Step [500], Loss: 1.8110\n",
      "Epoch [36/100], Step [600], Loss: 1.7786\n",
      "Accuracy for epoch 36 is 78%\n",
      "Epoch [37/100], Step [100], Loss: 1.8587\n",
      "Epoch [37/100], Step [200], Loss: 1.5592\n",
      "Epoch [37/100], Step [300], Loss: 1.7956\n",
      "Epoch [37/100], Step [400], Loss: 1.7866\n",
      "Epoch [37/100], Step [500], Loss: 1.6287\n",
      "Epoch [37/100], Step [600], Loss: 1.8667\n",
      "Accuracy for epoch 37 is 78%\n",
      "Epoch [38/100], Step [100], Loss: 1.7332\n",
      "Epoch [38/100], Step [200], Loss: 1.5776\n",
      "Epoch [38/100], Step [300], Loss: 1.8807\n",
      "Epoch [38/100], Step [400], Loss: 1.8944\n",
      "Epoch [38/100], Step [500], Loss: 2.0289\n",
      "Epoch [38/100], Step [600], Loss: 1.8808\n",
      "Accuracy for epoch 38 is 79%\n",
      "Epoch [39/100], Step [100], Loss: 1.8751\n",
      "Epoch [39/100], Step [200], Loss: 1.7976\n",
      "Epoch [39/100], Step [300], Loss: 1.6383\n",
      "Epoch [39/100], Step [400], Loss: 1.7691\n",
      "Epoch [39/100], Step [500], Loss: 1.6118\n",
      "Epoch [39/100], Step [600], Loss: 1.7519\n",
      "Accuracy for epoch 39 is 79%\n",
      "Epoch [40/100], Step [100], Loss: 1.8775\n",
      "Epoch [40/100], Step [200], Loss: 1.7224\n",
      "Epoch [40/100], Step [300], Loss: 1.7958\n",
      "Epoch [40/100], Step [400], Loss: 1.6679\n",
      "Epoch [40/100], Step [500], Loss: 1.6466\n",
      "Epoch [40/100], Step [600], Loss: 1.7083\n",
      "Accuracy for epoch 40 is 79%\n",
      "Epoch [41/100], Step [100], Loss: 1.6589\n",
      "Epoch [41/100], Step [200], Loss: 1.6932\n",
      "Epoch [41/100], Step [300], Loss: 1.7331\n",
      "Epoch [41/100], Step [400], Loss: 1.6102\n",
      "Epoch [41/100], Step [500], Loss: 1.8207\n",
      "Epoch [41/100], Step [600], Loss: 1.6738\n",
      "Accuracy for epoch 41 is 79%\n",
      "Epoch [42/100], Step [100], Loss: 1.8596\n",
      "Epoch [42/100], Step [200], Loss: 1.6824\n",
      "Epoch [42/100], Step [300], Loss: 1.7721\n",
      "Epoch [42/100], Step [400], Loss: 1.6047\n",
      "Epoch [42/100], Step [500], Loss: 1.6771\n",
      "Epoch [42/100], Step [600], Loss: 2.0212\n",
      "Accuracy for epoch 42 is 80%\n",
      "Epoch [43/100], Step [100], Loss: 1.6424\n",
      "Epoch [43/100], Step [200], Loss: 1.7268\n",
      "Epoch [43/100], Step [300], Loss: 1.5320\n",
      "Epoch [43/100], Step [400], Loss: 1.7173\n",
      "Epoch [43/100], Step [500], Loss: 1.6358\n",
      "Epoch [43/100], Step [600], Loss: 1.7627\n",
      "Accuracy for epoch 43 is 80%\n",
      "Epoch [44/100], Step [100], Loss: 1.6366\n",
      "Epoch [44/100], Step [200], Loss: 1.9879\n",
      "Epoch [44/100], Step [300], Loss: 1.5963\n",
      "Epoch [44/100], Step [400], Loss: 1.6368\n",
      "Epoch [44/100], Step [500], Loss: 1.9309\n",
      "Epoch [44/100], Step [600], Loss: 1.5909\n",
      "Accuracy for epoch 44 is 80%\n",
      "Epoch [45/100], Step [100], Loss: 1.7393\n",
      "Epoch [45/100], Step [200], Loss: 1.8796\n",
      "Epoch [45/100], Step [300], Loss: 1.6264\n",
      "Epoch [45/100], Step [400], Loss: 1.8134\n",
      "Epoch [45/100], Step [500], Loss: 1.6988\n",
      "Epoch [45/100], Step [600], Loss: 1.5017\n",
      "Accuracy for epoch 45 is 80%\n",
      "Epoch [46/100], Step [100], Loss: 1.6874\n",
      "Epoch [46/100], Step [200], Loss: 1.5781\n",
      "Epoch [46/100], Step [300], Loss: 1.6082\n",
      "Epoch [46/100], Step [400], Loss: 1.6303\n",
      "Epoch [46/100], Step [500], Loss: 1.7492\n",
      "Epoch [46/100], Step [600], Loss: 1.6105\n",
      "Accuracy for epoch 46 is 81%\n",
      "Epoch [47/100], Step [100], Loss: 1.6416\n",
      "Epoch [47/100], Step [200], Loss: 1.8302\n",
      "Epoch [47/100], Step [300], Loss: 1.6552\n",
      "Epoch [47/100], Step [400], Loss: 1.6939\n",
      "Epoch [47/100], Step [500], Loss: 1.6467\n",
      "Epoch [47/100], Step [600], Loss: 1.7514\n",
      "Accuracy for epoch 47 is 81%\n",
      "Epoch [48/100], Step [100], Loss: 1.6905\n",
      "Epoch [48/100], Step [200], Loss: 1.5737\n",
      "Epoch [48/100], Step [300], Loss: 1.7423\n",
      "Epoch [48/100], Step [400], Loss: 1.6531\n",
      "Epoch [48/100], Step [500], Loss: 1.6262\n",
      "Epoch [48/100], Step [600], Loss: 1.6683\n",
      "Accuracy for epoch 48 is 81%\n",
      "Epoch [49/100], Step [100], Loss: 1.4988\n",
      "Epoch [49/100], Step [200], Loss: 1.7075\n",
      "Epoch [49/100], Step [300], Loss: 1.5378\n",
      "Epoch [49/100], Step [400], Loss: 1.8355\n",
      "Epoch [49/100], Step [500], Loss: 1.6297\n",
      "Epoch [49/100], Step [600], Loss: 1.7568\n",
      "Accuracy for epoch 49 is 81%\n",
      "Epoch [50/100], Step [100], Loss: 1.6584\n",
      "Epoch [50/100], Step [200], Loss: 1.6244\n",
      "Epoch [50/100], Step [300], Loss: 1.5839\n",
      "Epoch [50/100], Step [400], Loss: 1.7364\n",
      "Epoch [50/100], Step [500], Loss: 1.7479\n",
      "Epoch [50/100], Step [600], Loss: 1.6962\n",
      "Accuracy for epoch 50 is 81%\n",
      "Epoch [51/100], Step [100], Loss: 1.6348\n",
      "Epoch [51/100], Step [200], Loss: 1.7333\n",
      "Epoch [51/100], Step [300], Loss: 1.5209\n",
      "Epoch [51/100], Step [400], Loss: 1.6407\n",
      "Epoch [51/100], Step [500], Loss: 1.7720\n",
      "Epoch [51/100], Step [600], Loss: 1.6123\n",
      "Accuracy for epoch 51 is 81%\n",
      "Epoch [52/100], Step [100], Loss: 1.9306\n",
      "Epoch [52/100], Step [200], Loss: 1.7161\n",
      "Epoch [52/100], Step [300], Loss: 1.6333\n",
      "Epoch [52/100], Step [400], Loss: 1.6795\n",
      "Epoch [52/100], Step [500], Loss: 1.6970\n",
      "Epoch [52/100], Step [600], Loss: 1.7895\n",
      "Accuracy for epoch 52 is 81%\n",
      "Epoch [53/100], Step [100], Loss: 1.6743\n",
      "Epoch [53/100], Step [200], Loss: 1.6531\n",
      "Epoch [53/100], Step [300], Loss: 1.5281\n",
      "Epoch [53/100], Step [400], Loss: 1.8788\n",
      "Epoch [53/100], Step [500], Loss: 1.7085\n",
      "Epoch [53/100], Step [600], Loss: 1.6236\n",
      "Accuracy for epoch 53 is 81%\n",
      "Epoch [54/100], Step [100], Loss: 1.7588\n",
      "Epoch [54/100], Step [200], Loss: 1.8786\n",
      "Epoch [54/100], Step [300], Loss: 1.7167\n",
      "Epoch [54/100], Step [400], Loss: 1.5884\n",
      "Epoch [54/100], Step [500], Loss: 1.6134\n",
      "Epoch [54/100], Step [600], Loss: 1.5196\n",
      "Accuracy for epoch 54 is 81%\n",
      "Epoch [55/100], Step [100], Loss: 1.7310\n",
      "Epoch [55/100], Step [200], Loss: 1.5733\n",
      "Epoch [55/100], Step [300], Loss: 1.7577\n",
      "Epoch [55/100], Step [400], Loss: 1.5989\n",
      "Epoch [55/100], Step [500], Loss: 1.6881\n",
      "Epoch [55/100], Step [600], Loss: 1.8388\n",
      "Accuracy for epoch 55 is 82%\n",
      "Epoch [56/100], Step [100], Loss: 1.6215\n",
      "Epoch [56/100], Step [200], Loss: 1.6685\n",
      "Epoch [56/100], Step [300], Loss: 1.5605\n",
      "Epoch [56/100], Step [400], Loss: 1.7533\n",
      "Epoch [56/100], Step [500], Loss: 1.5956\n",
      "Epoch [56/100], Step [600], Loss: 1.7427\n",
      "Accuracy for epoch 56 is 81%\n",
      "Epoch [57/100], Step [100], Loss: 1.7040\n",
      "Epoch [57/100], Step [200], Loss: 1.6917\n",
      "Epoch [57/100], Step [300], Loss: 1.5476\n",
      "Epoch [57/100], Step [400], Loss: 1.7318\n",
      "Epoch [57/100], Step [500], Loss: 1.6854\n",
      "Epoch [57/100], Step [600], Loss: 1.6919\n",
      "Accuracy for epoch 57 is 82%\n",
      "Epoch [58/100], Step [100], Loss: 1.4722\n",
      "Epoch [58/100], Step [200], Loss: 1.7454\n",
      "Epoch [58/100], Step [300], Loss: 1.7858\n",
      "Epoch [58/100], Step [400], Loss: 1.6256\n",
      "Epoch [58/100], Step [500], Loss: 1.5849\n",
      "Epoch [58/100], Step [600], Loss: 1.6140\n",
      "Accuracy for epoch 58 is 82%\n",
      "Epoch [59/100], Step [100], Loss: 1.6384\n",
      "Epoch [59/100], Step [200], Loss: 1.9355\n",
      "Epoch [59/100], Step [300], Loss: 1.6460\n",
      "Epoch [59/100], Step [400], Loss: 1.6787\n",
      "Epoch [59/100], Step [500], Loss: 1.6863\n",
      "Epoch [59/100], Step [600], Loss: 1.4951\n",
      "Accuracy for epoch 59 is 82%\n",
      "Epoch [60/100], Step [100], Loss: 1.8727\n",
      "Epoch [60/100], Step [200], Loss: 1.6186\n",
      "Epoch [60/100], Step [300], Loss: 1.6794\n",
      "Epoch [60/100], Step [400], Loss: 1.5951\n",
      "Epoch [60/100], Step [500], Loss: 1.9347\n",
      "Epoch [60/100], Step [600], Loss: 1.7716\n",
      "Accuracy for epoch 60 is 82%\n",
      "Epoch [61/100], Step [100], Loss: 1.7278\n",
      "Epoch [61/100], Step [200], Loss: 1.5945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Step [300], Loss: 1.6818\n",
      "Epoch [61/100], Step [400], Loss: 1.8513\n",
      "Epoch [61/100], Step [500], Loss: 1.6407\n",
      "Epoch [61/100], Step [600], Loss: 1.7031\n",
      "Accuracy for epoch 61 is 82%\n",
      "Epoch [62/100], Step [100], Loss: 1.5841\n",
      "Epoch [62/100], Step [200], Loss: 1.5860\n",
      "Epoch [62/100], Step [300], Loss: 1.5784\n",
      "Epoch [62/100], Step [400], Loss: 1.7581\n",
      "Epoch [62/100], Step [500], Loss: 1.6031\n",
      "Epoch [62/100], Step [600], Loss: 1.7192\n",
      "Accuracy for epoch 62 is 82%\n",
      "Epoch [63/100], Step [100], Loss: 1.7442\n",
      "Epoch [63/100], Step [200], Loss: 1.5965\n",
      "Epoch [63/100], Step [300], Loss: 1.6072\n",
      "Epoch [63/100], Step [400], Loss: 1.7021\n",
      "Epoch [63/100], Step [500], Loss: 1.7016\n",
      "Epoch [63/100], Step [600], Loss: 1.6685\n",
      "Accuracy for epoch 63 is 82%\n",
      "Epoch [64/100], Step [100], Loss: 1.6716\n",
      "Epoch [64/100], Step [200], Loss: 1.6970\n",
      "Epoch [64/100], Step [300], Loss: 1.7462\n",
      "Epoch [64/100], Step [400], Loss: 1.6160\n",
      "Epoch [64/100], Step [500], Loss: 1.7042\n",
      "Epoch [64/100], Step [600], Loss: 1.4809\n",
      "Accuracy for epoch 64 is 82%\n",
      "Epoch [65/100], Step [100], Loss: 1.6394\n",
      "Epoch [65/100], Step [200], Loss: 1.7602\n",
      "Epoch [65/100], Step [300], Loss: 1.5862\n",
      "Epoch [65/100], Step [400], Loss: 1.6042\n",
      "Epoch [65/100], Step [500], Loss: 1.6686\n",
      "Epoch [65/100], Step [600], Loss: 1.5793\n",
      "Accuracy for epoch 65 is 82%\n",
      "Epoch [66/100], Step [100], Loss: 1.5058\n",
      "Epoch [66/100], Step [200], Loss: 1.5892\n",
      "Epoch [66/100], Step [300], Loss: 1.8418\n",
      "Epoch [66/100], Step [400], Loss: 1.7333\n",
      "Epoch [66/100], Step [500], Loss: 1.5278\n",
      "Epoch [66/100], Step [600], Loss: 1.5365\n",
      "Accuracy for epoch 66 is 82%\n",
      "Epoch [67/100], Step [100], Loss: 1.7455\n",
      "Epoch [67/100], Step [200], Loss: 1.6141\n",
      "Epoch [67/100], Step [300], Loss: 1.7610\n",
      "Epoch [67/100], Step [400], Loss: 1.6324\n",
      "Epoch [67/100], Step [500], Loss: 1.6258\n",
      "Epoch [67/100], Step [600], Loss: 1.5719\n",
      "Accuracy for epoch 67 is 82%\n",
      "Epoch [68/100], Step [100], Loss: 1.5848\n",
      "Epoch [68/100], Step [200], Loss: 1.6238\n",
      "Epoch [68/100], Step [300], Loss: 1.4926\n",
      "Epoch [68/100], Step [400], Loss: 1.9682\n",
      "Epoch [68/100], Step [500], Loss: 1.5506\n",
      "Epoch [68/100], Step [600], Loss: 1.6361\n",
      "Accuracy for epoch 68 is 82%\n",
      "Epoch [69/100], Step [100], Loss: 1.7713\n",
      "Epoch [69/100], Step [200], Loss: 1.5488\n",
      "Epoch [69/100], Step [300], Loss: 1.5766\n",
      "Epoch [69/100], Step [400], Loss: 1.6948\n",
      "Epoch [69/100], Step [500], Loss: 1.5470\n",
      "Epoch [69/100], Step [600], Loss: 1.7334\n",
      "Accuracy for epoch 69 is 82%\n",
      "Epoch [70/100], Step [100], Loss: 1.6630\n",
      "Epoch [70/100], Step [200], Loss: 1.6648\n",
      "Epoch [70/100], Step [300], Loss: 1.6919\n",
      "Epoch [70/100], Step [400], Loss: 1.5184\n",
      "Epoch [70/100], Step [500], Loss: 1.5135\n",
      "Epoch [70/100], Step [600], Loss: 1.6539\n",
      "Accuracy for epoch 70 is 82%\n",
      "Epoch [71/100], Step [100], Loss: 1.7241\n",
      "Epoch [71/100], Step [200], Loss: 1.8469\n",
      "Epoch [71/100], Step [300], Loss: 1.7616\n",
      "Epoch [71/100], Step [400], Loss: 1.7774\n",
      "Epoch [71/100], Step [500], Loss: 1.5982\n",
      "Epoch [71/100], Step [600], Loss: 1.6661\n",
      "Accuracy for epoch 71 is 83%\n",
      "Epoch [72/100], Step [100], Loss: 1.6090\n",
      "Epoch [72/100], Step [200], Loss: 1.6546\n",
      "Epoch [72/100], Step [300], Loss: 1.5846\n",
      "Epoch [72/100], Step [400], Loss: 1.6951\n",
      "Epoch [72/100], Step [500], Loss: 1.5740\n",
      "Epoch [72/100], Step [600], Loss: 1.7509\n",
      "Accuracy for epoch 72 is 83%\n",
      "Epoch [73/100], Step [100], Loss: 1.7937\n",
      "Epoch [73/100], Step [200], Loss: 1.8444\n",
      "Epoch [73/100], Step [300], Loss: 1.6117\n",
      "Epoch [73/100], Step [400], Loss: 1.6652\n",
      "Epoch [73/100], Step [500], Loss: 1.6161\n",
      "Epoch [73/100], Step [600], Loss: 1.6997\n",
      "Accuracy for epoch 73 is 83%\n",
      "Epoch [74/100], Step [100], Loss: 1.5289\n",
      "Epoch [74/100], Step [200], Loss: 1.8817\n",
      "Epoch [74/100], Step [300], Loss: 1.5907\n",
      "Epoch [74/100], Step [400], Loss: 1.5387\n",
      "Epoch [74/100], Step [500], Loss: 1.6781\n",
      "Epoch [74/100], Step [600], Loss: 1.4963\n",
      "Accuracy for epoch 74 is 83%\n",
      "Epoch [75/100], Step [100], Loss: 1.5662\n",
      "Epoch [75/100], Step [200], Loss: 1.5276\n",
      "Epoch [75/100], Step [300], Loss: 1.6177\n",
      "Epoch [75/100], Step [400], Loss: 1.6346\n",
      "Epoch [75/100], Step [500], Loss: 1.6271\n",
      "Epoch [75/100], Step [600], Loss: 1.6578\n",
      "Accuracy for epoch 75 is 83%\n",
      "Epoch [76/100], Step [100], Loss: 1.7585\n",
      "Epoch [76/100], Step [200], Loss: 1.5703\n",
      "Epoch [76/100], Step [300], Loss: 1.7570\n",
      "Epoch [76/100], Step [400], Loss: 1.7746\n",
      "Epoch [76/100], Step [500], Loss: 1.8259\n",
      "Epoch [76/100], Step [600], Loss: 1.6415\n",
      "Accuracy for epoch 76 is 83%\n",
      "Epoch [77/100], Step [100], Loss: 1.5604\n",
      "Epoch [77/100], Step [200], Loss: 1.6183\n",
      "Epoch [77/100], Step [300], Loss: 1.5045\n",
      "Epoch [77/100], Step [400], Loss: 1.6056\n",
      "Epoch [77/100], Step [500], Loss: 1.5150\n",
      "Epoch [77/100], Step [600], Loss: 1.6927\n",
      "Accuracy for epoch 77 is 83%\n",
      "Epoch [78/100], Step [100], Loss: 1.7519\n",
      "Epoch [78/100], Step [200], Loss: 1.5279\n",
      "Epoch [78/100], Step [300], Loss: 1.7244\n",
      "Epoch [78/100], Step [400], Loss: 1.5912\n",
      "Epoch [78/100], Step [500], Loss: 1.5571\n",
      "Epoch [78/100], Step [600], Loss: 1.7678\n",
      "Accuracy for epoch 78 is 83%\n",
      "Epoch [79/100], Step [100], Loss: 1.6968\n",
      "Epoch [79/100], Step [200], Loss: 1.5930\n",
      "Epoch [79/100], Step [300], Loss: 1.7109\n",
      "Epoch [79/100], Step [400], Loss: 1.6448\n",
      "Epoch [79/100], Step [500], Loss: 1.6433\n",
      "Epoch [79/100], Step [600], Loss: 1.7675\n",
      "Accuracy for epoch 79 is 83%\n",
      "Epoch [80/100], Step [100], Loss: 1.6808\n",
      "Epoch [80/100], Step [200], Loss: 1.5799\n",
      "Epoch [80/100], Step [300], Loss: 1.5946\n",
      "Epoch [80/100], Step [400], Loss: 1.5112\n",
      "Epoch [80/100], Step [500], Loss: 1.6749\n",
      "Epoch [80/100], Step [600], Loss: 1.5376\n",
      "Accuracy for epoch 80 is 83%\n",
      "Epoch [81/100], Step [100], Loss: 1.8833\n",
      "Epoch [81/100], Step [200], Loss: 1.6703\n",
      "Epoch [81/100], Step [300], Loss: 1.5700\n",
      "Epoch [81/100], Step [400], Loss: 1.5563\n",
      "Epoch [81/100], Step [500], Loss: 1.6409\n",
      "Epoch [81/100], Step [600], Loss: 1.7698\n",
      "Accuracy for epoch 81 is 83%\n",
      "Epoch [82/100], Step [100], Loss: 1.5668\n",
      "Epoch [82/100], Step [200], Loss: 1.7613\n",
      "Epoch [82/100], Step [300], Loss: 1.5782\n",
      "Epoch [82/100], Step [400], Loss: 1.6385\n",
      "Epoch [82/100], Step [500], Loss: 1.5639\n",
      "Epoch [82/100], Step [600], Loss: 1.7814\n",
      "Accuracy for epoch 82 is 83%\n",
      "Epoch [83/100], Step [100], Loss: 1.5430\n",
      "Epoch [83/100], Step [200], Loss: 1.7361\n",
      "Epoch [83/100], Step [300], Loss: 1.6537\n",
      "Epoch [83/100], Step [400], Loss: 1.5306\n",
      "Epoch [83/100], Step [500], Loss: 1.8191\n",
      "Epoch [83/100], Step [600], Loss: 1.7444\n",
      "Accuracy for epoch 83 is 83%\n",
      "Epoch [84/100], Step [100], Loss: 1.7175\n",
      "Epoch [84/100], Step [200], Loss: 1.5604\n",
      "Epoch [84/100], Step [300], Loss: 1.7124\n",
      "Epoch [84/100], Step [400], Loss: 1.6416\n",
      "Epoch [84/100], Step [500], Loss: 1.8873\n",
      "Epoch [84/100], Step [600], Loss: 1.6439\n",
      "Accuracy for epoch 84 is 83%\n",
      "Epoch [85/100], Step [100], Loss: 1.5787\n",
      "Epoch [85/100], Step [200], Loss: 1.6824\n",
      "Epoch [85/100], Step [300], Loss: 1.6582\n",
      "Epoch [85/100], Step [400], Loss: 1.6421\n",
      "Epoch [85/100], Step [500], Loss: 1.5122\n",
      "Epoch [85/100], Step [600], Loss: 1.6743\n",
      "Accuracy for epoch 85 is 83%\n",
      "Epoch [86/100], Step [100], Loss: 1.6908\n",
      "Epoch [86/100], Step [200], Loss: 1.7828\n",
      "Epoch [86/100], Step [300], Loss: 1.7955\n",
      "Epoch [86/100], Step [400], Loss: 1.5496\n",
      "Epoch [86/100], Step [500], Loss: 1.6750\n",
      "Epoch [86/100], Step [600], Loss: 1.6520\n",
      "Accuracy for epoch 86 is 83%\n",
      "Epoch [87/100], Step [100], Loss: 1.7753\n",
      "Epoch [87/100], Step [200], Loss: 1.5174\n",
      "Epoch [87/100], Step [300], Loss: 1.6018\n",
      "Epoch [87/100], Step [400], Loss: 1.4966\n",
      "Epoch [87/100], Step [500], Loss: 1.6766\n",
      "Epoch [87/100], Step [600], Loss: 1.6304\n",
      "Accuracy for epoch 87 is 83%\n",
      "Epoch [88/100], Step [100], Loss: 1.5769\n",
      "Epoch [88/100], Step [200], Loss: 1.7681\n",
      "Epoch [88/100], Step [300], Loss: 1.6736\n",
      "Epoch [88/100], Step [400], Loss: 1.7419\n",
      "Epoch [88/100], Step [500], Loss: 1.4959\n",
      "Epoch [88/100], Step [600], Loss: 1.5818\n",
      "Accuracy for epoch 88 is 83%\n",
      "Epoch [89/100], Step [100], Loss: 1.8342\n",
      "Epoch [89/100], Step [200], Loss: 1.7908\n",
      "Epoch [89/100], Step [300], Loss: 1.6648\n",
      "Epoch [89/100], Step [400], Loss: 1.7229\n",
      "Epoch [89/100], Step [500], Loss: 1.5775\n",
      "Epoch [89/100], Step [600], Loss: 1.5605\n",
      "Accuracy for epoch 89 is 83%\n",
      "Epoch [90/100], Step [100], Loss: 1.5547\n",
      "Epoch [90/100], Step [200], Loss: 2.0040\n",
      "Epoch [90/100], Step [300], Loss: 1.8554\n",
      "Epoch [90/100], Step [400], Loss: 1.6642\n",
      "Epoch [90/100], Step [500], Loss: 1.6750\n",
      "Epoch [90/100], Step [600], Loss: 1.6672\n",
      "Accuracy for epoch 90 is 83%\n",
      "Epoch [91/100], Step [100], Loss: 1.7592\n",
      "Epoch [91/100], Step [200], Loss: 1.5824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Step [300], Loss: 1.5669\n",
      "Epoch [91/100], Step [400], Loss: 1.4638\n",
      "Epoch [91/100], Step [500], Loss: 1.5825\n",
      "Epoch [91/100], Step [600], Loss: 1.7333\n",
      "Accuracy for epoch 91 is 83%\n",
      "Epoch [92/100], Step [100], Loss: 1.6989\n",
      "Epoch [92/100], Step [200], Loss: 1.8082\n",
      "Epoch [92/100], Step [300], Loss: 1.4876\n",
      "Epoch [92/100], Step [400], Loss: 1.6293\n",
      "Epoch [92/100], Step [500], Loss: 1.6492\n",
      "Epoch [92/100], Step [600], Loss: 1.6704\n",
      "Accuracy for epoch 92 is 83%\n",
      "Epoch [93/100], Step [100], Loss: 1.8258\n",
      "Epoch [93/100], Step [200], Loss: 1.7190\n",
      "Epoch [93/100], Step [300], Loss: 1.8549\n",
      "Epoch [93/100], Step [400], Loss: 1.5541\n",
      "Epoch [93/100], Step [500], Loss: 1.5012\n",
      "Epoch [93/100], Step [600], Loss: 1.6919\n",
      "Accuracy for epoch 93 is 84%\n",
      "Epoch [94/100], Step [100], Loss: 1.6379\n",
      "Epoch [94/100], Step [200], Loss: 1.7094\n",
      "Epoch [94/100], Step [300], Loss: 1.5856\n",
      "Epoch [94/100], Step [400], Loss: 1.8348\n",
      "Epoch [94/100], Step [500], Loss: 1.6987\n",
      "Epoch [94/100], Step [600], Loss: 1.7825\n",
      "Accuracy for epoch 94 is 84%\n",
      "Epoch [95/100], Step [100], Loss: 1.7735\n",
      "Epoch [95/100], Step [200], Loss: 1.7619\n",
      "Epoch [95/100], Step [300], Loss: 1.5676\n",
      "Epoch [95/100], Step [400], Loss: 1.6276\n",
      "Epoch [95/100], Step [500], Loss: 1.6784\n",
      "Epoch [95/100], Step [600], Loss: 1.7025\n",
      "Accuracy for epoch 95 is 84%\n",
      "Epoch [96/100], Step [100], Loss: 1.5998\n",
      "Epoch [96/100], Step [200], Loss: 1.5670\n",
      "Epoch [96/100], Step [300], Loss: 1.5863\n",
      "Epoch [96/100], Step [400], Loss: 1.5950\n",
      "Epoch [96/100], Step [500], Loss: 1.5832\n",
      "Epoch [96/100], Step [600], Loss: 1.5512\n",
      "Accuracy for epoch 96 is 84%\n",
      "Epoch [97/100], Step [100], Loss: 1.6043\n",
      "Epoch [97/100], Step [200], Loss: 1.6803\n",
      "Epoch [97/100], Step [300], Loss: 1.7591\n",
      "Epoch [97/100], Step [400], Loss: 1.6584\n",
      "Epoch [97/100], Step [500], Loss: 1.5972\n",
      "Epoch [97/100], Step [600], Loss: 1.5369\n",
      "Accuracy for epoch 97 is 84%\n",
      "Epoch [98/100], Step [100], Loss: 1.6100\n",
      "Epoch [98/100], Step [200], Loss: 1.4851\n",
      "Epoch [98/100], Step [300], Loss: 1.6236\n",
      "Epoch [98/100], Step [400], Loss: 1.4746\n",
      "Epoch [98/100], Step [500], Loss: 1.5790\n",
      "Epoch [98/100], Step [600], Loss: 1.4840\n",
      "Accuracy for epoch 98 is 84%\n",
      "Epoch [99/100], Step [100], Loss: 1.6804\n",
      "Epoch [99/100], Step [200], Loss: 1.6587\n",
      "Epoch [99/100], Step [300], Loss: 1.6588\n",
      "Epoch [99/100], Step [400], Loss: 1.8344\n",
      "Epoch [99/100], Step [500], Loss: 1.6422\n",
      "Epoch [99/100], Step [600], Loss: 1.7327\n",
      "Accuracy for epoch 99 is 84%\n",
      "Epoch [100/100], Step [100], Loss: 1.4836\n",
      "Epoch [100/100], Step [200], Loss: 1.6211\n",
      "Epoch [100/100], Step [300], Loss: 1.6778\n",
      "Epoch [100/100], Step [400], Loss: 1.6480\n",
      "Epoch [100/100], Step [500], Loss: 1.6657\n",
      "Epoch [100/100], Step [600], Loss: 1.5346\n",
      "Accuracy for epoch 100 is 84%\n"
     ]
    }
   ],
   "source": [
    "# In each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For each batch of images in train set\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Initialize gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (this calls the \"forward\" function within Net)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Find the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Find the gradients of all weights using the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights using the optimizer\n",
    "        # For e.g.: w = w - (delta_w)*lr\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Find the output by doing a forward pass through the network\n",
    "        outputs = net(images)\n",
    "\n",
    "        # Find the class of each sample by taking a max across the probabilities of each class\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, loss.item()))\n",
    "            \n",
    "    print('Accuracy for epoch {} is {}%'.format(epoch+1, (100 * correct / total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 83 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# For each batch of images in test set\n",
    "with torch.set_grad_enabled(False):\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "      # Get the images\n",
    "      images = images.view(-1, 28*28)\n",
    "\n",
    "      images = images.to(device)\n",
    "\n",
    "      # Find the output by doing a forward pass through the network\n",
    "      outputs = net(images)\n",
    "\n",
    "      # Find the class of each sample by taking a max across the probabilities of each class\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
