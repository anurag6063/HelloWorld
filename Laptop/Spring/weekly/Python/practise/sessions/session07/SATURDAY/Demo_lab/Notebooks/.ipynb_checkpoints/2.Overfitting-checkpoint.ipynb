{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layered Perceptron (MLP) - A quick (re-)introduction usng PyTorch\n",
    "\n",
    "In this notebook, we will train a simple MLP on the EMNIST data set using PyTorch library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as weight_init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from loadFashionMNIST import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting gpu device if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 0.4.1 CUDA: True\n"
     ]
    }
   ],
   "source": [
    "### To test whether GPU instance is present in the system of not.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data set\n",
    "\n",
    "For the demo, we will consider only 1% (or 0.01 fraction) of the original training data. We will keep the entire test set as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionMNIST(split='train', frac=0.01)\n",
    "test_dataset = FashionMNIST(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f652d79cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].size())\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(train_dataset[0][0].squeeze().numpy(),cmap='gray')\n",
    "print(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "#loading the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# loading the test dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f652d79c8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting 10 samples\n",
    "for (X_train, y_train) in train_loader:\n",
    "    pltsize=1\n",
    "    plt.figure(figsize=(10*pltsize, pltsize))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(1,10,i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple MLP\n",
    "\n",
    "Let's define the network as a Python class. This Python class inherits functions from _nn.module_.\n",
    "\n",
    "There are three convenient functions that are defined in this class:\n",
    "\n",
    "- ### **\\__init__()**:\n",
    "In this function, we shall declare all the layers of our neural network, including the number of neurons, non-linear activations, etc.\n",
    "\n",
    "- ### **forward()**:\n",
    "This is the function that is used to compute forward pass of the network. Here, we shall connect the different layers we had defined in \\__init__, according to the network architecture we want to make. In this case, $x -> fc1 -> relu -> fc2 -> out$.\n",
    "\n",
    "\"forward\" can be called by calling the object of this class directly. For example:\n",
    "\n",
    "```\n",
    "net = Network()\n",
    "out = net(x)\n",
    "```\n",
    "\n",
    "- ### **backward()**:\n",
    "This function is used to compute gradients across the entire network, and is called from the loss function at the end of the network.\n",
    "\n",
    "```\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "We have to write the **__init__()** and **forward()** methods, and PyTorch will automatically generate a **backward()** method for computing the gradients for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        #Weight Initialization\n",
    "        for m in self.modules():\n",
    "          if isinstance(m,nn.Linear):\n",
    "            weight_init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.softmax(self.fc3(out), dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MLP object and transfering it to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)\n",
    "print(net)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss for optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10], Loss: 2.2749\n",
      "Epoch [1/100], Step [20], Loss: 2.1130\n",
      "Epoch [1/100], Step [30], Loss: 2.3013\n",
      "Epoch [1/100], Step [40], Loss: 2.2101\n",
      "Epoch [1/100], Step [50], Loss: 2.0474\n",
      "Epoch [1/100], Step [60], Loss: 2.0584\n",
      "Accuracy for epoch 1 is 38%\n",
      "Epoch [2/100], Step [10], Loss: 1.9370\n",
      "Epoch [2/100], Step [20], Loss: 2.0455\n",
      "Epoch [2/100], Step [30], Loss: 1.9134\n",
      "Epoch [2/100], Step [40], Loss: 1.9830\n",
      "Epoch [2/100], Step [50], Loss: 1.8592\n",
      "Epoch [2/100], Step [60], Loss: 1.8549\n",
      "Accuracy for epoch 2 is 58%\n",
      "Epoch [3/100], Step [10], Loss: 1.7615\n",
      "Epoch [3/100], Step [20], Loss: 1.8508\n",
      "Epoch [3/100], Step [30], Loss: 1.8590\n",
      "Epoch [3/100], Step [40], Loss: 2.1461\n",
      "Epoch [3/100], Step [50], Loss: 1.7814\n",
      "Epoch [3/100], Step [60], Loss: 1.7702\n",
      "Accuracy for epoch 3 is 65%\n",
      "Epoch [4/100], Step [10], Loss: 1.8739\n",
      "Epoch [4/100], Step [20], Loss: 1.7439\n",
      "Epoch [4/100], Step [30], Loss: 1.8605\n",
      "Epoch [4/100], Step [40], Loss: 1.8322\n",
      "Epoch [4/100], Step [50], Loss: 1.6097\n",
      "Epoch [4/100], Step [60], Loss: 1.9549\n",
      "Accuracy for epoch 4 is 71%\n",
      "Epoch [5/100], Step [10], Loss: 1.8623\n",
      "Epoch [5/100], Step [20], Loss: 1.7721\n",
      "Epoch [5/100], Step [30], Loss: 1.8883\n",
      "Epoch [5/100], Step [40], Loss: 1.7637\n",
      "Epoch [5/100], Step [50], Loss: 1.8560\n",
      "Epoch [5/100], Step [60], Loss: 1.9245\n",
      "Accuracy for epoch 5 is 73%\n",
      "Epoch [6/100], Step [10], Loss: 1.6018\n",
      "Epoch [6/100], Step [20], Loss: 1.6274\n",
      "Epoch [6/100], Step [30], Loss: 1.7477\n",
      "Epoch [6/100], Step [40], Loss: 1.7162\n",
      "Epoch [6/100], Step [50], Loss: 1.9020\n",
      "Epoch [6/100], Step [60], Loss: 1.8855\n",
      "Accuracy for epoch 6 is 74%\n",
      "Epoch [7/100], Step [10], Loss: 1.6411\n",
      "Epoch [7/100], Step [20], Loss: 1.7764\n",
      "Epoch [7/100], Step [30], Loss: 1.5746\n",
      "Epoch [7/100], Step [40], Loss: 1.6528\n",
      "Epoch [7/100], Step [50], Loss: 1.7233\n",
      "Epoch [7/100], Step [60], Loss: 1.7617\n",
      "Accuracy for epoch 7 is 74%\n",
      "Epoch [8/100], Step [10], Loss: 1.5149\n",
      "Epoch [8/100], Step [20], Loss: 1.8939\n",
      "Epoch [8/100], Step [30], Loss: 1.6963\n",
      "Epoch [8/100], Step [40], Loss: 1.6420\n",
      "Epoch [8/100], Step [50], Loss: 1.9216\n",
      "Epoch [8/100], Step [60], Loss: 1.8218\n",
      "Accuracy for epoch 8 is 76%\n",
      "Epoch [9/100], Step [10], Loss: 1.6026\n",
      "Epoch [9/100], Step [20], Loss: 1.6866\n",
      "Epoch [9/100], Step [30], Loss: 1.9016\n",
      "Epoch [9/100], Step [40], Loss: 1.5846\n",
      "Epoch [9/100], Step [50], Loss: 1.8486\n",
      "Epoch [9/100], Step [60], Loss: 1.6812\n",
      "Accuracy for epoch 9 is 76%\n",
      "Epoch [10/100], Step [10], Loss: 1.5361\n",
      "Epoch [10/100], Step [20], Loss: 1.8700\n",
      "Epoch [10/100], Step [30], Loss: 1.5304\n",
      "Epoch [10/100], Step [40], Loss: 1.9637\n",
      "Epoch [10/100], Step [50], Loss: 1.7710\n",
      "Epoch [10/100], Step [60], Loss: 1.5653\n",
      "Accuracy for epoch 10 is 76%\n",
      "Epoch [11/100], Step [10], Loss: 2.0477\n",
      "Epoch [11/100], Step [20], Loss: 1.6778\n",
      "Epoch [11/100], Step [30], Loss: 1.8791\n",
      "Epoch [11/100], Step [40], Loss: 1.6590\n",
      "Epoch [11/100], Step [50], Loss: 1.7811\n",
      "Epoch [11/100], Step [60], Loss: 1.6450\n",
      "Accuracy for epoch 11 is 77%\n",
      "Epoch [12/100], Step [10], Loss: 1.4669\n",
      "Epoch [12/100], Step [20], Loss: 1.6623\n",
      "Epoch [12/100], Step [30], Loss: 1.6606\n",
      "Epoch [12/100], Step [40], Loss: 1.8844\n",
      "Epoch [12/100], Step [50], Loss: 1.7223\n",
      "Epoch [12/100], Step [60], Loss: 1.5206\n",
      "Accuracy for epoch 12 is 78%\n",
      "Epoch [13/100], Step [10], Loss: 1.8608\n",
      "Epoch [13/100], Step [20], Loss: 1.6502\n",
      "Epoch [13/100], Step [30], Loss: 1.6611\n",
      "Epoch [13/100], Step [40], Loss: 1.6815\n",
      "Epoch [13/100], Step [50], Loss: 1.5373\n",
      "Epoch [13/100], Step [60], Loss: 1.5619\n",
      "Accuracy for epoch 13 is 77%\n",
      "Epoch [14/100], Step [10], Loss: 1.7588\n",
      "Epoch [14/100], Step [20], Loss: 1.9475\n",
      "Epoch [14/100], Step [30], Loss: 1.6723\n",
      "Epoch [14/100], Step [40], Loss: 1.6521\n",
      "Epoch [14/100], Step [50], Loss: 1.5729\n",
      "Epoch [14/100], Step [60], Loss: 1.7691\n",
      "Accuracy for epoch 14 is 78%\n",
      "Epoch [15/100], Step [10], Loss: 1.6701\n",
      "Epoch [15/100], Step [20], Loss: 1.5667\n",
      "Epoch [15/100], Step [30], Loss: 1.5653\n",
      "Epoch [15/100], Step [40], Loss: 1.5920\n",
      "Epoch [15/100], Step [50], Loss: 1.5230\n",
      "Epoch [15/100], Step [60], Loss: 1.5774\n",
      "Accuracy for epoch 15 is 78%\n",
      "Epoch [16/100], Step [10], Loss: 1.5682\n",
      "Epoch [16/100], Step [20], Loss: 1.5682\n",
      "Epoch [16/100], Step [30], Loss: 1.6502\n",
      "Epoch [16/100], Step [40], Loss: 1.6951\n",
      "Epoch [16/100], Step [50], Loss: 1.8451\n",
      "Epoch [16/100], Step [60], Loss: 1.7228\n",
      "Accuracy for epoch 16 is 78%\n",
      "Epoch [17/100], Step [10], Loss: 1.5753\n",
      "Epoch [17/100], Step [20], Loss: 1.6583\n",
      "Epoch [17/100], Step [30], Loss: 1.6314\n",
      "Epoch [17/100], Step [40], Loss: 1.4896\n",
      "Epoch [17/100], Step [50], Loss: 1.6612\n",
      "Epoch [17/100], Step [60], Loss: 1.5658\n",
      "Accuracy for epoch 17 is 78%\n",
      "Epoch [18/100], Step [10], Loss: 2.1017\n",
      "Epoch [18/100], Step [20], Loss: 1.7779\n",
      "Epoch [18/100], Step [30], Loss: 1.7638\n",
      "Epoch [18/100], Step [40], Loss: 1.5256\n",
      "Epoch [18/100], Step [50], Loss: 1.4896\n",
      "Epoch [18/100], Step [60], Loss: 1.5752\n",
      "Accuracy for epoch 18 is 78%\n",
      "Epoch [19/100], Step [10], Loss: 1.6615\n",
      "Epoch [19/100], Step [20], Loss: 1.6591\n",
      "Epoch [19/100], Step [30], Loss: 1.9990\n",
      "Epoch [19/100], Step [40], Loss: 1.5841\n",
      "Epoch [19/100], Step [50], Loss: 1.4807\n",
      "Epoch [19/100], Step [60], Loss: 1.7593\n",
      "Accuracy for epoch 19 is 78%\n",
      "Epoch [20/100], Step [10], Loss: 1.6752\n",
      "Epoch [20/100], Step [20], Loss: 1.6616\n",
      "Epoch [20/100], Step [30], Loss: 1.6515\n",
      "Epoch [20/100], Step [40], Loss: 1.7608\n",
      "Epoch [20/100], Step [50], Loss: 1.7607\n",
      "Epoch [20/100], Step [60], Loss: 1.8640\n",
      "Accuracy for epoch 20 is 78%\n",
      "Epoch [21/100], Step [10], Loss: 1.8677\n",
      "Epoch [21/100], Step [20], Loss: 1.6623\n",
      "Epoch [21/100], Step [30], Loss: 1.5622\n",
      "Epoch [21/100], Step [40], Loss: 1.4910\n",
      "Epoch [21/100], Step [50], Loss: 1.7748\n",
      "Epoch [21/100], Step [60], Loss: 1.7850\n",
      "Accuracy for epoch 21 is 78%\n",
      "Epoch [22/100], Step [10], Loss: 1.6146\n",
      "Epoch [22/100], Step [20], Loss: 1.4783\n",
      "Epoch [22/100], Step [30], Loss: 1.6706\n",
      "Epoch [22/100], Step [40], Loss: 1.6635\n",
      "Epoch [22/100], Step [50], Loss: 1.6996\n",
      "Epoch [22/100], Step [60], Loss: 1.4641\n",
      "Accuracy for epoch 22 is 78%\n",
      "Epoch [23/100], Step [10], Loss: 1.8595\n",
      "Epoch [23/100], Step [20], Loss: 1.4707\n",
      "Epoch [23/100], Step [30], Loss: 1.7509\n",
      "Epoch [23/100], Step [40], Loss: 1.5581\n",
      "Epoch [23/100], Step [50], Loss: 1.5703\n",
      "Epoch [23/100], Step [60], Loss: 1.6613\n",
      "Accuracy for epoch 23 is 79%\n",
      "Epoch [24/100], Step [10], Loss: 1.5620\n",
      "Epoch [24/100], Step [20], Loss: 1.6623\n",
      "Epoch [24/100], Step [30], Loss: 1.7711\n",
      "Epoch [24/100], Step [40], Loss: 1.5652\n",
      "Epoch [24/100], Step [50], Loss: 1.5639\n",
      "Epoch [24/100], Step [60], Loss: 1.6636\n",
      "Accuracy for epoch 24 is 79%\n",
      "Epoch [25/100], Step [10], Loss: 1.6901\n",
      "Epoch [25/100], Step [20], Loss: 1.8554\n",
      "Epoch [25/100], Step [30], Loss: 1.5631\n",
      "Epoch [25/100], Step [40], Loss: 1.4635\n",
      "Epoch [25/100], Step [50], Loss: 1.6694\n",
      "Epoch [25/100], Step [60], Loss: 1.5656\n",
      "Accuracy for epoch 25 is 79%\n",
      "Epoch [26/100], Step [10], Loss: 1.7640\n",
      "Epoch [26/100], Step [20], Loss: 1.7619\n",
      "Epoch [26/100], Step [30], Loss: 1.5625\n",
      "Epoch [26/100], Step [40], Loss: 1.6594\n",
      "Epoch [26/100], Step [50], Loss: 1.6743\n",
      "Epoch [26/100], Step [60], Loss: 1.8618\n",
      "Accuracy for epoch 26 is 79%\n",
      "Epoch [27/100], Step [10], Loss: 1.8775\n",
      "Epoch [27/100], Step [20], Loss: 1.6730\n",
      "Epoch [27/100], Step [30], Loss: 1.6716\n",
      "Epoch [27/100], Step [40], Loss: 1.7620\n",
      "Epoch [27/100], Step [50], Loss: 1.5652\n",
      "Epoch [27/100], Step [60], Loss: 1.5612\n",
      "Accuracy for epoch 27 is 79%\n",
      "Epoch [28/100], Step [10], Loss: 1.7639\n",
      "Epoch [28/100], Step [20], Loss: 1.6772\n",
      "Epoch [28/100], Step [30], Loss: 1.6605\n",
      "Epoch [28/100], Step [40], Loss: 1.7702\n",
      "Epoch [28/100], Step [50], Loss: 1.6873\n",
      "Epoch [28/100], Step [60], Loss: 1.5659\n",
      "Accuracy for epoch 28 is 79%\n",
      "Epoch [29/100], Step [10], Loss: 1.8616\n",
      "Epoch [29/100], Step [20], Loss: 1.5589\n",
      "Epoch [29/100], Step [30], Loss: 1.7627\n",
      "Epoch [29/100], Step [40], Loss: 1.5773\n",
      "Epoch [29/100], Step [50], Loss: 1.4633\n",
      "Epoch [29/100], Step [60], Loss: 1.6666\n",
      "Accuracy for epoch 29 is 79%\n",
      "Epoch [30/100], Step [10], Loss: 1.6709\n",
      "Epoch [30/100], Step [20], Loss: 1.7583\n",
      "Epoch [30/100], Step [30], Loss: 1.5650\n",
      "Epoch [30/100], Step [40], Loss: 1.7623\n",
      "Epoch [30/100], Step [50], Loss: 1.7575\n",
      "Epoch [30/100], Step [60], Loss: 1.4618\n",
      "Accuracy for epoch 30 is 79%\n",
      "Epoch [31/100], Step [10], Loss: 1.6637\n",
      "Epoch [31/100], Step [20], Loss: 1.7598\n",
      "Epoch [31/100], Step [30], Loss: 1.5698\n",
      "Epoch [31/100], Step [40], Loss: 1.5629\n",
      "Epoch [31/100], Step [50], Loss: 1.5633\n",
      "Epoch [31/100], Step [60], Loss: 1.7659\n",
      "Accuracy for epoch 31 is 79%\n",
      "Epoch [32/100], Step [10], Loss: 1.5622\n",
      "Epoch [32/100], Step [20], Loss: 1.7605\n",
      "Epoch [32/100], Step [30], Loss: 1.5643\n",
      "Epoch [32/100], Step [40], Loss: 1.6612\n",
      "Epoch [32/100], Step [50], Loss: 1.6606\n",
      "Epoch [32/100], Step [60], Loss: 1.7659\n",
      "Accuracy for epoch 32 is 79%\n",
      "Epoch [33/100], Step [10], Loss: 1.6654\n",
      "Epoch [33/100], Step [20], Loss: 1.4625\n",
      "Epoch [33/100], Step [30], Loss: 1.7634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Step [40], Loss: 1.7624\n",
      "Epoch [33/100], Step [50], Loss: 1.7609\n",
      "Epoch [33/100], Step [60], Loss: 1.4644\n",
      "Accuracy for epoch 33 is 79%\n",
      "Epoch [34/100], Step [10], Loss: 1.8532\n",
      "Epoch [34/100], Step [20], Loss: 1.7610\n",
      "Epoch [34/100], Step [30], Loss: 1.7616\n",
      "Epoch [34/100], Step [40], Loss: 1.8646\n",
      "Epoch [34/100], Step [50], Loss: 1.5649\n",
      "Epoch [34/100], Step [60], Loss: 1.5639\n",
      "Accuracy for epoch 34 is 79%\n",
      "Epoch [35/100], Step [10], Loss: 1.6611\n",
      "Epoch [35/100], Step [20], Loss: 1.6098\n",
      "Epoch [35/100], Step [30], Loss: 1.5639\n",
      "Epoch [35/100], Step [40], Loss: 1.4615\n",
      "Epoch [35/100], Step [50], Loss: 1.7611\n",
      "Epoch [35/100], Step [60], Loss: 1.5619\n",
      "Accuracy for epoch 35 is 79%\n",
      "Epoch [36/100], Step [10], Loss: 1.5614\n",
      "Epoch [36/100], Step [20], Loss: 1.5639\n",
      "Epoch [36/100], Step [30], Loss: 1.6665\n",
      "Epoch [36/100], Step [40], Loss: 1.6637\n",
      "Epoch [36/100], Step [50], Loss: 1.6668\n",
      "Epoch [36/100], Step [60], Loss: 1.7619\n",
      "Accuracy for epoch 36 is 79%\n",
      "Epoch [37/100], Step [10], Loss: 1.7607\n",
      "Epoch [37/100], Step [20], Loss: 1.8610\n",
      "Epoch [37/100], Step [30], Loss: 1.6612\n",
      "Epoch [37/100], Step [40], Loss: 1.5629\n",
      "Epoch [37/100], Step [50], Loss: 1.6619\n",
      "Epoch [37/100], Step [60], Loss: 1.6623\n",
      "Accuracy for epoch 37 is 79%\n",
      "Epoch [38/100], Step [10], Loss: 1.5619\n",
      "Epoch [38/100], Step [20], Loss: 1.5608\n",
      "Epoch [38/100], Step [30], Loss: 1.4653\n",
      "Epoch [38/100], Step [40], Loss: 1.8619\n",
      "Epoch [38/100], Step [50], Loss: 1.6623\n",
      "Epoch [38/100], Step [60], Loss: 1.5643\n",
      "Accuracy for epoch 38 is 79%\n",
      "Epoch [39/100], Step [10], Loss: 1.6618\n",
      "Epoch [39/100], Step [20], Loss: 1.5655\n",
      "Epoch [39/100], Step [30], Loss: 1.5626\n",
      "Epoch [39/100], Step [40], Loss: 1.7603\n",
      "Epoch [39/100], Step [50], Loss: 1.7568\n",
      "Epoch [39/100], Step [60], Loss: 1.6622\n",
      "Accuracy for epoch 39 is 79%\n",
      "Epoch [40/100], Step [10], Loss: 1.5674\n",
      "Epoch [40/100], Step [20], Loss: 1.6605\n",
      "Epoch [40/100], Step [30], Loss: 1.6624\n",
      "Epoch [40/100], Step [40], Loss: 1.7636\n",
      "Epoch [40/100], Step [50], Loss: 1.8633\n",
      "Epoch [40/100], Step [60], Loss: 1.6608\n",
      "Accuracy for epoch 40 is 79%\n",
      "Epoch [41/100], Step [10], Loss: 1.5675\n",
      "Epoch [41/100], Step [20], Loss: 1.5696\n",
      "Epoch [41/100], Step [30], Loss: 1.4631\n",
      "Epoch [41/100], Step [40], Loss: 1.7619\n",
      "Epoch [41/100], Step [50], Loss: 1.5618\n",
      "Epoch [41/100], Step [60], Loss: 1.7618\n",
      "Accuracy for epoch 41 is 79%\n",
      "Epoch [42/100], Step [10], Loss: 1.8611\n",
      "Epoch [42/100], Step [20], Loss: 1.6628\n",
      "Epoch [42/100], Step [30], Loss: 1.6608\n",
      "Epoch [42/100], Step [40], Loss: 1.5607\n",
      "Epoch [42/100], Step [50], Loss: 1.5618\n",
      "Epoch [42/100], Step [60], Loss: 1.7614\n",
      "Accuracy for epoch 42 is 79%\n",
      "Epoch [43/100], Step [10], Loss: 1.6611\n",
      "Epoch [43/100], Step [20], Loss: 1.5607\n",
      "Epoch [43/100], Step [30], Loss: 1.8618\n",
      "Epoch [43/100], Step [40], Loss: 1.7609\n",
      "Epoch [43/100], Step [50], Loss: 1.4630\n",
      "Epoch [43/100], Step [60], Loss: 1.6573\n",
      "Accuracy for epoch 43 is 79%\n",
      "Epoch [44/100], Step [10], Loss: 1.7625\n",
      "Epoch [44/100], Step [20], Loss: 1.4628\n",
      "Epoch [44/100], Step [30], Loss: 1.8576\n",
      "Epoch [44/100], Step [40], Loss: 1.4677\n",
      "Epoch [44/100], Step [50], Loss: 1.5578\n",
      "Epoch [44/100], Step [60], Loss: 1.6621\n",
      "Accuracy for epoch 44 is 79%\n",
      "Epoch [45/100], Step [10], Loss: 1.5624\n",
      "Epoch [45/100], Step [20], Loss: 1.5622\n",
      "Epoch [45/100], Step [30], Loss: 1.7592\n",
      "Epoch [45/100], Step [40], Loss: 1.5618\n",
      "Epoch [45/100], Step [50], Loss: 1.5612\n",
      "Epoch [45/100], Step [60], Loss: 1.4631\n",
      "Accuracy for epoch 45 is 79%\n",
      "Epoch [46/100], Step [10], Loss: 1.7600\n",
      "Epoch [46/100], Step [20], Loss: 1.7626\n",
      "Epoch [46/100], Step [30], Loss: 1.5618\n",
      "Epoch [46/100], Step [40], Loss: 1.5624\n",
      "Epoch [46/100], Step [50], Loss: 1.9531\n",
      "Epoch [46/100], Step [60], Loss: 1.6626\n",
      "Accuracy for epoch 46 is 79%\n",
      "Epoch [47/100], Step [10], Loss: 1.7572\n",
      "Epoch [47/100], Step [20], Loss: 1.4626\n",
      "Epoch [47/100], Step [30], Loss: 1.4632\n",
      "Epoch [47/100], Step [40], Loss: 1.6743\n",
      "Epoch [47/100], Step [50], Loss: 1.4622\n",
      "Epoch [47/100], Step [60], Loss: 1.6612\n",
      "Accuracy for epoch 47 is 79%\n",
      "Epoch [48/100], Step [10], Loss: 1.6621\n",
      "Epoch [48/100], Step [20], Loss: 1.5711\n",
      "Epoch [48/100], Step [30], Loss: 1.5618\n",
      "Epoch [48/100], Step [40], Loss: 1.6618\n",
      "Epoch [48/100], Step [50], Loss: 1.7625\n",
      "Epoch [48/100], Step [60], Loss: 1.5623\n",
      "Accuracy for epoch 48 is 79%\n",
      "Epoch [49/100], Step [10], Loss: 1.4615\n",
      "Epoch [49/100], Step [20], Loss: 1.6622\n",
      "Epoch [49/100], Step [30], Loss: 1.6626\n",
      "Epoch [49/100], Step [40], Loss: 1.4618\n",
      "Epoch [49/100], Step [50], Loss: 1.6585\n",
      "Epoch [49/100], Step [60], Loss: 1.5616\n",
      "Accuracy for epoch 49 is 79%\n",
      "Epoch [50/100], Step [10], Loss: 1.6620\n",
      "Epoch [50/100], Step [20], Loss: 1.8613\n",
      "Epoch [50/100], Step [30], Loss: 1.5606\n",
      "Epoch [50/100], Step [40], Loss: 1.4613\n",
      "Epoch [50/100], Step [50], Loss: 1.7562\n",
      "Epoch [50/100], Step [60], Loss: 1.6586\n",
      "Accuracy for epoch 50 is 79%\n",
      "Epoch [51/100], Step [10], Loss: 1.6600\n",
      "Epoch [51/100], Step [20], Loss: 1.5660\n",
      "Epoch [51/100], Step [30], Loss: 1.5621\n",
      "Epoch [51/100], Step [40], Loss: 1.7626\n",
      "Epoch [51/100], Step [50], Loss: 1.8586\n",
      "Epoch [51/100], Step [60], Loss: 1.5619\n",
      "Accuracy for epoch 51 is 79%\n",
      "Epoch [52/100], Step [10], Loss: 1.6615\n",
      "Epoch [52/100], Step [20], Loss: 1.8614\n",
      "Epoch [52/100], Step [30], Loss: 1.5564\n",
      "Epoch [52/100], Step [40], Loss: 1.6646\n",
      "Epoch [52/100], Step [50], Loss: 1.4627\n",
      "Epoch [52/100], Step [60], Loss: 1.6616\n",
      "Accuracy for epoch 52 is 79%\n",
      "Epoch [53/100], Step [10], Loss: 1.6591\n",
      "Epoch [53/100], Step [20], Loss: 1.7627\n",
      "Epoch [53/100], Step [30], Loss: 1.6591\n",
      "Epoch [53/100], Step [40], Loss: 1.5631\n",
      "Epoch [53/100], Step [50], Loss: 1.6609\n",
      "Epoch [53/100], Step [60], Loss: 1.6633\n",
      "Accuracy for epoch 53 is 79%\n",
      "Epoch [54/100], Step [10], Loss: 1.8609\n",
      "Epoch [54/100], Step [20], Loss: 1.7589\n",
      "Epoch [54/100], Step [30], Loss: 1.5614\n",
      "Epoch [54/100], Step [40], Loss: 1.7625\n",
      "Epoch [54/100], Step [50], Loss: 1.6613\n",
      "Epoch [54/100], Step [60], Loss: 1.6612\n",
      "Accuracy for epoch 54 is 79%\n",
      "Epoch [55/100], Step [10], Loss: 1.7912\n",
      "Epoch [55/100], Step [20], Loss: 1.7652\n",
      "Epoch [55/100], Step [30], Loss: 1.8636\n",
      "Epoch [55/100], Step [40], Loss: 1.5687\n",
      "Epoch [55/100], Step [50], Loss: 1.7605\n",
      "Epoch [55/100], Step [60], Loss: 1.5626\n",
      "Accuracy for epoch 55 is 79%\n",
      "Epoch [56/100], Step [10], Loss: 1.5613\n",
      "Epoch [56/100], Step [20], Loss: 1.6634\n",
      "Epoch [56/100], Step [30], Loss: 1.4625\n",
      "Epoch [56/100], Step [40], Loss: 1.6588\n",
      "Epoch [56/100], Step [50], Loss: 1.6618\n",
      "Epoch [56/100], Step [60], Loss: 1.9605\n",
      "Accuracy for epoch 56 is 79%\n",
      "Epoch [57/100], Step [10], Loss: 1.8564\n",
      "Epoch [57/100], Step [20], Loss: 1.6614\n",
      "Epoch [57/100], Step [30], Loss: 1.6569\n",
      "Epoch [57/100], Step [40], Loss: 1.4613\n",
      "Epoch [57/100], Step [50], Loss: 1.5616\n",
      "Epoch [57/100], Step [60], Loss: 1.8630\n",
      "Accuracy for epoch 57 is 79%\n",
      "Epoch [58/100], Step [10], Loss: 1.5686\n",
      "Epoch [58/100], Step [20], Loss: 1.5625\n",
      "Epoch [58/100], Step [30], Loss: 1.7610\n",
      "Epoch [58/100], Step [40], Loss: 1.7624\n",
      "Epoch [58/100], Step [50], Loss: 1.5625\n",
      "Epoch [58/100], Step [60], Loss: 1.7633\n",
      "Accuracy for epoch 58 is 79%\n",
      "Epoch [59/100], Step [10], Loss: 1.6582\n",
      "Epoch [59/100], Step [20], Loss: 1.5628\n",
      "Epoch [59/100], Step [30], Loss: 1.6567\n",
      "Epoch [59/100], Step [40], Loss: 1.8553\n",
      "Epoch [59/100], Step [50], Loss: 1.7631\n",
      "Epoch [59/100], Step [60], Loss: 1.8576\n",
      "Accuracy for epoch 59 is 79%\n",
      "Epoch [60/100], Step [10], Loss: 1.7577\n",
      "Epoch [60/100], Step [20], Loss: 1.6640\n",
      "Epoch [60/100], Step [30], Loss: 1.5653\n",
      "Epoch [60/100], Step [40], Loss: 1.7581\n",
      "Epoch [60/100], Step [50], Loss: 1.6587\n",
      "Epoch [60/100], Step [60], Loss: 1.8604\n",
      "Accuracy for epoch 60 is 79%\n",
      "Epoch [61/100], Step [10], Loss: 1.5610\n",
      "Epoch [61/100], Step [20], Loss: 1.5620\n",
      "Epoch [61/100], Step [30], Loss: 1.5597\n",
      "Epoch [61/100], Step [40], Loss: 1.5610\n",
      "Epoch [61/100], Step [50], Loss: 1.8601\n",
      "Epoch [61/100], Step [60], Loss: 1.6567\n",
      "Accuracy for epoch 61 is 79%\n",
      "Epoch [62/100], Step [10], Loss: 1.9611\n",
      "Epoch [62/100], Step [20], Loss: 1.6624\n",
      "Epoch [62/100], Step [30], Loss: 1.5612\n",
      "Epoch [62/100], Step [40], Loss: 1.4624\n",
      "Epoch [62/100], Step [50], Loss: 1.4645\n",
      "Epoch [62/100], Step [60], Loss: 1.5600\n",
      "Accuracy for epoch 62 is 79%\n",
      "Epoch [63/100], Step [10], Loss: 1.7618\n",
      "Epoch [63/100], Step [20], Loss: 1.7597\n",
      "Epoch [63/100], Step [30], Loss: 1.5606\n",
      "Epoch [63/100], Step [40], Loss: 1.6616\n",
      "Epoch [63/100], Step [50], Loss: 1.7656\n",
      "Epoch [63/100], Step [60], Loss: 1.4628\n",
      "Accuracy for epoch 63 is 79%\n",
      "Epoch [64/100], Step [10], Loss: 1.8605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Step [20], Loss: 1.7618\n",
      "Epoch [64/100], Step [30], Loss: 1.7573\n",
      "Epoch [64/100], Step [40], Loss: 1.5581\n",
      "Epoch [64/100], Step [50], Loss: 1.5742\n",
      "Epoch [64/100], Step [60], Loss: 1.5636\n",
      "Accuracy for epoch 64 is 79%\n",
      "Epoch [65/100], Step [10], Loss: 1.7572\n",
      "Epoch [65/100], Step [20], Loss: 1.6702\n",
      "Epoch [65/100], Step [30], Loss: 1.7578\n",
      "Epoch [65/100], Step [40], Loss: 1.8571\n",
      "Epoch [65/100], Step [50], Loss: 2.0556\n",
      "Epoch [65/100], Step [60], Loss: 1.7620\n",
      "Accuracy for epoch 65 is 79%\n",
      "Epoch [66/100], Step [10], Loss: 1.8564\n",
      "Epoch [66/100], Step [20], Loss: 1.7607\n",
      "Epoch [66/100], Step [30], Loss: 1.9099\n",
      "Epoch [66/100], Step [40], Loss: 1.5646\n",
      "Epoch [66/100], Step [50], Loss: 1.6597\n",
      "Epoch [66/100], Step [60], Loss: 1.6901\n",
      "Accuracy for epoch 66 is 80%\n",
      "Epoch [67/100], Step [10], Loss: 1.5616\n",
      "Epoch [67/100], Step [20], Loss: 1.6616\n",
      "Epoch [67/100], Step [30], Loss: 1.5530\n",
      "Epoch [67/100], Step [40], Loss: 1.7577\n",
      "Epoch [67/100], Step [50], Loss: 1.5624\n",
      "Epoch [67/100], Step [60], Loss: 1.5563\n",
      "Accuracy for epoch 67 is 79%\n",
      "Epoch [68/100], Step [10], Loss: 1.7584\n",
      "Epoch [68/100], Step [20], Loss: 1.5620\n",
      "Epoch [68/100], Step [30], Loss: 1.5614\n",
      "Epoch [68/100], Step [40], Loss: 1.6604\n",
      "Epoch [68/100], Step [50], Loss: 1.6572\n",
      "Epoch [68/100], Step [60], Loss: 1.6604\n",
      "Accuracy for epoch 68 is 79%\n",
      "Epoch [69/100], Step [10], Loss: 1.6620\n",
      "Epoch [69/100], Step [20], Loss: 1.5626\n",
      "Epoch [69/100], Step [30], Loss: 1.6620\n",
      "Epoch [69/100], Step [40], Loss: 1.5692\n",
      "Epoch [69/100], Step [50], Loss: 1.8626\n",
      "Epoch [69/100], Step [60], Loss: 1.6636\n",
      "Accuracy for epoch 69 is 80%\n",
      "Epoch [70/100], Step [10], Loss: 1.6610\n",
      "Epoch [70/100], Step [20], Loss: 1.8557\n",
      "Epoch [70/100], Step [30], Loss: 1.5615\n",
      "Epoch [70/100], Step [40], Loss: 1.5611\n",
      "Epoch [70/100], Step [50], Loss: 1.4618\n",
      "Epoch [70/100], Step [60], Loss: 1.5607\n",
      "Accuracy for epoch 70 is 80%\n",
      "Epoch [71/100], Step [10], Loss: 1.6633\n",
      "Epoch [71/100], Step [20], Loss: 1.8622\n",
      "Epoch [71/100], Step [30], Loss: 1.5769\n",
      "Epoch [71/100], Step [40], Loss: 1.6621\n",
      "Epoch [71/100], Step [50], Loss: 1.5591\n",
      "Epoch [71/100], Step [60], Loss: 1.8528\n",
      "Accuracy for epoch 71 is 80%\n",
      "Epoch [72/100], Step [10], Loss: 1.6636\n",
      "Epoch [72/100], Step [20], Loss: 1.6611\n",
      "Epoch [72/100], Step [30], Loss: 1.5652\n",
      "Epoch [72/100], Step [40], Loss: 1.6614\n",
      "Epoch [72/100], Step [50], Loss: 1.6612\n",
      "Epoch [72/100], Step [60], Loss: 1.6596\n",
      "Accuracy for epoch 72 is 80%\n",
      "Epoch [73/100], Step [10], Loss: 1.7611\n",
      "Epoch [73/100], Step [20], Loss: 1.7604\n",
      "Epoch [73/100], Step [30], Loss: 1.5616\n",
      "Epoch [73/100], Step [40], Loss: 1.5615\n",
      "Epoch [73/100], Step [50], Loss: 1.8592\n",
      "Epoch [73/100], Step [60], Loss: 1.7579\n",
      "Accuracy for epoch 73 is 80%\n",
      "Epoch [74/100], Step [10], Loss: 1.5612\n",
      "Epoch [74/100], Step [20], Loss: 1.7838\n",
      "Epoch [74/100], Step [30], Loss: 1.8577\n",
      "Epoch [74/100], Step [40], Loss: 1.8584\n",
      "Epoch [74/100], Step [50], Loss: 1.7598\n",
      "Epoch [74/100], Step [60], Loss: 1.6594\n",
      "Accuracy for epoch 74 is 80%\n",
      "Epoch [75/100], Step [10], Loss: 1.4612\n",
      "Epoch [75/100], Step [20], Loss: 1.5649\n",
      "Epoch [75/100], Step [30], Loss: 1.5617\n",
      "Epoch [75/100], Step [40], Loss: 1.6624\n",
      "Epoch [75/100], Step [50], Loss: 1.9498\n",
      "Epoch [75/100], Step [60], Loss: 1.4621\n",
      "Accuracy for epoch 75 is 80%\n",
      "Epoch [76/100], Step [10], Loss: 1.5621\n",
      "Epoch [76/100], Step [20], Loss: 1.7610\n",
      "Epoch [76/100], Step [30], Loss: 1.6603\n",
      "Epoch [76/100], Step [40], Loss: 1.5585\n",
      "Epoch [76/100], Step [50], Loss: 1.7550\n",
      "Epoch [76/100], Step [60], Loss: 1.5620\n",
      "Accuracy for epoch 76 is 80%\n",
      "Epoch [77/100], Step [10], Loss: 1.5640\n",
      "Epoch [77/100], Step [20], Loss: 1.7609\n",
      "Epoch [77/100], Step [30], Loss: 1.7575\n",
      "Epoch [77/100], Step [40], Loss: 1.5616\n",
      "Epoch [77/100], Step [50], Loss: 1.5617\n",
      "Epoch [77/100], Step [60], Loss: 1.5623\n",
      "Accuracy for epoch 77 is 80%\n",
      "Epoch [78/100], Step [10], Loss: 1.5611\n",
      "Epoch [78/100], Step [20], Loss: 1.5589\n",
      "Epoch [78/100], Step [30], Loss: 1.7517\n",
      "Epoch [78/100], Step [40], Loss: 1.7546\n",
      "Epoch [78/100], Step [50], Loss: 1.4617\n",
      "Epoch [78/100], Step [60], Loss: 1.9580\n",
      "Accuracy for epoch 78 is 80%\n",
      "Epoch [79/100], Step [10], Loss: 1.5633\n",
      "Epoch [79/100], Step [20], Loss: 1.4617\n",
      "Epoch [79/100], Step [30], Loss: 1.6613\n",
      "Epoch [79/100], Step [40], Loss: 1.6625\n",
      "Epoch [79/100], Step [50], Loss: 1.7628\n",
      "Epoch [79/100], Step [60], Loss: 1.6584\n",
      "Accuracy for epoch 79 is 80%\n",
      "Epoch [80/100], Step [10], Loss: 1.6606\n",
      "Epoch [80/100], Step [20], Loss: 1.5592\n",
      "Epoch [80/100], Step [30], Loss: 1.4622\n",
      "Epoch [80/100], Step [40], Loss: 1.6587\n",
      "Epoch [80/100], Step [50], Loss: 1.6639\n",
      "Epoch [80/100], Step [60], Loss: 1.5614\n",
      "Accuracy for epoch 80 is 80%\n",
      "Epoch [81/100], Step [10], Loss: 1.7617\n",
      "Epoch [81/100], Step [20], Loss: 1.7575\n",
      "Epoch [81/100], Step [30], Loss: 1.7565\n",
      "Epoch [81/100], Step [40], Loss: 1.6630\n",
      "Epoch [81/100], Step [50], Loss: 1.5598\n",
      "Epoch [81/100], Step [60], Loss: 1.5612\n",
      "Accuracy for epoch 81 is 80%\n",
      "Epoch [82/100], Step [10], Loss: 1.6540\n",
      "Epoch [82/100], Step [20], Loss: 1.5608\n",
      "Epoch [82/100], Step [30], Loss: 1.8499\n",
      "Epoch [82/100], Step [40], Loss: 1.4618\n",
      "Epoch [82/100], Step [50], Loss: 1.7534\n",
      "Epoch [82/100], Step [60], Loss: 1.5639\n",
      "Accuracy for epoch 82 is 80%\n",
      "Epoch [83/100], Step [10], Loss: 1.5592\n",
      "Epoch [83/100], Step [20], Loss: 1.4621\n",
      "Epoch [83/100], Step [30], Loss: 1.5621\n",
      "Epoch [83/100], Step [40], Loss: 1.6615\n",
      "Epoch [83/100], Step [50], Loss: 1.8631\n",
      "Epoch [83/100], Step [60], Loss: 1.4635\n",
      "Accuracy for epoch 83 is 80%\n",
      "Epoch [84/100], Step [10], Loss: 1.5619\n",
      "Epoch [84/100], Step [20], Loss: 1.7608\n",
      "Epoch [84/100], Step [30], Loss: 1.5611\n",
      "Epoch [84/100], Step [40], Loss: 1.6605\n",
      "Epoch [84/100], Step [50], Loss: 1.5621\n",
      "Epoch [84/100], Step [60], Loss: 1.5611\n",
      "Accuracy for epoch 84 is 80%\n",
      "Epoch [85/100], Step [10], Loss: 1.6617\n",
      "Epoch [85/100], Step [20], Loss: 1.7522\n",
      "Epoch [85/100], Step [30], Loss: 1.7579\n",
      "Epoch [85/100], Step [40], Loss: 1.6594\n",
      "Epoch [85/100], Step [50], Loss: 1.5612\n",
      "Epoch [85/100], Step [60], Loss: 1.8619\n",
      "Accuracy for epoch 85 is 80%\n",
      "Epoch [86/100], Step [10], Loss: 1.7573\n",
      "Epoch [86/100], Step [20], Loss: 1.6576\n",
      "Epoch [86/100], Step [30], Loss: 1.6608\n",
      "Epoch [86/100], Step [40], Loss: 1.6614\n",
      "Epoch [86/100], Step [50], Loss: 1.6615\n",
      "Epoch [86/100], Step [60], Loss: 1.6606\n",
      "Accuracy for epoch 86 is 80%\n",
      "Epoch [87/100], Step [10], Loss: 1.8604\n",
      "Epoch [87/100], Step [20], Loss: 1.6621\n",
      "Epoch [87/100], Step [30], Loss: 1.6646\n",
      "Epoch [87/100], Step [40], Loss: 1.6587\n",
      "Epoch [87/100], Step [50], Loss: 1.6551\n",
      "Epoch [87/100], Step [60], Loss: 1.4618\n",
      "Accuracy for epoch 87 is 80%\n",
      "Epoch [88/100], Step [10], Loss: 1.7522\n",
      "Epoch [88/100], Step [20], Loss: 1.6601\n",
      "Epoch [88/100], Step [30], Loss: 1.4628\n",
      "Epoch [88/100], Step [40], Loss: 1.5629\n",
      "Epoch [88/100], Step [50], Loss: 1.4616\n",
      "Epoch [88/100], Step [60], Loss: 1.7609\n",
      "Accuracy for epoch 88 is 80%\n",
      "Epoch [89/100], Step [10], Loss: 1.7578\n",
      "Epoch [89/100], Step [20], Loss: 1.6622\n",
      "Epoch [89/100], Step [30], Loss: 1.6552\n",
      "Epoch [89/100], Step [40], Loss: 1.6615\n",
      "Epoch [89/100], Step [50], Loss: 1.6636\n",
      "Epoch [89/100], Step [60], Loss: 1.6490\n",
      "Accuracy for epoch 89 is 80%\n",
      "Epoch [90/100], Step [10], Loss: 1.8628\n",
      "Epoch [90/100], Step [20], Loss: 1.5616\n",
      "Epoch [90/100], Step [30], Loss: 1.7604\n",
      "Epoch [90/100], Step [40], Loss: 1.7589\n",
      "Epoch [90/100], Step [50], Loss: 1.5615\n",
      "Epoch [90/100], Step [60], Loss: 1.5618\n",
      "Accuracy for epoch 90 is 80%\n",
      "Epoch [91/100], Step [10], Loss: 1.5619\n",
      "Epoch [91/100], Step [20], Loss: 1.6591\n",
      "Epoch [91/100], Step [30], Loss: 1.6615\n",
      "Epoch [91/100], Step [40], Loss: 1.5618\n",
      "Epoch [91/100], Step [50], Loss: 1.6620\n",
      "Epoch [91/100], Step [60], Loss: 1.5634\n",
      "Accuracy for epoch 91 is 80%\n",
      "Epoch [92/100], Step [10], Loss: 1.4613\n",
      "Epoch [92/100], Step [20], Loss: 1.6612\n",
      "Epoch [92/100], Step [30], Loss: 1.6649\n",
      "Epoch [92/100], Step [40], Loss: 1.5570\n",
      "Epoch [92/100], Step [50], Loss: 1.6617\n",
      "Epoch [92/100], Step [60], Loss: 1.5625\n",
      "Accuracy for epoch 92 is 80%\n",
      "Epoch [93/100], Step [10], Loss: 1.7612\n",
      "Epoch [93/100], Step [20], Loss: 1.4614\n",
      "Epoch [93/100], Step [30], Loss: 1.6624\n",
      "Epoch [93/100], Step [40], Loss: 1.5602\n",
      "Epoch [93/100], Step [50], Loss: 1.6618\n",
      "Epoch [93/100], Step [60], Loss: 1.8612\n",
      "Accuracy for epoch 93 is 80%\n",
      "Epoch [94/100], Step [10], Loss: 1.8605\n",
      "Epoch [94/100], Step [20], Loss: 1.5657\n",
      "Epoch [94/100], Step [30], Loss: 1.9598\n",
      "Epoch [94/100], Step [40], Loss: 1.6616\n",
      "Epoch [94/100], Step [50], Loss: 1.7593\n",
      "Epoch [94/100], Step [60], Loss: 1.6638\n",
      "Accuracy for epoch 94 is 80%\n",
      "Epoch [95/100], Step [10], Loss: 1.4615\n",
      "Epoch [95/100], Step [20], Loss: 1.6612\n",
      "Epoch [95/100], Step [30], Loss: 1.5620\n",
      "Epoch [95/100], Step [40], Loss: 1.6617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100], Step [50], Loss: 1.4617\n",
      "Epoch [95/100], Step [60], Loss: 1.8601\n",
      "Accuracy for epoch 95 is 80%\n",
      "Epoch [96/100], Step [10], Loss: 1.7573\n",
      "Epoch [96/100], Step [20], Loss: 1.7545\n",
      "Epoch [96/100], Step [30], Loss: 1.8568\n",
      "Epoch [96/100], Step [40], Loss: 1.7346\n",
      "Epoch [96/100], Step [50], Loss: 1.6610\n",
      "Epoch [96/100], Step [60], Loss: 1.5612\n",
      "Accuracy for epoch 96 is 80%\n",
      "Epoch [97/100], Step [10], Loss: 1.5647\n",
      "Epoch [97/100], Step [20], Loss: 1.9577\n",
      "Epoch [97/100], Step [30], Loss: 1.8486\n",
      "Epoch [97/100], Step [40], Loss: 1.4624\n",
      "Epoch [97/100], Step [50], Loss: 1.5616\n",
      "Epoch [97/100], Step [60], Loss: 1.9500\n",
      "Accuracy for epoch 97 is 80%\n",
      "Epoch [98/100], Step [10], Loss: 1.6840\n",
      "Epoch [98/100], Step [20], Loss: 1.8584\n",
      "Epoch [98/100], Step [30], Loss: 1.7566\n",
      "Epoch [98/100], Step [40], Loss: 1.5460\n",
      "Epoch [98/100], Step [50], Loss: 1.5577\n",
      "Epoch [98/100], Step [60], Loss: 1.6606\n",
      "Accuracy for epoch 98 is 80%\n",
      "Epoch [99/100], Step [10], Loss: 1.5290\n",
      "Epoch [99/100], Step [20], Loss: 1.5605\n",
      "Epoch [99/100], Step [30], Loss: 1.6616\n",
      "Epoch [99/100], Step [40], Loss: 1.7570\n",
      "Epoch [99/100], Step [50], Loss: 1.5877\n",
      "Epoch [99/100], Step [60], Loss: 1.6597\n",
      "Accuracy for epoch 99 is 81%\n",
      "Epoch [100/100], Step [10], Loss: 1.5553\n",
      "Epoch [100/100], Step [20], Loss: 1.5671\n",
      "Epoch [100/100], Step [30], Loss: 2.0395\n",
      "Epoch [100/100], Step [40], Loss: 1.7189\n",
      "Epoch [100/100], Step [50], Loss: 1.6606\n",
      "Epoch [100/100], Step [60], Loss: 1.5955\n",
      "Accuracy for epoch 100 is 82%\n"
     ]
    }
   ],
   "source": [
    "# In each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For each batch of images in train set\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Initialize gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (this calls the \"forward\" function within Net)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Find the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Find the gradients of all weights using the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights using the optimizer\n",
    "        # For e.g.: w = w - (delta_w)*lr\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Find the output by doing a forward pass through the network\n",
    "        outputs = net(images)\n",
    "\n",
    "        # Find the class of each sample by taking a max across the probabilities of each class\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, loss.item()))\n",
    "            \n",
    "    print('Accuracy for epoch {} is {}%'.format(epoch+1, (100 * correct / total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 66 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# For each batch of images in test set\n",
    "with torch.set_grad_enabled(False):\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "      # Get the images\n",
    "      images = images.view(-1, 28*28)\n",
    "\n",
    "      images = images.to(device)\n",
    "\n",
    "      # Find the output by doing a forward pass through the network\n",
    "      outputs = net(images)\n",
    "\n",
    "      # Find the class of each sample by taking a max across the probabilities of each class\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
