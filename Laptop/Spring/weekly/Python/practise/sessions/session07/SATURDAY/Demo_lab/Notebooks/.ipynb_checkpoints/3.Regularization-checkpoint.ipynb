{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layered Perceptron (MLP) - A quick (re-)introduction usng PyTorch\n",
    "\n",
    "In this notebook, we will train a simple MLP on the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) data set using PyTorch library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init as weight_init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from loadFashionMNIST import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting gpu device if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 0.4.1 CUDA: True\n"
     ]
    }
   ],
   "source": [
    "### To test whether GPU instance is present in the system of not.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data set\n",
    "\n",
    "For the demo, we will consider only 1% (or 0.01 fraction) of the original training data. We will keep the entire test set as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionMNIST(split='train', frac=0.01)\n",
    "test_dataset = FashionMNIST(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training and a validation data set from the given train split\n",
    "\n",
    "We will split the given training data into train and val set based on 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio = 0.8\n",
    "\n",
    "length = len(train_dataset)\n",
    "inds = np.arange(0, length)\n",
    "random.shuffle(inds)\n",
    "train_inds = inds[:round(training_ratio*length)]\n",
    "val_inds = inds[round(training_ratio*length):]\n",
    "\n",
    "train_set = Subset(train_dataset, train_inds)\n",
    "val_set = Subset(train_dataset, val_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 480\n",
      "Number of validation data: 120\n",
      "Number of test data: 10000\n",
      "Image dimensions: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efba01d4940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efba6a013c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Number of training data: {}'.format(len(train_set)))\n",
    "print('Number of validation data: {}'.format(len(val_set)))\n",
    "print('Number of test data: {}'.format(len(test_dataset)))\n",
    "print('Image dimensions: {}'.format(train_dataset[0][0].size()))\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(train_dataset[0][0].squeeze().numpy(),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "# loading the train data set\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# loading the validation data set\n",
    "test_loader = torch.utils.data.DataLoader(dataset=val_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# loading the test data set\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efba01246d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting 10 samples\n",
    "for (X_train, y_train) in train_loader:\n",
    "    pltsize=1\n",
    "    plt.figure(figsize=(10*pltsize, pltsize))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(1,10,i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple MLP\n",
    "\n",
    "Let's define the network as a Python class. This Python class inherits functions from _nn.module_.\n",
    "\n",
    "There are three convenient functions that are defined in this class:\n",
    "\n",
    "- ### **\\__init__()**:\n",
    "In this function, we shall declare all the layers of our neural network, including the number of neurons, non-linear activations, etc.\n",
    "\n",
    "- ### **forward()**:\n",
    "This is the function that is used to compute forward pass of the network. Here, we shall connect the different layers we had defined in \\__init__, according to the network architecture we want to make. In this case, $x -> fc1 -> relu -> fc2 -> relu -> fc3 -> softmax/out$.\n",
    "\n",
    "\"forward\" can be called by calling the object of this class directly. For example:\n",
    "\n",
    "```\n",
    "net = Network()\n",
    "out = net(x)\n",
    "```\n",
    "\n",
    "- ### **backward()**:\n",
    "This function is used to compute gradients across the entire network, and is called from the loss function at the end of the network.\n",
    "\n",
    "```\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "We have to write the **__init__()** and **forward()** methods, and PyTorch will automatically generate a **backward()** method for computing the gradients for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden1_size = 512\n",
    "hidden2_size = 256\n",
    "num_classes = 10\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
    "        \n",
    "        #Weight Initialization\n",
    "        for m in self.modules():\n",
    "          if isinstance(m,nn.Linear):\n",
    "            weight_init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.softmax(self.fc3(out), dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MLP object and transfering it to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(input_size, hidden1_size, hidden2_size, num_classes)\n",
    "print(net)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss for optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [10], Loss: 2.2897\n",
      "Epoch [1/200], Step [20], Loss: 2.2292\n",
      "Epoch [1/200], Step [30], Loss: 2.1506\n",
      "Epoch [1/200], Step [40], Loss: 2.2195\n",
      "Training accuracy for epoch 1 is 42%\n",
      "Validation accuracy for epoch 1 is 49%\n",
      "Epoch [2/200], Step [10], Loss: 2.2342\n",
      "Epoch [2/200], Step [20], Loss: 1.6577\n",
      "Epoch [2/200], Step [30], Loss: 2.0098\n",
      "Epoch [2/200], Step [40], Loss: 1.7889\n",
      "Training accuracy for epoch 2 is 63%\n",
      "Validation accuracy for epoch 2 is 60%\n",
      "Epoch [3/200], Step [10], Loss: 1.8436\n",
      "Epoch [3/200], Step [20], Loss: 1.9364\n",
      "Epoch [3/200], Step [30], Loss: 2.0330\n",
      "Epoch [3/200], Step [40], Loss: 1.9055\n",
      "Training accuracy for epoch 3 is 66%\n",
      "Validation accuracy for epoch 3 is 61%\n",
      "Epoch [4/200], Step [10], Loss: 1.5668\n",
      "Epoch [4/200], Step [20], Loss: 1.8057\n",
      "Epoch [4/200], Step [30], Loss: 1.7462\n",
      "Epoch [4/200], Step [40], Loss: 1.9238\n",
      "Training accuracy for epoch 4 is 68%\n",
      "Validation accuracy for epoch 4 is 62%\n",
      "Epoch [5/200], Step [10], Loss: 2.2156\n",
      "Epoch [5/200], Step [20], Loss: 1.6642\n",
      "Epoch [5/200], Step [30], Loss: 1.8063\n",
      "Epoch [5/200], Step [40], Loss: 1.7533\n",
      "Training accuracy for epoch 5 is 72%\n",
      "Validation accuracy for epoch 5 is 63%\n",
      "Epoch [6/200], Step [10], Loss: 1.7585\n",
      "Epoch [6/200], Step [20], Loss: 1.7081\n",
      "Epoch [6/200], Step [30], Loss: 1.5378\n",
      "Epoch [6/200], Step [40], Loss: 2.0398\n",
      "Training accuracy for epoch 6 is 74%\n",
      "Validation accuracy for epoch 6 is 64%\n",
      "Epoch [7/200], Step [10], Loss: 1.7306\n",
      "Epoch [7/200], Step [20], Loss: 1.5084\n",
      "Epoch [7/200], Step [30], Loss: 1.7247\n",
      "Epoch [7/200], Step [40], Loss: 1.6478\n",
      "Training accuracy for epoch 7 is 75%\n",
      "Validation accuracy for epoch 7 is 67%\n",
      "Epoch [8/200], Step [10], Loss: 1.8748\n",
      "Epoch [8/200], Step [20], Loss: 1.7785\n",
      "Epoch [8/200], Step [30], Loss: 1.9132\n",
      "Epoch [8/200], Step [40], Loss: 1.6440\n",
      "Training accuracy for epoch 8 is 76%\n",
      "Validation accuracy for epoch 8 is 68%\n",
      "Epoch [9/200], Step [10], Loss: 1.8173\n",
      "Epoch [9/200], Step [20], Loss: 1.9267\n",
      "Epoch [9/200], Step [30], Loss: 1.7808\n",
      "Epoch [9/200], Step [40], Loss: 1.5110\n",
      "Training accuracy for epoch 9 is 76%\n",
      "Validation accuracy for epoch 9 is 69%\n",
      "Epoch [10/200], Step [10], Loss: 1.8732\n",
      "Epoch [10/200], Step [20], Loss: 1.6636\n",
      "Epoch [10/200], Step [30], Loss: 1.6816\n",
      "Epoch [10/200], Step [40], Loss: 1.7588\n",
      "Training accuracy for epoch 10 is 76%\n",
      "Validation accuracy for epoch 10 is 67%\n",
      "Epoch [11/200], Step [10], Loss: 1.8388\n",
      "Epoch [11/200], Step [20], Loss: 1.9110\n",
      "Epoch [11/200], Step [30], Loss: 1.6871\n",
      "Epoch [11/200], Step [40], Loss: 1.7160\n",
      "Training accuracy for epoch 11 is 76%\n",
      "Validation accuracy for epoch 11 is 69%\n",
      "Epoch [12/200], Step [10], Loss: 1.8171\n",
      "Epoch [12/200], Step [20], Loss: 1.7687\n",
      "Epoch [12/200], Step [30], Loss: 1.5988\n",
      "Epoch [12/200], Step [40], Loss: 1.6191\n",
      "Training accuracy for epoch 12 is 78%\n",
      "Validation accuracy for epoch 12 is 69%\n",
      "Epoch [13/200], Step [10], Loss: 1.6968\n",
      "Epoch [13/200], Step [20], Loss: 1.6763\n",
      "Epoch [13/200], Step [30], Loss: 1.8362\n",
      "Epoch [13/200], Step [40], Loss: 1.9141\n",
      "Training accuracy for epoch 13 is 77%\n",
      "Validation accuracy for epoch 13 is 68%\n",
      "Epoch [14/200], Step [10], Loss: 1.6372\n",
      "Epoch [14/200], Step [20], Loss: 1.5598\n",
      "Epoch [14/200], Step [30], Loss: 1.8641\n",
      "Epoch [14/200], Step [40], Loss: 1.7042\n",
      "Training accuracy for epoch 14 is 78%\n",
      "Validation accuracy for epoch 14 is 68%\n",
      "Epoch [15/200], Step [10], Loss: 1.5918\n",
      "Epoch [15/200], Step [20], Loss: 1.7240\n",
      "Epoch [15/200], Step [30], Loss: 1.8551\n",
      "Epoch [15/200], Step [40], Loss: 1.8464\n",
      "Training accuracy for epoch 15 is 79%\n",
      "Validation accuracy for epoch 15 is 71%\n",
      "Epoch [16/200], Step [10], Loss: 1.6615\n",
      "Epoch [16/200], Step [20], Loss: 1.8749\n",
      "Epoch [16/200], Step [30], Loss: 1.7766\n",
      "Epoch [16/200], Step [40], Loss: 1.9025\n",
      "Training accuracy for epoch 16 is 80%\n",
      "Validation accuracy for epoch 16 is 67%\n",
      "Epoch [17/200], Step [10], Loss: 1.6306\n",
      "Epoch [17/200], Step [20], Loss: 1.8522\n",
      "Epoch [17/200], Step [30], Loss: 1.6633\n",
      "Epoch [17/200], Step [40], Loss: 1.6692\n",
      "Training accuracy for epoch 17 is 82%\n",
      "Validation accuracy for epoch 17 is 72%\n",
      "Epoch [18/200], Step [10], Loss: 1.5134\n",
      "Epoch [18/200], Step [20], Loss: 1.6261\n",
      "Epoch [18/200], Step [30], Loss: 1.9038\n",
      "Epoch [18/200], Step [40], Loss: 1.8294\n",
      "Training accuracy for epoch 18 is 83%\n",
      "Validation accuracy for epoch 18 is 68%\n",
      "Epoch [19/200], Step [10], Loss: 1.5793\n",
      "Epoch [19/200], Step [20], Loss: 1.9535\n",
      "Epoch [19/200], Step [30], Loss: 1.6173\n",
      "Epoch [19/200], Step [40], Loss: 1.6578\n",
      "Training accuracy for epoch 19 is 83%\n",
      "Validation accuracy for epoch 19 is 71%\n",
      "Epoch [20/200], Step [10], Loss: 1.6909\n",
      "Epoch [20/200], Step [20], Loss: 1.5674\n",
      "Epoch [20/200], Step [30], Loss: 1.8431\n",
      "Epoch [20/200], Step [40], Loss: 1.6634\n",
      "Training accuracy for epoch 20 is 84%\n",
      "Validation accuracy for epoch 20 is 71%\n",
      "Epoch [21/200], Step [10], Loss: 1.7776\n",
      "Epoch [21/200], Step [20], Loss: 1.4678\n",
      "Epoch [21/200], Step [30], Loss: 1.5989\n",
      "Epoch [21/200], Step [40], Loss: 1.7002\n",
      "Training accuracy for epoch 21 is 83%\n",
      "Validation accuracy for epoch 21 is 72%\n",
      "Epoch [22/200], Step [10], Loss: 1.6005\n",
      "Epoch [22/200], Step [20], Loss: 1.9482\n",
      "Epoch [22/200], Step [30], Loss: 1.5489\n",
      "Epoch [22/200], Step [40], Loss: 1.7259\n",
      "Training accuracy for epoch 22 is 84%\n",
      "Validation accuracy for epoch 22 is 71%\n",
      "Epoch [23/200], Step [10], Loss: 1.7518\n",
      "Epoch [23/200], Step [20], Loss: 1.8045\n",
      "Epoch [23/200], Step [30], Loss: 1.6131\n",
      "Epoch [23/200], Step [40], Loss: 1.8340\n",
      "Training accuracy for epoch 23 is 84%\n",
      "Validation accuracy for epoch 23 is 72%\n",
      "Epoch [24/200], Step [10], Loss: 1.5438\n",
      "Epoch [24/200], Step [20], Loss: 1.6350\n",
      "Epoch [24/200], Step [30], Loss: 1.5668\n",
      "Epoch [24/200], Step [40], Loss: 1.5855\n",
      "Training accuracy for epoch 24 is 84%\n",
      "Validation accuracy for epoch 24 is 70%\n",
      "Epoch [25/200], Step [10], Loss: 1.5960\n",
      "Epoch [25/200], Step [20], Loss: 1.5769\n",
      "Epoch [25/200], Step [30], Loss: 1.6749\n",
      "Epoch [25/200], Step [40], Loss: 1.6693\n",
      "Training accuracy for epoch 25 is 84%\n",
      "Validation accuracy for epoch 25 is 72%\n",
      "Epoch [26/200], Step [10], Loss: 1.5877\n",
      "Epoch [26/200], Step [20], Loss: 1.5632\n",
      "Epoch [26/200], Step [30], Loss: 1.6919\n",
      "Epoch [26/200], Step [40], Loss: 1.4642\n",
      "Training accuracy for epoch 26 is 84%\n",
      "Validation accuracy for epoch 26 is 71%\n",
      "Epoch [27/200], Step [10], Loss: 1.5809\n",
      "Epoch [27/200], Step [20], Loss: 1.8507\n",
      "Epoch [27/200], Step [30], Loss: 1.7428\n",
      "Epoch [27/200], Step [40], Loss: 1.8761\n",
      "Training accuracy for epoch 27 is 84%\n",
      "Validation accuracy for epoch 27 is 72%\n",
      "Epoch [28/200], Step [10], Loss: 1.6333\n",
      "Epoch [28/200], Step [20], Loss: 1.6744\n",
      "Epoch [28/200], Step [30], Loss: 1.5176\n",
      "Epoch [28/200], Step [40], Loss: 1.8886\n",
      "Training accuracy for epoch 28 is 85%\n",
      "Validation accuracy for epoch 28 is 72%\n",
      "Epoch [29/200], Step [10], Loss: 1.5104\n",
      "Epoch [29/200], Step [20], Loss: 1.7999\n",
      "Epoch [29/200], Step [30], Loss: 1.6306\n",
      "Epoch [29/200], Step [40], Loss: 1.6520\n",
      "Training accuracy for epoch 29 is 85%\n",
      "Validation accuracy for epoch 29 is 72%\n",
      "Epoch [30/200], Step [10], Loss: 1.5696\n",
      "Epoch [30/200], Step [20], Loss: 1.6589\n",
      "Epoch [30/200], Step [30], Loss: 1.6724\n",
      "Epoch [30/200], Step [40], Loss: 1.4880\n",
      "Training accuracy for epoch 30 is 85%\n",
      "Validation accuracy for epoch 30 is 72%\n",
      "Epoch [31/200], Step [10], Loss: 1.9511\n",
      "Epoch [31/200], Step [20], Loss: 1.4843\n",
      "Epoch [31/200], Step [30], Loss: 1.5461\n",
      "Epoch [31/200], Step [40], Loss: 1.6139\n",
      "Training accuracy for epoch 31 is 85%\n",
      "Validation accuracy for epoch 31 is 71%\n",
      "Epoch [32/200], Step [10], Loss: 1.5473\n",
      "Epoch [32/200], Step [20], Loss: 1.7583\n",
      "Epoch [32/200], Step [30], Loss: 1.5757\n",
      "Epoch [32/200], Step [40], Loss: 1.4728\n",
      "Training accuracy for epoch 32 is 86%\n",
      "Validation accuracy for epoch 32 is 73%\n",
      "Epoch [33/200], Step [10], Loss: 1.5670\n",
      "Epoch [33/200], Step [20], Loss: 1.6703\n",
      "Epoch [33/200], Step [30], Loss: 1.6828\n",
      "Epoch [33/200], Step [40], Loss: 1.6711\n",
      "Training accuracy for epoch 33 is 86%\n",
      "Validation accuracy for epoch 33 is 67%\n",
      "Epoch [34/200], Step [10], Loss: 1.7036\n",
      "Epoch [34/200], Step [20], Loss: 1.5791\n",
      "Epoch [34/200], Step [30], Loss: 1.5710\n",
      "Epoch [34/200], Step [40], Loss: 1.4932\n",
      "Training accuracy for epoch 34 is 85%\n",
      "Validation accuracy for epoch 34 is 71%\n",
      "Epoch [35/200], Step [10], Loss: 1.9461\n",
      "Epoch [35/200], Step [20], Loss: 1.4692\n",
      "Epoch [35/200], Step [30], Loss: 1.5528\n",
      "Epoch [35/200], Step [40], Loss: 1.6628\n",
      "Training accuracy for epoch 35 is 85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for epoch 35 is 73%\n",
      "Epoch [36/200], Step [10], Loss: 1.6211\n",
      "Epoch [36/200], Step [20], Loss: 1.5750\n",
      "Epoch [36/200], Step [30], Loss: 1.6558\n",
      "Epoch [36/200], Step [40], Loss: 1.5642\n",
      "Training accuracy for epoch 36 is 86%\n",
      "Validation accuracy for epoch 36 is 73%\n",
      "Epoch [37/200], Step [10], Loss: 1.6638\n",
      "Epoch [37/200], Step [20], Loss: 1.6747\n",
      "Epoch [37/200], Step [30], Loss: 1.5790\n",
      "Epoch [37/200], Step [40], Loss: 1.4757\n",
      "Training accuracy for epoch 37 is 86%\n",
      "Validation accuracy for epoch 37 is 71%\n",
      "Epoch [38/200], Step [10], Loss: 1.8370\n",
      "Epoch [38/200], Step [20], Loss: 1.5707\n",
      "Epoch [38/200], Step [30], Loss: 1.5749\n",
      "Epoch [38/200], Step [40], Loss: 1.5693\n",
      "Training accuracy for epoch 38 is 85%\n",
      "Validation accuracy for epoch 38 is 73%\n",
      "Epoch [39/200], Step [10], Loss: 1.4735\n",
      "Epoch [39/200], Step [20], Loss: 1.4703\n",
      "Epoch [39/200], Step [30], Loss: 1.7184\n",
      "Epoch [39/200], Step [40], Loss: 1.5606\n",
      "Training accuracy for epoch 39 is 86%\n",
      "Validation accuracy for epoch 39 is 71%\n",
      "Epoch [40/200], Step [10], Loss: 1.6558\n",
      "Epoch [40/200], Step [20], Loss: 1.4917\n",
      "Epoch [40/200], Step [30], Loss: 1.4939\n",
      "Epoch [40/200], Step [40], Loss: 1.6645\n",
      "Training accuracy for epoch 40 is 86%\n",
      "Validation accuracy for epoch 40 is 73%\n",
      "Epoch [41/200], Step [10], Loss: 1.5973\n",
      "Epoch [41/200], Step [20], Loss: 1.6684\n",
      "Epoch [41/200], Step [30], Loss: 1.8612\n",
      "Epoch [41/200], Step [40], Loss: 1.5728\n",
      "Training accuracy for epoch 41 is 86%\n",
      "Validation accuracy for epoch 41 is 72%\n",
      "Epoch [42/200], Step [10], Loss: 1.4748\n",
      "Epoch [42/200], Step [20], Loss: 1.4675\n",
      "Epoch [42/200], Step [30], Loss: 1.5819\n",
      "Epoch [42/200], Step [40], Loss: 1.5623\n",
      "Training accuracy for epoch 42 is 86%\n",
      "Validation accuracy for epoch 42 is 73%\n",
      "Epoch [43/200], Step [10], Loss: 1.5749\n",
      "Epoch [43/200], Step [20], Loss: 1.5657\n",
      "Epoch [43/200], Step [30], Loss: 1.5391\n",
      "Epoch [43/200], Step [40], Loss: 1.4680\n",
      "Training accuracy for epoch 43 is 86%\n",
      "Validation accuracy for epoch 43 is 73%\n",
      "Epoch [44/200], Step [10], Loss: 1.6471\n",
      "Epoch [44/200], Step [20], Loss: 1.5993\n",
      "Epoch [44/200], Step [30], Loss: 1.4960\n",
      "Epoch [44/200], Step [40], Loss: 1.6084\n",
      "Training accuracy for epoch 44 is 86%\n",
      "Validation accuracy for epoch 44 is 73%\n",
      "Epoch [45/200], Step [10], Loss: 1.4637\n",
      "Epoch [45/200], Step [20], Loss: 1.6543\n",
      "Epoch [45/200], Step [30], Loss: 1.6929\n",
      "Epoch [45/200], Step [40], Loss: 1.5692\n",
      "Training accuracy for epoch 45 is 86%\n",
      "Validation accuracy for epoch 45 is 71%\n",
      "Epoch [46/200], Step [10], Loss: 1.5658\n",
      "Epoch [46/200], Step [20], Loss: 1.5842\n",
      "Epoch [46/200], Step [30], Loss: 1.4675\n",
      "Epoch [46/200], Step [40], Loss: 1.6447\n",
      "Training accuracy for epoch 46 is 86%\n",
      "Validation accuracy for epoch 46 is 71%\n",
      "Epoch [47/200], Step [10], Loss: 1.6633\n",
      "Epoch [47/200], Step [20], Loss: 1.5650\n",
      "Epoch [47/200], Step [30], Loss: 1.5614\n",
      "Epoch [47/200], Step [40], Loss: 1.6692\n",
      "Training accuracy for epoch 47 is 86%\n",
      "Validation accuracy for epoch 47 is 72%\n",
      "Epoch [48/200], Step [10], Loss: 1.5811\n",
      "Epoch [48/200], Step [20], Loss: 1.7581\n",
      "Epoch [48/200], Step [30], Loss: 1.6634\n",
      "Epoch [48/200], Step [40], Loss: 1.5701\n",
      "Training accuracy for epoch 48 is 86%\n",
      "Validation accuracy for epoch 48 is 73%\n",
      "Epoch [49/200], Step [10], Loss: 1.5661\n",
      "Epoch [49/200], Step [20], Loss: 1.4651\n",
      "Epoch [49/200], Step [30], Loss: 1.4841\n",
      "Epoch [49/200], Step [40], Loss: 1.6635\n",
      "Training accuracy for epoch 49 is 86%\n",
      "Validation accuracy for epoch 49 is 73%\n",
      "Epoch [50/200], Step [10], Loss: 1.5600\n",
      "Epoch [50/200], Step [20], Loss: 1.5643\n",
      "Epoch [50/200], Step [30], Loss: 1.6581\n",
      "Epoch [50/200], Step [40], Loss: 1.6700\n",
      "Training accuracy for epoch 50 is 86%\n",
      "Validation accuracy for epoch 50 is 69%\n",
      "Epoch [51/200], Step [10], Loss: 1.7606\n",
      "Epoch [51/200], Step [20], Loss: 1.5701\n",
      "Epoch [51/200], Step [30], Loss: 1.9526\n",
      "Epoch [51/200], Step [40], Loss: 1.5562\n",
      "Training accuracy for epoch 51 is 86%\n",
      "Validation accuracy for epoch 51 is 73%\n",
      "Epoch [52/200], Step [10], Loss: 1.4636\n",
      "Epoch [52/200], Step [20], Loss: 1.6570\n",
      "Epoch [52/200], Step [30], Loss: 1.6448\n",
      "Epoch [52/200], Step [40], Loss: 1.6238\n",
      "Training accuracy for epoch 52 is 86%\n",
      "Validation accuracy for epoch 52 is 72%\n",
      "Epoch [53/200], Step [10], Loss: 1.5018\n",
      "Epoch [53/200], Step [20], Loss: 1.4730\n",
      "Epoch [53/200], Step [30], Loss: 1.4783\n",
      "Epoch [53/200], Step [40], Loss: 1.5883\n",
      "Training accuracy for epoch 53 is 86%\n",
      "Validation accuracy for epoch 53 is 73%\n",
      "Epoch [54/200], Step [10], Loss: 1.6621\n",
      "Epoch [54/200], Step [20], Loss: 1.7561\n",
      "Epoch [54/200], Step [30], Loss: 1.6654\n",
      "Epoch [54/200], Step [40], Loss: 1.4656\n",
      "Training accuracy for epoch 54 is 86%\n",
      "Validation accuracy for epoch 54 is 73%\n",
      "Epoch [55/200], Step [10], Loss: 1.5437\n",
      "Epoch [55/200], Step [20], Loss: 1.5623\n",
      "Epoch [55/200], Step [30], Loss: 1.5669\n",
      "Epoch [55/200], Step [40], Loss: 1.6642\n",
      "Training accuracy for epoch 55 is 86%\n",
      "Validation accuracy for epoch 55 is 72%\n",
      "Epoch [56/200], Step [10], Loss: 1.5641\n",
      "Epoch [56/200], Step [20], Loss: 1.6857\n",
      "Epoch [56/200], Step [30], Loss: 1.6615\n",
      "Epoch [56/200], Step [40], Loss: 1.7574\n",
      "Training accuracy for epoch 56 is 86%\n",
      "Validation accuracy for epoch 56 is 73%\n",
      "Epoch [57/200], Step [10], Loss: 1.7314\n",
      "Epoch [57/200], Step [20], Loss: 1.6670\n",
      "Epoch [57/200], Step [30], Loss: 1.5642\n",
      "Epoch [57/200], Step [40], Loss: 1.5592\n",
      "Training accuracy for epoch 57 is 86%\n",
      "Validation accuracy for epoch 57 is 73%\n",
      "Epoch [58/200], Step [10], Loss: 1.6685\n",
      "Epoch [58/200], Step [20], Loss: 1.7600\n",
      "Epoch [58/200], Step [30], Loss: 1.5622\n",
      "Epoch [58/200], Step [40], Loss: 1.5712\n",
      "Training accuracy for epoch 58 is 86%\n",
      "Validation accuracy for epoch 58 is 72%\n",
      "Epoch [59/200], Step [10], Loss: 1.4747\n",
      "Epoch [59/200], Step [20], Loss: 1.5619\n",
      "Epoch [59/200], Step [30], Loss: 1.7688\n",
      "Epoch [59/200], Step [40], Loss: 1.6672\n",
      "Training accuracy for epoch 59 is 86%\n",
      "Validation accuracy for epoch 59 is 73%\n",
      "Epoch [60/200], Step [10], Loss: 1.5617\n",
      "Epoch [60/200], Step [20], Loss: 1.4637\n",
      "Epoch [60/200], Step [30], Loss: 1.4669\n",
      "Epoch [60/200], Step [40], Loss: 1.8547\n",
      "Training accuracy for epoch 60 is 86%\n",
      "Validation accuracy for epoch 60 is 73%\n",
      "Epoch [61/200], Step [10], Loss: 1.5687\n",
      "Epoch [61/200], Step [20], Loss: 1.5667\n",
      "Epoch [61/200], Step [30], Loss: 1.4636\n",
      "Epoch [61/200], Step [40], Loss: 1.5650\n",
      "Training accuracy for epoch 61 is 86%\n",
      "Validation accuracy for epoch 61 is 72%\n",
      "Epoch [62/200], Step [10], Loss: 1.4744\n",
      "Epoch [62/200], Step [20], Loss: 1.4646\n",
      "Epoch [62/200], Step [30], Loss: 1.6626\n",
      "Epoch [62/200], Step [40], Loss: 1.8546\n",
      "Training accuracy for epoch 62 is 86%\n",
      "Validation accuracy for epoch 62 is 73%\n",
      "Epoch [63/200], Step [10], Loss: 1.5698\n",
      "Epoch [63/200], Step [20], Loss: 1.5867\n",
      "Epoch [63/200], Step [30], Loss: 1.5635\n",
      "Epoch [63/200], Step [40], Loss: 1.5726\n",
      "Training accuracy for epoch 63 is 86%\n",
      "Validation accuracy for epoch 63 is 73%\n",
      "Epoch [64/200], Step [10], Loss: 1.5587\n",
      "Epoch [64/200], Step [20], Loss: 1.6662\n",
      "Epoch [64/200], Step [30], Loss: 1.5874\n",
      "Epoch [64/200], Step [40], Loss: 1.5642\n",
      "Training accuracy for epoch 64 is 86%\n",
      "Validation accuracy for epoch 64 is 73%\n",
      "Epoch [65/200], Step [10], Loss: 1.6618\n",
      "Epoch [65/200], Step [20], Loss: 1.4634\n",
      "Epoch [65/200], Step [30], Loss: 1.5597\n",
      "Epoch [65/200], Step [40], Loss: 1.7576\n",
      "Training accuracy for epoch 65 is 86%\n",
      "Validation accuracy for epoch 65 is 73%\n",
      "Epoch [66/200], Step [10], Loss: 1.6639\n",
      "Epoch [66/200], Step [20], Loss: 1.6604\n",
      "Epoch [66/200], Step [30], Loss: 1.8535\n",
      "Epoch [66/200], Step [40], Loss: 1.4695\n",
      "Training accuracy for epoch 66 is 86%\n",
      "Validation accuracy for epoch 66 is 73%\n",
      "Epoch [67/200], Step [10], Loss: 1.5621\n",
      "Epoch [67/200], Step [20], Loss: 1.7575\n",
      "Epoch [67/200], Step [30], Loss: 1.7653\n",
      "Epoch [67/200], Step [40], Loss: 1.6490\n",
      "Training accuracy for epoch 67 is 86%\n",
      "Validation accuracy for epoch 67 is 73%\n",
      "Epoch [68/200], Step [10], Loss: 1.6642\n",
      "Epoch [68/200], Step [20], Loss: 1.4665\n",
      "Epoch [68/200], Step [30], Loss: 1.5728\n",
      "Epoch [68/200], Step [40], Loss: 1.5682\n",
      "Training accuracy for epoch 68 is 86%\n",
      "Validation accuracy for epoch 68 is 73%\n",
      "Epoch [69/200], Step [10], Loss: 1.5670\n",
      "Epoch [69/200], Step [20], Loss: 1.5612\n",
      "Epoch [69/200], Step [30], Loss: 1.6623\n",
      "Epoch [69/200], Step [40], Loss: 1.4742\n",
      "Training accuracy for epoch 69 is 86%\n",
      "Validation accuracy for epoch 69 is 72%\n",
      "Epoch [70/200], Step [10], Loss: 1.4720\n",
      "Epoch [70/200], Step [20], Loss: 1.4675\n",
      "Epoch [70/200], Step [30], Loss: 1.5628\n",
      "Epoch [70/200], Step [40], Loss: 1.5647\n",
      "Training accuracy for epoch 70 is 86%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for epoch 70 is 73%\n",
      "Epoch [71/200], Step [10], Loss: 1.6617\n",
      "Epoch [71/200], Step [20], Loss: 1.5724\n",
      "Epoch [71/200], Step [30], Loss: 1.5698\n",
      "Epoch [71/200], Step [40], Loss: 1.8574\n",
      "Training accuracy for epoch 71 is 86%\n",
      "Validation accuracy for epoch 71 is 73%\n",
      "Epoch [72/200], Step [10], Loss: 1.7589\n",
      "Epoch [72/200], Step [20], Loss: 1.7632\n",
      "Epoch [72/200], Step [30], Loss: 1.5602\n",
      "Epoch [72/200], Step [40], Loss: 1.7592\n",
      "Training accuracy for epoch 72 is 86%\n",
      "Validation accuracy for epoch 72 is 73%\n",
      "Epoch [73/200], Step [10], Loss: 1.6615\n",
      "Epoch [73/200], Step [20], Loss: 1.6711\n",
      "Epoch [73/200], Step [30], Loss: 1.7570\n",
      "Epoch [73/200], Step [40], Loss: 1.5661\n",
      "Training accuracy for epoch 73 is 86%\n",
      "Validation accuracy for epoch 73 is 73%\n",
      "Epoch [74/200], Step [10], Loss: 1.6613\n",
      "Epoch [74/200], Step [20], Loss: 1.5639\n",
      "Epoch [74/200], Step [30], Loss: 1.7580\n",
      "Epoch [74/200], Step [40], Loss: 1.6611\n",
      "Training accuracy for epoch 74 is 86%\n",
      "Validation accuracy for epoch 74 is 73%\n",
      "Epoch [75/200], Step [10], Loss: 1.9521\n",
      "Epoch [75/200], Step [20], Loss: 1.4626\n",
      "Epoch [75/200], Step [30], Loss: 1.7615\n",
      "Epoch [75/200], Step [40], Loss: 1.8604\n",
      "Training accuracy for epoch 75 is 86%\n",
      "Validation accuracy for epoch 75 is 73%\n",
      "Epoch [76/200], Step [10], Loss: 1.5652\n",
      "Epoch [76/200], Step [20], Loss: 1.5585\n",
      "Epoch [76/200], Step [30], Loss: 1.5672\n",
      "Epoch [76/200], Step [40], Loss: 1.5643\n",
      "Training accuracy for epoch 76 is 86%\n",
      "Validation accuracy for epoch 76 is 73%\n",
      "Epoch [77/200], Step [10], Loss: 1.4667\n",
      "Epoch [77/200], Step [20], Loss: 1.4636\n",
      "Epoch [77/200], Step [30], Loss: 1.6619\n",
      "Epoch [77/200], Step [40], Loss: 1.6596\n",
      "Training accuracy for epoch 77 is 86%\n",
      "Validation accuracy for epoch 77 is 72%\n",
      "Epoch [78/200], Step [10], Loss: 1.5629\n",
      "Epoch [78/200], Step [20], Loss: 1.5552\n",
      "Epoch [78/200], Step [30], Loss: 1.6640\n",
      "Epoch [78/200], Step [40], Loss: 1.6588\n",
      "Training accuracy for epoch 78 is 86%\n",
      "Validation accuracy for epoch 78 is 73%\n",
      "Epoch [79/200], Step [10], Loss: 1.7592\n",
      "Epoch [79/200], Step [20], Loss: 1.6599\n",
      "Epoch [79/200], Step [30], Loss: 1.6634\n",
      "Epoch [79/200], Step [40], Loss: 1.8631\n",
      "Training accuracy for epoch 79 is 86%\n",
      "Validation accuracy for epoch 79 is 73%\n",
      "Epoch [80/200], Step [10], Loss: 1.5629\n",
      "Epoch [80/200], Step [20], Loss: 1.5730\n",
      "Epoch [80/200], Step [30], Loss: 1.5644\n",
      "Epoch [80/200], Step [40], Loss: 1.4716\n",
      "Training accuracy for epoch 80 is 86%\n",
      "Validation accuracy for epoch 80 is 73%\n",
      "Epoch [81/200], Step [10], Loss: 1.5601\n",
      "Epoch [81/200], Step [20], Loss: 1.6658\n",
      "Epoch [81/200], Step [30], Loss: 1.6607\n",
      "Epoch [81/200], Step [40], Loss: 1.6629\n",
      "Training accuracy for epoch 81 is 86%\n",
      "Validation accuracy for epoch 81 is 73%\n",
      "Epoch [82/200], Step [10], Loss: 1.6569\n",
      "Epoch [82/200], Step [20], Loss: 1.7580\n",
      "Epoch [82/200], Step [30], Loss: 1.5672\n",
      "Epoch [82/200], Step [40], Loss: 1.4620\n",
      "Training accuracy for epoch 82 is 86%\n",
      "Validation accuracy for epoch 82 is 73%\n",
      "Epoch [83/200], Step [10], Loss: 1.6638\n",
      "Epoch [83/200], Step [20], Loss: 1.5609\n",
      "Epoch [83/200], Step [30], Loss: 1.5663\n",
      "Epoch [83/200], Step [40], Loss: 1.6590\n",
      "Training accuracy for epoch 83 is 86%\n",
      "Validation accuracy for epoch 83 is 73%\n",
      "Epoch [84/200], Step [10], Loss: 1.4631\n",
      "Epoch [84/200], Step [20], Loss: 1.6630\n",
      "Epoch [84/200], Step [30], Loss: 1.5638\n",
      "Epoch [84/200], Step [40], Loss: 1.5613\n",
      "Training accuracy for epoch 84 is 86%\n",
      "Validation accuracy for epoch 84 is 73%\n",
      "Epoch [85/200], Step [10], Loss: 1.6557\n",
      "Epoch [85/200], Step [20], Loss: 1.4637\n",
      "Epoch [85/200], Step [30], Loss: 1.4647\n",
      "Epoch [85/200], Step [40], Loss: 1.6621\n",
      "Training accuracy for epoch 85 is 86%\n",
      "Validation accuracy for epoch 85 is 73%\n",
      "Epoch [86/200], Step [10], Loss: 1.5597\n",
      "Epoch [86/200], Step [20], Loss: 1.5634\n",
      "Epoch [86/200], Step [30], Loss: 1.5592\n",
      "Epoch [86/200], Step [40], Loss: 1.4668\n",
      "Training accuracy for epoch 86 is 86%\n",
      "Validation accuracy for epoch 86 is 73%\n",
      "Epoch [87/200], Step [10], Loss: 1.4624\n",
      "Epoch [87/200], Step [20], Loss: 1.4639\n",
      "Epoch [87/200], Step [30], Loss: 1.8544\n",
      "Epoch [87/200], Step [40], Loss: 1.6591\n",
      "Training accuracy for epoch 87 is 86%\n",
      "Validation accuracy for epoch 87 is 73%\n",
      "Epoch [88/200], Step [10], Loss: 1.4629\n",
      "Epoch [88/200], Step [20], Loss: 1.5652\n",
      "Epoch [88/200], Step [30], Loss: 1.6575\n",
      "Epoch [88/200], Step [40], Loss: 1.4720\n",
      "Training accuracy for epoch 88 is 86%\n",
      "Validation accuracy for epoch 88 is 73%\n",
      "Epoch [89/200], Step [10], Loss: 1.5654\n",
      "Epoch [89/200], Step [20], Loss: 1.7660\n",
      "Epoch [89/200], Step [30], Loss: 1.6616\n",
      "Epoch [89/200], Step [40], Loss: 1.7632\n",
      "Training accuracy for epoch 89 is 86%\n",
      "Validation accuracy for epoch 89 is 73%\n",
      "Epoch [90/200], Step [10], Loss: 1.4623\n",
      "Epoch [90/200], Step [20], Loss: 1.6602\n",
      "Epoch [90/200], Step [30], Loss: 1.4683\n",
      "Epoch [90/200], Step [40], Loss: 1.6592\n",
      "Training accuracy for epoch 90 is 86%\n",
      "Validation accuracy for epoch 90 is 73%\n",
      "Epoch [91/200], Step [10], Loss: 1.7580\n",
      "Epoch [91/200], Step [20], Loss: 1.7649\n",
      "Epoch [91/200], Step [30], Loss: 1.4658\n",
      "Epoch [91/200], Step [40], Loss: 1.5618\n",
      "Training accuracy for epoch 91 is 86%\n",
      "Validation accuracy for epoch 91 is 73%\n",
      "Epoch [92/200], Step [10], Loss: 1.5621\n",
      "Epoch [92/200], Step [20], Loss: 1.4648\n",
      "Epoch [92/200], Step [30], Loss: 1.5638\n",
      "Epoch [92/200], Step [40], Loss: 1.8563\n",
      "Training accuracy for epoch 92 is 86%\n",
      "Validation accuracy for epoch 92 is 73%\n",
      "Epoch [93/200], Step [10], Loss: 1.5699\n",
      "Epoch [93/200], Step [20], Loss: 1.4701\n",
      "Epoch [93/200], Step [30], Loss: 1.7639\n",
      "Epoch [93/200], Step [40], Loss: 1.8597\n",
      "Training accuracy for epoch 93 is 86%\n",
      "Validation accuracy for epoch 93 is 73%\n",
      "Epoch [94/200], Step [10], Loss: 1.6606\n",
      "Epoch [94/200], Step [20], Loss: 1.6669\n",
      "Epoch [94/200], Step [30], Loss: 1.5629\n",
      "Epoch [94/200], Step [40], Loss: 1.5679\n",
      "Training accuracy for epoch 94 is 86%\n",
      "Validation accuracy for epoch 94 is 73%\n",
      "Epoch [95/200], Step [10], Loss: 1.5615\n",
      "Epoch [95/200], Step [20], Loss: 1.6653\n",
      "Epoch [95/200], Step [30], Loss: 1.4626\n",
      "Epoch [95/200], Step [40], Loss: 1.7593\n",
      "Training accuracy for epoch 95 is 86%\n",
      "Validation accuracy for epoch 95 is 73%\n",
      "Epoch [96/200], Step [10], Loss: 1.8642\n",
      "Epoch [96/200], Step [20], Loss: 1.5729\n",
      "Epoch [96/200], Step [30], Loss: 1.7557\n",
      "Epoch [96/200], Step [40], Loss: 1.4626\n",
      "Training accuracy for epoch 96 is 86%\n",
      "Validation accuracy for epoch 96 is 73%\n",
      "Epoch [97/200], Step [10], Loss: 1.4620\n",
      "Epoch [97/200], Step [20], Loss: 1.4662\n",
      "Epoch [97/200], Step [30], Loss: 1.4630\n",
      "Epoch [97/200], Step [40], Loss: 1.5638\n",
      "Training accuracy for epoch 97 is 86%\n",
      "Validation accuracy for epoch 97 is 73%\n",
      "Epoch [98/200], Step [10], Loss: 1.6630\n",
      "Epoch [98/200], Step [20], Loss: 1.5618\n",
      "Epoch [98/200], Step [30], Loss: 1.6658\n",
      "Epoch [98/200], Step [40], Loss: 1.4709\n",
      "Training accuracy for epoch 98 is 86%\n",
      "Validation accuracy for epoch 98 is 73%\n",
      "Epoch [99/200], Step [10], Loss: 1.6625\n",
      "Epoch [99/200], Step [20], Loss: 1.4630\n",
      "Epoch [99/200], Step [30], Loss: 1.8563\n",
      "Epoch [99/200], Step [40], Loss: 1.5632\n",
      "Training accuracy for epoch 99 is 86%\n",
      "Validation accuracy for epoch 99 is 73%\n",
      "Epoch [100/200], Step [10], Loss: 1.5650\n",
      "Epoch [100/200], Step [20], Loss: 1.5589\n",
      "Epoch [100/200], Step [30], Loss: 1.5619\n",
      "Epoch [100/200], Step [40], Loss: 1.5682\n",
      "Training accuracy for epoch 100 is 86%\n",
      "Validation accuracy for epoch 100 is 73%\n",
      "Epoch [101/200], Step [10], Loss: 1.5657\n",
      "Epoch [101/200], Step [20], Loss: 1.4639\n",
      "Epoch [101/200], Step [30], Loss: 1.6619\n",
      "Epoch [101/200], Step [40], Loss: 1.4618\n",
      "Training accuracy for epoch 101 is 86%\n",
      "Validation accuracy for epoch 101 is 73%\n",
      "Epoch [102/200], Step [10], Loss: 1.4642\n",
      "Epoch [102/200], Step [20], Loss: 1.6613\n",
      "Epoch [102/200], Step [30], Loss: 1.5693\n",
      "Epoch [102/200], Step [40], Loss: 1.5594\n",
      "Training accuracy for epoch 102 is 86%\n",
      "Validation accuracy for epoch 102 is 73%\n",
      "Epoch [103/200], Step [10], Loss: 1.7572\n",
      "Epoch [103/200], Step [20], Loss: 1.5619\n",
      "Epoch [103/200], Step [30], Loss: 1.5615\n",
      "Epoch [103/200], Step [40], Loss: 1.5601\n",
      "Training accuracy for epoch 103 is 86%\n",
      "Validation accuracy for epoch 103 is 73%\n",
      "Epoch [104/200], Step [10], Loss: 1.5657\n",
      "Epoch [104/200], Step [20], Loss: 1.5593\n",
      "Epoch [104/200], Step [30], Loss: 1.4667\n",
      "Epoch [104/200], Step [40], Loss: 1.6593\n",
      "Training accuracy for epoch 104 is 86%\n",
      "Validation accuracy for epoch 104 is 73%\n",
      "Epoch [105/200], Step [10], Loss: 1.5592\n",
      "Epoch [105/200], Step [20], Loss: 1.4659\n",
      "Epoch [105/200], Step [30], Loss: 1.7587\n",
      "Epoch [105/200], Step [40], Loss: 1.5591\n",
      "Training accuracy for epoch 105 is 87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for epoch 105 is 73%\n",
      "Epoch [106/200], Step [10], Loss: 1.4620\n",
      "Epoch [106/200], Step [20], Loss: 1.6615\n",
      "Epoch [106/200], Step [30], Loss: 1.4647\n",
      "Epoch [106/200], Step [40], Loss: 1.7597\n",
      "Training accuracy for epoch 106 is 87%\n",
      "Validation accuracy for epoch 106 is 73%\n",
      "Epoch [107/200], Step [10], Loss: 1.5643\n",
      "Epoch [107/200], Step [20], Loss: 1.5620\n",
      "Epoch [107/200], Step [30], Loss: 1.6591\n",
      "Epoch [107/200], Step [40], Loss: 1.4636\n",
      "Training accuracy for epoch 107 is 87%\n",
      "Validation accuracy for epoch 107 is 73%\n",
      "Epoch [108/200], Step [10], Loss: 1.5623\n",
      "Epoch [108/200], Step [20], Loss: 1.6585\n",
      "Epoch [108/200], Step [30], Loss: 1.5581\n",
      "Epoch [108/200], Step [40], Loss: 1.5623\n",
      "Training accuracy for epoch 108 is 87%\n",
      "Validation accuracy for epoch 108 is 73%\n",
      "Epoch [109/200], Step [10], Loss: 1.4635\n",
      "Epoch [109/200], Step [20], Loss: 1.5613\n",
      "Epoch [109/200], Step [30], Loss: 1.4636\n",
      "Epoch [109/200], Step [40], Loss: 1.6601\n",
      "Training accuracy for epoch 109 is 87%\n",
      "Validation accuracy for epoch 109 is 73%\n",
      "Epoch [110/200], Step [10], Loss: 1.5614\n",
      "Epoch [110/200], Step [20], Loss: 1.5606\n",
      "Epoch [110/200], Step [30], Loss: 1.7574\n",
      "Epoch [110/200], Step [40], Loss: 1.4624\n",
      "Training accuracy for epoch 110 is 87%\n",
      "Validation accuracy for epoch 110 is 73%\n",
      "Epoch [111/200], Step [10], Loss: 1.7530\n",
      "Epoch [111/200], Step [20], Loss: 1.5582\n",
      "Epoch [111/200], Step [30], Loss: 1.5659\n",
      "Epoch [111/200], Step [40], Loss: 1.5646\n",
      "Training accuracy for epoch 111 is 87%\n",
      "Validation accuracy for epoch 111 is 73%\n",
      "Epoch [112/200], Step [10], Loss: 1.6602\n",
      "Epoch [112/200], Step [20], Loss: 1.6598\n",
      "Epoch [112/200], Step [30], Loss: 1.4615\n",
      "Epoch [112/200], Step [40], Loss: 1.5637\n",
      "Training accuracy for epoch 112 is 87%\n",
      "Validation accuracy for epoch 112 is 73%\n",
      "Epoch [113/200], Step [10], Loss: 1.4631\n",
      "Epoch [113/200], Step [20], Loss: 1.6563\n",
      "Epoch [113/200], Step [30], Loss: 1.5626\n",
      "Epoch [113/200], Step [40], Loss: 1.5613\n",
      "Training accuracy for epoch 113 is 87%\n",
      "Validation accuracy for epoch 113 is 73%\n",
      "Epoch [114/200], Step [10], Loss: 1.5593\n",
      "Epoch [114/200], Step [20], Loss: 1.6615\n",
      "Epoch [114/200], Step [30], Loss: 1.5599\n",
      "Epoch [114/200], Step [40], Loss: 1.4651\n",
      "Training accuracy for epoch 114 is 87%\n",
      "Validation accuracy for epoch 114 is 73%\n",
      "Epoch [115/200], Step [10], Loss: 1.7581\n",
      "Epoch [115/200], Step [20], Loss: 1.4642\n",
      "Epoch [115/200], Step [30], Loss: 1.6569\n",
      "Epoch [115/200], Step [40], Loss: 1.4630\n",
      "Training accuracy for epoch 115 is 87%\n",
      "Validation accuracy for epoch 115 is 73%\n",
      "Epoch [116/200], Step [10], Loss: 1.4663\n",
      "Epoch [116/200], Step [20], Loss: 1.4654\n",
      "Epoch [116/200], Step [30], Loss: 1.6579\n",
      "Epoch [116/200], Step [40], Loss: 1.4649\n",
      "Training accuracy for epoch 116 is 87%\n",
      "Validation accuracy for epoch 116 is 73%\n",
      "Epoch [117/200], Step [10], Loss: 1.6543\n",
      "Epoch [117/200], Step [20], Loss: 1.5631\n",
      "Epoch [117/200], Step [30], Loss: 1.4644\n",
      "Epoch [117/200], Step [40], Loss: 1.6613\n",
      "Training accuracy for epoch 117 is 87%\n",
      "Validation accuracy for epoch 117 is 73%\n",
      "Epoch [118/200], Step [10], Loss: 1.4694\n",
      "Epoch [118/200], Step [20], Loss: 1.6611\n",
      "Epoch [118/200], Step [30], Loss: 1.5662\n",
      "Epoch [118/200], Step [40], Loss: 1.6549\n",
      "Training accuracy for epoch 118 is 92%\n",
      "Validation accuracy for epoch 118 is 75%\n",
      "Epoch [119/200], Step [10], Loss: 1.6343\n",
      "Epoch [119/200], Step [20], Loss: 1.4749\n",
      "Epoch [119/200], Step [30], Loss: 1.6715\n",
      "Epoch [119/200], Step [40], Loss: 1.4659\n",
      "Training accuracy for epoch 119 is 95%\n",
      "Validation accuracy for epoch 119 is 72%\n",
      "Epoch [120/200], Step [10], Loss: 1.6062\n",
      "Epoch [120/200], Step [20], Loss: 1.5668\n",
      "Epoch [120/200], Step [30], Loss: 1.6699\n",
      "Epoch [120/200], Step [40], Loss: 1.6488\n",
      "Training accuracy for epoch 120 is 94%\n",
      "Validation accuracy for epoch 120 is 74%\n",
      "Epoch [121/200], Step [10], Loss: 1.7280\n",
      "Epoch [121/200], Step [20], Loss: 1.5714\n",
      "Epoch [121/200], Step [30], Loss: 1.4709\n",
      "Epoch [121/200], Step [40], Loss: 1.5746\n",
      "Training accuracy for epoch 121 is 94%\n",
      "Validation accuracy for epoch 121 is 75%\n",
      "Epoch [122/200], Step [10], Loss: 1.4791\n",
      "Epoch [122/200], Step [20], Loss: 1.7197\n",
      "Epoch [122/200], Step [30], Loss: 1.5736\n",
      "Epoch [122/200], Step [40], Loss: 1.4663\n",
      "Training accuracy for epoch 122 is 94%\n",
      "Validation accuracy for epoch 122 is 77%\n",
      "Epoch [123/200], Step [10], Loss: 1.4613\n",
      "Epoch [123/200], Step [20], Loss: 1.5602\n",
      "Epoch [123/200], Step [30], Loss: 1.5138\n",
      "Epoch [123/200], Step [40], Loss: 1.5892\n",
      "Training accuracy for epoch 123 is 94%\n",
      "Validation accuracy for epoch 123 is 76%\n",
      "Epoch [124/200], Step [10], Loss: 1.4787\n",
      "Epoch [124/200], Step [20], Loss: 1.5637\n",
      "Epoch [124/200], Step [30], Loss: 1.5050\n",
      "Epoch [124/200], Step [40], Loss: 1.4984\n",
      "Training accuracy for epoch 124 is 95%\n",
      "Validation accuracy for epoch 124 is 77%\n",
      "Epoch [125/200], Step [10], Loss: 1.4622\n",
      "Epoch [125/200], Step [20], Loss: 1.4629\n",
      "Epoch [125/200], Step [30], Loss: 1.5616\n",
      "Epoch [125/200], Step [40], Loss: 1.4629\n",
      "Training accuracy for epoch 125 is 95%\n",
      "Validation accuracy for epoch 125 is 75%\n",
      "Epoch [126/200], Step [10], Loss: 1.4826\n",
      "Epoch [126/200], Step [20], Loss: 1.5634\n",
      "Epoch [126/200], Step [30], Loss: 1.4690\n",
      "Epoch [126/200], Step [40], Loss: 1.4640\n",
      "Training accuracy for epoch 126 is 95%\n",
      "Validation accuracy for epoch 126 is 76%\n",
      "Epoch [127/200], Step [10], Loss: 1.4649\n",
      "Epoch [127/200], Step [20], Loss: 1.4684\n",
      "Epoch [127/200], Step [30], Loss: 1.4729\n",
      "Epoch [127/200], Step [40], Loss: 1.4668\n",
      "Training accuracy for epoch 127 is 95%\n",
      "Validation accuracy for epoch 127 is 75%\n",
      "Epoch [128/200], Step [10], Loss: 1.6617\n",
      "Epoch [128/200], Step [20], Loss: 1.5654\n",
      "Epoch [128/200], Step [30], Loss: 1.4626\n",
      "Epoch [128/200], Step [40], Loss: 1.4754\n",
      "Training accuracy for epoch 128 is 95%\n",
      "Validation accuracy for epoch 128 is 77%\n",
      "Epoch [129/200], Step [10], Loss: 1.4673\n",
      "Epoch [129/200], Step [20], Loss: 1.4651\n",
      "Epoch [129/200], Step [30], Loss: 1.4636\n",
      "Epoch [129/200], Step [40], Loss: 1.5727\n",
      "Training accuracy for epoch 129 is 95%\n",
      "Validation accuracy for epoch 129 is 77%\n",
      "Epoch [130/200], Step [10], Loss: 1.5602\n",
      "Epoch [130/200], Step [20], Loss: 1.6580\n",
      "Epoch [130/200], Step [30], Loss: 1.4629\n",
      "Epoch [130/200], Step [40], Loss: 1.4670\n",
      "Training accuracy for epoch 130 is 95%\n",
      "Validation accuracy for epoch 130 is 77%\n",
      "Epoch [131/200], Step [10], Loss: 1.4638\n",
      "Epoch [131/200], Step [20], Loss: 1.4691\n",
      "Epoch [131/200], Step [30], Loss: 1.4645\n",
      "Epoch [131/200], Step [40], Loss: 1.5008\n",
      "Training accuracy for epoch 131 is 95%\n",
      "Validation accuracy for epoch 131 is 77%\n",
      "Epoch [132/200], Step [10], Loss: 1.5668\n",
      "Epoch [132/200], Step [20], Loss: 1.4767\n",
      "Epoch [132/200], Step [30], Loss: 1.4908\n",
      "Epoch [132/200], Step [40], Loss: 1.5656\n",
      "Training accuracy for epoch 132 is 95%\n",
      "Validation accuracy for epoch 132 is 77%\n",
      "Epoch [133/200], Step [10], Loss: 1.4683\n",
      "Epoch [133/200], Step [20], Loss: 1.5804\n",
      "Epoch [133/200], Step [30], Loss: 1.4707\n",
      "Epoch [133/200], Step [40], Loss: 1.5630\n",
      "Training accuracy for epoch 133 is 95%\n",
      "Validation accuracy for epoch 133 is 77%\n",
      "Epoch [134/200], Step [10], Loss: 1.4635\n",
      "Epoch [134/200], Step [20], Loss: 1.4673\n",
      "Epoch [134/200], Step [30], Loss: 1.4716\n",
      "Epoch [134/200], Step [40], Loss: 1.4726\n",
      "Training accuracy for epoch 134 is 95%\n",
      "Validation accuracy for epoch 134 is 74%\n",
      "Epoch [135/200], Step [10], Loss: 1.4626\n",
      "Epoch [135/200], Step [20], Loss: 1.5769\n",
      "Epoch [135/200], Step [30], Loss: 1.4633\n",
      "Epoch [135/200], Step [40], Loss: 1.5591\n",
      "Training accuracy for epoch 135 is 95%\n",
      "Validation accuracy for epoch 135 is 76%\n",
      "Epoch [136/200], Step [10], Loss: 1.5557\n",
      "Epoch [136/200], Step [20], Loss: 1.4756\n",
      "Epoch [136/200], Step [30], Loss: 1.4633\n",
      "Epoch [136/200], Step [40], Loss: 1.4650\n",
      "Training accuracy for epoch 136 is 95%\n",
      "Validation accuracy for epoch 136 is 77%\n",
      "Epoch [137/200], Step [10], Loss: 1.5571\n",
      "Epoch [137/200], Step [20], Loss: 1.6103\n",
      "Epoch [137/200], Step [30], Loss: 1.4624\n",
      "Epoch [137/200], Step [40], Loss: 1.5701\n",
      "Training accuracy for epoch 137 is 95%\n",
      "Validation accuracy for epoch 137 is 77%\n",
      "Epoch [138/200], Step [10], Loss: 1.5627\n",
      "Epoch [138/200], Step [20], Loss: 1.4808\n",
      "Epoch [138/200], Step [30], Loss: 1.4795\n",
      "Epoch [138/200], Step [40], Loss: 1.5728\n",
      "Training accuracy for epoch 138 is 95%\n",
      "Validation accuracy for epoch 138 is 77%\n",
      "Epoch [139/200], Step [10], Loss: 1.4623\n",
      "Epoch [139/200], Step [20], Loss: 1.4638\n",
      "Epoch [139/200], Step [30], Loss: 1.4620\n",
      "Epoch [139/200], Step [40], Loss: 1.5616\n",
      "Training accuracy for epoch 139 is 94%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for epoch 139 is 77%\n",
      "Epoch [140/200], Step [10], Loss: 1.5591\n",
      "Epoch [140/200], Step [20], Loss: 1.4654\n",
      "Epoch [140/200], Step [30], Loss: 1.4670\n",
      "Epoch [140/200], Step [40], Loss: 1.4632\n",
      "Training accuracy for epoch 140 is 95%\n",
      "Validation accuracy for epoch 140 is 77%\n",
      "Epoch [141/200], Step [10], Loss: 1.4625\n",
      "Epoch [141/200], Step [20], Loss: 1.5743\n",
      "Epoch [141/200], Step [30], Loss: 1.4701\n",
      "Epoch [141/200], Step [40], Loss: 1.5646\n",
      "Training accuracy for epoch 141 is 95%\n",
      "Validation accuracy for epoch 141 is 77%\n",
      "Epoch [142/200], Step [10], Loss: 1.6656\n",
      "Epoch [142/200], Step [20], Loss: 1.5588\n",
      "Epoch [142/200], Step [30], Loss: 1.4876\n",
      "Epoch [142/200], Step [40], Loss: 1.4644\n",
      "Training accuracy for epoch 142 is 95%\n",
      "Validation accuracy for epoch 142 is 77%\n",
      "Epoch [143/200], Step [10], Loss: 1.4622\n",
      "Epoch [143/200], Step [20], Loss: 1.4632\n",
      "Epoch [143/200], Step [30], Loss: 1.6108\n",
      "Epoch [143/200], Step [40], Loss: 1.4860\n",
      "Training accuracy for epoch 143 is 96%\n",
      "Validation accuracy for epoch 143 is 76%\n",
      "Epoch [144/200], Step [10], Loss: 1.4724\n",
      "Epoch [144/200], Step [20], Loss: 1.4987\n",
      "Epoch [144/200], Step [30], Loss: 1.5619\n",
      "Epoch [144/200], Step [40], Loss: 1.4735\n",
      "Training accuracy for epoch 144 is 96%\n",
      "Validation accuracy for epoch 144 is 77%\n",
      "Epoch [145/200], Step [10], Loss: 1.6571\n",
      "Epoch [145/200], Step [20], Loss: 1.4659\n",
      "Epoch [145/200], Step [30], Loss: 1.5617\n",
      "Epoch [145/200], Step [40], Loss: 1.4880\n",
      "Training accuracy for epoch 145 is 95%\n",
      "Validation accuracy for epoch 145 is 77%\n",
      "Epoch [146/200], Step [10], Loss: 1.4630\n",
      "Epoch [146/200], Step [20], Loss: 1.4665\n",
      "Epoch [146/200], Step [30], Loss: 1.5566\n",
      "Epoch [146/200], Step [40], Loss: 1.5622\n",
      "Training accuracy for epoch 146 is 96%\n",
      "Validation accuracy for epoch 146 is 77%\n",
      "Epoch [147/200], Step [10], Loss: 1.4666\n",
      "Epoch [147/200], Step [20], Loss: 1.4662\n",
      "Epoch [147/200], Step [30], Loss: 1.5032\n",
      "Epoch [147/200], Step [40], Loss: 1.5605\n",
      "Training accuracy for epoch 147 is 96%\n",
      "Validation accuracy for epoch 147 is 77%\n",
      "Epoch [148/200], Step [10], Loss: 1.5597\n",
      "Epoch [148/200], Step [20], Loss: 1.4624\n",
      "Epoch [148/200], Step [30], Loss: 1.6565\n",
      "Epoch [148/200], Step [40], Loss: 1.4690\n",
      "Training accuracy for epoch 148 is 96%\n",
      "Validation accuracy for epoch 148 is 77%\n",
      "Epoch [149/200], Step [10], Loss: 1.6612\n",
      "Epoch [149/200], Step [20], Loss: 1.4962\n",
      "Epoch [149/200], Step [30], Loss: 1.4636\n",
      "Epoch [149/200], Step [40], Loss: 1.4623\n",
      "Training accuracy for epoch 149 is 96%\n",
      "Validation accuracy for epoch 149 is 75%\n",
      "Epoch [150/200], Step [10], Loss: 1.6607\n",
      "Epoch [150/200], Step [20], Loss: 1.5695\n",
      "Epoch [150/200], Step [30], Loss: 1.4662\n",
      "Epoch [150/200], Step [40], Loss: 1.4667\n",
      "Training accuracy for epoch 150 is 96%\n",
      "Validation accuracy for epoch 150 is 77%\n",
      "Epoch [151/200], Step [10], Loss: 1.4703\n",
      "Epoch [151/200], Step [20], Loss: 1.4638\n",
      "Epoch [151/200], Step [30], Loss: 1.4649\n",
      "Epoch [151/200], Step [40], Loss: 1.5566\n",
      "Training accuracy for epoch 151 is 96%\n",
      "Validation accuracy for epoch 151 is 77%\n",
      "Epoch [152/200], Step [10], Loss: 1.4616\n",
      "Epoch [152/200], Step [20], Loss: 1.6575\n",
      "Epoch [152/200], Step [30], Loss: 1.4942\n",
      "Epoch [152/200], Step [40], Loss: 1.5626\n",
      "Training accuracy for epoch 152 is 96%\n",
      "Validation accuracy for epoch 152 is 77%\n",
      "Epoch [153/200], Step [10], Loss: 1.6498\n",
      "Epoch [153/200], Step [20], Loss: 1.4638\n",
      "Epoch [153/200], Step [30], Loss: 1.4639\n",
      "Epoch [153/200], Step [40], Loss: 1.4637\n",
      "Training accuracy for epoch 153 is 96%\n",
      "Validation accuracy for epoch 153 is 77%\n",
      "Epoch [154/200], Step [10], Loss: 1.4662\n",
      "Epoch [154/200], Step [20], Loss: 1.4661\n",
      "Epoch [154/200], Step [30], Loss: 1.4618\n",
      "Epoch [154/200], Step [40], Loss: 1.4625\n",
      "Training accuracy for epoch 154 is 96%\n",
      "Validation accuracy for epoch 154 is 77%\n",
      "Epoch [155/200], Step [10], Loss: 1.4635\n",
      "Epoch [155/200], Step [20], Loss: 1.6566\n",
      "Epoch [155/200], Step [30], Loss: 1.4647\n",
      "Epoch [155/200], Step [40], Loss: 1.4630\n",
      "Training accuracy for epoch 155 is 96%\n",
      "Validation accuracy for epoch 155 is 77%\n",
      "Epoch [156/200], Step [10], Loss: 1.5453\n",
      "Epoch [156/200], Step [20], Loss: 1.4625\n",
      "Epoch [156/200], Step [30], Loss: 1.4615\n",
      "Epoch [156/200], Step [40], Loss: 1.6569\n",
      "Training accuracy for epoch 156 is 96%\n",
      "Validation accuracy for epoch 156 is 77%\n",
      "Epoch [157/200], Step [10], Loss: 1.4626\n",
      "Epoch [157/200], Step [20], Loss: 1.4645\n",
      "Epoch [157/200], Step [30], Loss: 1.5612\n",
      "Epoch [157/200], Step [40], Loss: 1.5579\n",
      "Training accuracy for epoch 157 is 96%\n",
      "Validation accuracy for epoch 157 is 77%\n",
      "Epoch [158/200], Step [10], Loss: 1.5643\n",
      "Epoch [158/200], Step [20], Loss: 1.6628\n",
      "Epoch [158/200], Step [30], Loss: 1.4671\n",
      "Epoch [158/200], Step [40], Loss: 1.5178\n",
      "Training accuracy for epoch 158 is 96%\n",
      "Validation accuracy for epoch 158 is 77%\n",
      "Epoch [159/200], Step [10], Loss: 1.4626\n",
      "Epoch [159/200], Step [20], Loss: 1.4635\n",
      "Epoch [159/200], Step [30], Loss: 1.4625\n",
      "Epoch [159/200], Step [40], Loss: 1.5621\n",
      "Training accuracy for epoch 159 is 96%\n",
      "Validation accuracy for epoch 159 is 77%\n",
      "Epoch [160/200], Step [10], Loss: 1.4704\n",
      "Epoch [160/200], Step [20], Loss: 1.5656\n",
      "Epoch [160/200], Step [30], Loss: 1.5621\n",
      "Epoch [160/200], Step [40], Loss: 1.4699\n",
      "Training accuracy for epoch 160 is 96%\n",
      "Validation accuracy for epoch 160 is 77%\n",
      "Epoch [161/200], Step [10], Loss: 1.5621\n",
      "Epoch [161/200], Step [20], Loss: 1.5623\n",
      "Epoch [161/200], Step [30], Loss: 1.4636\n",
      "Epoch [161/200], Step [40], Loss: 1.4621\n",
      "Training accuracy for epoch 161 is 96%\n",
      "Validation accuracy for epoch 161 is 77%\n",
      "Epoch [162/200], Step [10], Loss: 1.5631\n",
      "Epoch [162/200], Step [20], Loss: 1.4660\n",
      "Epoch [162/200], Step [30], Loss: 1.4613\n",
      "Epoch [162/200], Step [40], Loss: 1.4632\n",
      "Training accuracy for epoch 162 is 96%\n",
      "Validation accuracy for epoch 162 is 76%\n",
      "Epoch [163/200], Step [10], Loss: 1.4625\n",
      "Epoch [163/200], Step [20], Loss: 1.4675\n",
      "Epoch [163/200], Step [30], Loss: 1.4634\n",
      "Epoch [163/200], Step [40], Loss: 1.4630\n",
      "Training accuracy for epoch 163 is 96%\n",
      "Validation accuracy for epoch 163 is 77%\n",
      "Epoch [164/200], Step [10], Loss: 1.4621\n",
      "Epoch [164/200], Step [20], Loss: 1.4626\n",
      "Epoch [164/200], Step [30], Loss: 1.4640\n",
      "Epoch [164/200], Step [40], Loss: 1.5637\n",
      "Training accuracy for epoch 164 is 96%\n",
      "Validation accuracy for epoch 164 is 77%\n",
      "Epoch [165/200], Step [10], Loss: 1.4997\n",
      "Epoch [165/200], Step [20], Loss: 1.5666\n",
      "Epoch [165/200], Step [30], Loss: 1.4631\n",
      "Epoch [165/200], Step [40], Loss: 1.4631\n",
      "Training accuracy for epoch 165 is 96%\n",
      "Validation accuracy for epoch 165 is 77%\n",
      "Epoch [166/200], Step [10], Loss: 1.4616\n",
      "Epoch [166/200], Step [20], Loss: 1.4675\n",
      "Epoch [166/200], Step [30], Loss: 1.4632\n",
      "Epoch [166/200], Step [40], Loss: 1.5591\n",
      "Training accuracy for epoch 166 is 96%\n",
      "Validation accuracy for epoch 166 is 77%\n",
      "Epoch [167/200], Step [10], Loss: 1.5621\n",
      "Epoch [167/200], Step [20], Loss: 1.5619\n",
      "Epoch [167/200], Step [30], Loss: 1.4678\n",
      "Epoch [167/200], Step [40], Loss: 1.4641\n",
      "Training accuracy for epoch 167 is 96%\n",
      "Validation accuracy for epoch 167 is 77%\n",
      "Epoch [168/200], Step [10], Loss: 1.5624\n",
      "Epoch [168/200], Step [20], Loss: 1.4628\n",
      "Epoch [168/200], Step [30], Loss: 1.4643\n",
      "Epoch [168/200], Step [40], Loss: 1.4649\n",
      "Training accuracy for epoch 168 is 96%\n",
      "Validation accuracy for epoch 168 is 77%\n",
      "Epoch [169/200], Step [10], Loss: 1.6606\n",
      "Epoch [169/200], Step [20], Loss: 1.5593\n",
      "Epoch [169/200], Step [30], Loss: 1.5643\n",
      "Epoch [169/200], Step [40], Loss: 1.4667\n",
      "Training accuracy for epoch 169 is 96%\n",
      "Validation accuracy for epoch 169 is 77%\n",
      "Epoch [170/200], Step [10], Loss: 1.6549\n",
      "Epoch [170/200], Step [20], Loss: 1.4652\n",
      "Epoch [170/200], Step [30], Loss: 1.4625\n",
      "Epoch [170/200], Step [40], Loss: 1.4635\n",
      "Training accuracy for epoch 170 is 96%\n",
      "Validation accuracy for epoch 170 is 77%\n",
      "Epoch [171/200], Step [10], Loss: 1.4641\n",
      "Epoch [171/200], Step [20], Loss: 1.4627\n",
      "Epoch [171/200], Step [30], Loss: 1.5621\n",
      "Epoch [171/200], Step [40], Loss: 1.4622\n",
      "Training accuracy for epoch 171 is 96%\n",
      "Validation accuracy for epoch 171 is 77%\n",
      "Epoch [172/200], Step [10], Loss: 1.4623\n",
      "Epoch [172/200], Step [20], Loss: 1.5578\n",
      "Epoch [172/200], Step [30], Loss: 1.4621\n",
      "Epoch [172/200], Step [40], Loss: 1.4637\n",
      "Training accuracy for epoch 172 is 96%\n",
      "Validation accuracy for epoch 172 is 77%\n",
      "Epoch [173/200], Step [10], Loss: 1.4632\n",
      "Epoch [173/200], Step [20], Loss: 1.4623\n",
      "Epoch [173/200], Step [30], Loss: 1.6579\n",
      "Epoch [173/200], Step [40], Loss: 1.5581\n",
      "Training accuracy for epoch 173 is 96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for epoch 173 is 77%\n",
      "Epoch [174/200], Step [10], Loss: 1.4624\n",
      "Epoch [174/200], Step [20], Loss: 1.4652\n",
      "Epoch [174/200], Step [30], Loss: 1.4627\n",
      "Epoch [174/200], Step [40], Loss: 1.4658\n",
      "Training accuracy for epoch 174 is 96%\n",
      "Validation accuracy for epoch 174 is 76%\n",
      "Epoch [175/200], Step [10], Loss: 1.5594\n",
      "Epoch [175/200], Step [20], Loss: 1.5635\n",
      "Epoch [175/200], Step [30], Loss: 1.5624\n",
      "Epoch [175/200], Step [40], Loss: 1.5634\n",
      "Training accuracy for epoch 175 is 96%\n",
      "Validation accuracy for epoch 175 is 77%\n",
      "Epoch [176/200], Step [10], Loss: 1.4643\n",
      "Epoch [176/200], Step [20], Loss: 1.5640\n",
      "Epoch [176/200], Step [30], Loss: 1.4619\n",
      "Epoch [176/200], Step [40], Loss: 1.4645\n",
      "Training accuracy for epoch 176 is 96%\n",
      "Validation accuracy for epoch 176 is 77%\n",
      "Epoch [177/200], Step [10], Loss: 1.5630\n",
      "Epoch [177/200], Step [20], Loss: 1.5625\n",
      "Epoch [177/200], Step [30], Loss: 1.4651\n",
      "Epoch [177/200], Step [40], Loss: 1.5647\n",
      "Training accuracy for epoch 177 is 96%\n",
      "Validation accuracy for epoch 177 is 76%\n",
      "Epoch [178/200], Step [10], Loss: 1.4630\n",
      "Epoch [178/200], Step [20], Loss: 1.5581\n",
      "Epoch [178/200], Step [30], Loss: 1.4641\n",
      "Epoch [178/200], Step [40], Loss: 1.5590\n",
      "Training accuracy for epoch 178 is 96%\n",
      "Validation accuracy for epoch 178 is 77%\n",
      "Epoch [179/200], Step [10], Loss: 1.5574\n",
      "Epoch [179/200], Step [20], Loss: 1.4639\n",
      "Epoch [179/200], Step [30], Loss: 1.5612\n",
      "Epoch [179/200], Step [40], Loss: 1.6580\n",
      "Training accuracy for epoch 179 is 96%\n",
      "Validation accuracy for epoch 179 is 76%\n",
      "Epoch [180/200], Step [10], Loss: 1.4636\n",
      "Epoch [180/200], Step [20], Loss: 1.4618\n",
      "Epoch [180/200], Step [30], Loss: 1.4640\n",
      "Epoch [180/200], Step [40], Loss: 1.4635\n",
      "Training accuracy for epoch 180 is 96%\n",
      "Validation accuracy for epoch 180 is 77%\n",
      "Epoch [181/200], Step [10], Loss: 1.5633\n",
      "Epoch [181/200], Step [20], Loss: 1.4649\n",
      "Epoch [181/200], Step [30], Loss: 1.4635\n",
      "Epoch [181/200], Step [40], Loss: 1.5605\n",
      "Training accuracy for epoch 181 is 96%\n",
      "Validation accuracy for epoch 181 is 77%\n",
      "Epoch [182/200], Step [10], Loss: 1.5603\n",
      "Epoch [182/200], Step [20], Loss: 1.5625\n",
      "Epoch [182/200], Step [30], Loss: 1.4632\n",
      "Epoch [182/200], Step [40], Loss: 1.4635\n",
      "Training accuracy for epoch 182 is 96%\n",
      "Validation accuracy for epoch 182 is 77%\n",
      "Epoch [183/200], Step [10], Loss: 1.5649\n",
      "Epoch [183/200], Step [20], Loss: 1.4633\n",
      "Epoch [183/200], Step [30], Loss: 1.4627\n",
      "Epoch [183/200], Step [40], Loss: 1.4633\n",
      "Training accuracy for epoch 183 is 96%\n",
      "Validation accuracy for epoch 183 is 77%\n",
      "Epoch [184/200], Step [10], Loss: 1.4625\n",
      "Epoch [184/200], Step [20], Loss: 1.5589\n",
      "Epoch [184/200], Step [30], Loss: 1.4635\n",
      "Epoch [184/200], Step [40], Loss: 1.5617\n",
      "Training accuracy for epoch 184 is 96%\n",
      "Validation accuracy for epoch 184 is 77%\n",
      "Epoch [185/200], Step [10], Loss: 1.4625\n",
      "Epoch [185/200], Step [20], Loss: 1.5574\n",
      "Epoch [185/200], Step [30], Loss: 1.4643\n",
      "Epoch [185/200], Step [40], Loss: 1.4652\n",
      "Training accuracy for epoch 185 is 96%\n",
      "Validation accuracy for epoch 185 is 77%\n",
      "Epoch [186/200], Step [10], Loss: 1.4627\n",
      "Epoch [186/200], Step [20], Loss: 1.4622\n",
      "Epoch [186/200], Step [30], Loss: 1.4646\n",
      "Epoch [186/200], Step [40], Loss: 1.4624\n",
      "Training accuracy for epoch 186 is 96%\n",
      "Validation accuracy for epoch 186 is 77%\n",
      "Epoch [187/200], Step [10], Loss: 1.4616\n",
      "Epoch [187/200], Step [20], Loss: 1.5593\n",
      "Epoch [187/200], Step [30], Loss: 1.4620\n",
      "Epoch [187/200], Step [40], Loss: 1.5621\n",
      "Training accuracy for epoch 187 is 96%\n",
      "Validation accuracy for epoch 187 is 77%\n",
      "Epoch [188/200], Step [10], Loss: 1.6616\n",
      "Epoch [188/200], Step [20], Loss: 1.5608\n",
      "Epoch [188/200], Step [30], Loss: 1.4621\n",
      "Epoch [188/200], Step [40], Loss: 1.4618\n",
      "Training accuracy for epoch 188 is 96%\n",
      "Validation accuracy for epoch 188 is 77%\n",
      "Epoch [189/200], Step [10], Loss: 1.4619\n",
      "Epoch [189/200], Step [20], Loss: 1.6565\n",
      "Epoch [189/200], Step [30], Loss: 1.4624\n",
      "Epoch [189/200], Step [40], Loss: 1.4631\n",
      "Training accuracy for epoch 189 is 96%\n",
      "Validation accuracy for epoch 189 is 77%\n",
      "Epoch [190/200], Step [10], Loss: 1.5626\n",
      "Epoch [190/200], Step [20], Loss: 1.4621\n",
      "Epoch [190/200], Step [30], Loss: 1.4619\n",
      "Epoch [190/200], Step [40], Loss: 1.4620\n",
      "Training accuracy for epoch 190 is 96%\n",
      "Validation accuracy for epoch 190 is 77%\n",
      "Epoch [191/200], Step [10], Loss: 1.4624\n",
      "Epoch [191/200], Step [20], Loss: 1.4621\n",
      "Epoch [191/200], Step [30], Loss: 1.6560\n",
      "Epoch [191/200], Step [40], Loss: 1.4627\n",
      "Training accuracy for epoch 191 is 96%\n",
      "Validation accuracy for epoch 191 is 77%\n",
      "Epoch [192/200], Step [10], Loss: 1.4638\n",
      "Epoch [192/200], Step [20], Loss: 1.5623\n",
      "Epoch [192/200], Step [30], Loss: 1.4621\n",
      "Epoch [192/200], Step [40], Loss: 1.5629\n",
      "Training accuracy for epoch 192 is 96%\n",
      "Validation accuracy for epoch 192 is 77%\n",
      "Epoch [193/200], Step [10], Loss: 1.4628\n",
      "Epoch [193/200], Step [20], Loss: 1.4625\n",
      "Epoch [193/200], Step [30], Loss: 1.4626\n",
      "Epoch [193/200], Step [40], Loss: 1.5631\n",
      "Training accuracy for epoch 193 is 96%\n",
      "Validation accuracy for epoch 193 is 77%\n",
      "Epoch [194/200], Step [10], Loss: 1.4637\n",
      "Epoch [194/200], Step [20], Loss: 1.4629\n",
      "Epoch [194/200], Step [30], Loss: 1.4621\n",
      "Epoch [194/200], Step [40], Loss: 1.5625\n",
      "Training accuracy for epoch 194 is 96%\n",
      "Validation accuracy for epoch 194 is 77%\n",
      "Epoch [195/200], Step [10], Loss: 1.4616\n",
      "Epoch [195/200], Step [20], Loss: 1.4619\n",
      "Epoch [195/200], Step [30], Loss: 1.5593\n",
      "Epoch [195/200], Step [40], Loss: 1.5581\n",
      "Training accuracy for epoch 195 is 96%\n",
      "Validation accuracy for epoch 195 is 77%\n",
      "Epoch [196/200], Step [10], Loss: 1.6621\n",
      "Epoch [196/200], Step [20], Loss: 1.5594\n",
      "Epoch [196/200], Step [30], Loss: 1.4615\n",
      "Epoch [196/200], Step [40], Loss: 1.4623\n",
      "Training accuracy for epoch 196 is 96%\n",
      "Validation accuracy for epoch 196 is 77%\n",
      "Epoch [197/200], Step [10], Loss: 1.4617\n",
      "Epoch [197/200], Step [20], Loss: 1.4631\n",
      "Epoch [197/200], Step [30], Loss: 1.4620\n",
      "Epoch [197/200], Step [40], Loss: 1.5561\n",
      "Training accuracy for epoch 197 is 96%\n",
      "Validation accuracy for epoch 197 is 77%\n",
      "Epoch [198/200], Step [10], Loss: 1.4617\n",
      "Epoch [198/200], Step [20], Loss: 1.4621\n",
      "Epoch [198/200], Step [30], Loss: 1.4630\n",
      "Epoch [198/200], Step [40], Loss: 1.5623\n",
      "Training accuracy for epoch 198 is 96%\n",
      "Validation accuracy for epoch 198 is 77%\n",
      "Epoch [199/200], Step [10], Loss: 1.4626\n",
      "Epoch [199/200], Step [20], Loss: 1.4637\n",
      "Epoch [199/200], Step [30], Loss: 1.5596\n",
      "Epoch [199/200], Step [40], Loss: 1.4651\n",
      "Training accuracy for epoch 199 is 96%\n",
      "Validation accuracy for epoch 199 is 77%\n",
      "Epoch [200/200], Step [10], Loss: 1.4627\n",
      "Epoch [200/200], Step [20], Loss: 1.4636\n",
      "Epoch [200/200], Step [30], Loss: 1.4640\n",
      "Epoch [200/200], Step [40], Loss: 1.5599\n",
      "Training accuracy for epoch 200 is 96%\n",
      "Validation accuracy for epoch 200 is 77%\n"
     ]
    }
   ],
   "source": [
    "# In each epoch\n",
    "train_err = []\n",
    "val_err = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # For each batch of images in train set\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        images = images.view(-1, 28*28)\n",
    "        labels = labels\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Initialize gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (this calls the \"forward\" function within Net)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Find the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Find the gradients of all weights using the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights using the optimizer\n",
    "        # For e.g.: w = w - (delta_w)*lr\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Find the output by doing a forward pass through the network\n",
    "        outputs = net(images)\n",
    "\n",
    "        # Find the class of each sample by taking a max across the probabilities of each class\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, loss.item()))\n",
    "            \n",
    "    print('Training accuracy for epoch {} is {}%'.format(epoch+1, (100 * correct_train / total_train)))\n",
    "    train_err.append(100.0-(100 * correct_train / total_train))\n",
    "    \n",
    "    # Validation\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    # For each batch of images in test set\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for images, labels in test_loader:\n",
    "\n",
    "          # Get the images\n",
    "          images = images.view(-1, 28*28)\n",
    "\n",
    "          images = images.to(device)\n",
    "\n",
    "          # Find the output by doing a forward pass through the network\n",
    "          outputs = net(images)\n",
    "\n",
    "          # Find the class of each sample by taking a max across the probabilities of each class\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "          # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "          total_val += labels.size(0)\n",
    "          correct_val += (predicted.cpu() == labels).sum()\n",
    "\n",
    "    print('Validation accuracy for epoch {} is {}%'.format(epoch+1, (100 * correct_val / total_val)))\n",
    "    val_err.append(100.0-(100 * correct_val / total_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print training and validation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efb895eb320>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb896ca1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(num_epochs)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, train_err)\n",
    "plt.plot(x, val_err)\n",
    "plt.legend(['training error', 'validation error'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the network on the 10000 test images: 77 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# For each batch of images in test set\n",
    "with torch.set_grad_enabled(False):\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "      # Get the images\n",
    "      images = images.view(-1, 28*28)\n",
    "\n",
    "      images = images.to(device)\n",
    "\n",
    "      # Find the output by doing a forward pass through the network\n",
    "      outputs = net(images)\n",
    "\n",
    "      # Find the class of each sample by taking a max across the probabilities of each class\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "      # Increment 'total', and 'correct' according to whether the prediction was correct or not\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Test accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
