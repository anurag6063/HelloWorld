{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint\n",
    "#### To be done in the Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we explore how k-Means clustering is able to find patterns and produce groupings without any form of external information to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the text of the famous Russian novel \"War and Peace\" by Leo Tolstoy, downloaded from Project Gutenburg (http://www.gutenberg.org/), and extract all sentences from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing following actions :\n",
    "1. Use the nltk library to break the text into sentences. \n",
    "2. Build a word2vec representation using these sentences, after removing fluff words from these sentences.\n",
    "3. Build a k-means algorithm on top of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the experiment execute the following commands\n",
    "\n",
    "\n",
    "\n",
    "1.!pip3 install gensim\n",
    "\n",
    "2.!pip3 install nltk\n",
    "\n",
    "3.import nltk\n",
    "\n",
    "4.nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:42.777995Z",
     "start_time": "2018-07-31T05:44:41.709888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:42.783132Z",
     "start_time": "2018-07-31T05:44:42.780491Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = \"AIML_DS_WAR-And-PEACE_STD.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us  read the entire file into a list of lines, converting everything to lowercase as well as remove trailing and leading whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:42.898497Z",
     "start_time": "2018-07-31T05:44:42.785248Z"
    }
   },
   "outputs": [],
   "source": [
    "wp_text_stage0 = [line.strip().lower() for line in open(dataset,encoding=\"utf8\")]\n",
    "print(wp_text_stage0[4000:4010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine them into one gigantic string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:42.964681Z",
     "start_time": "2018-07-31T05:44:42.900955Z"
    }
   },
   "outputs": [],
   "source": [
    "wp_text_stage1 = ' '.join(wp_text_stage0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:43.070320Z",
     "start_time": "2018-07-31T05:44:42.966728Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(wp_text_stage1))\n",
    "print(wp_text_stage1[40000:40200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gigantic string with us. let us see how to break this string into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:45.661975Z",
     "start_time": "2018-07-31T05:44:43.072190Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "wp_text_stage2 = sent_tokenize(wp_text_stage1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:45.671930Z",
     "start_time": "2018-07-31T05:44:45.663715Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(wp_text_stage2))\n",
    "print(wp_text_stage2[5000:5010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have about 26k sentences, in the tome. We now take each sentence and clean it up as below:\n",
    " * replace all non-alphanumeric characters by space\n",
    " * split each sentence on whitespace\n",
    " * in each sentence drop words that are less than 3 letters long and are part of fluff words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the entire contents the fluff file into a set. As mentioned earlier a set is much faster for checking membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:45.864808Z",
     "start_time": "2018-07-31T05:44:45.675461Z"
    }
   },
   "outputs": [],
   "source": [
    "fluff = set([line.strip() for line in open(\"AIML_DS_STOPWORDS_STD.txt\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing all non-alphanumeric characters by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:46.179791Z",
     "start_time": "2018-07-31T05:44:45.866540Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "only_alnum = re.compile(r\"[^\\w]+\") ## \\w => unicode alphabet\n",
    "#only_alnum = re.compile(r\"[^a-z0-9]\") --> This will remove accented characters which are part of many names!\n",
    "\n",
    "## Replaces one or more occurrence of any characters other unicode alphabets and numbers\n",
    "def cleanUp(s):\n",
    "    return re.sub(only_alnum, \" \", s).strip()\n",
    "wp_text_stage3 = [cleanUp(s) for s in wp_text_stage2]\n",
    "print(wp_text_stage3[4000:4010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we break each sentence into words, and store these words as a list. We traverse this list and drop the unwanted words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:46.184203Z",
     "start_time": "2018-07-31T05:44:46.181692Z"
    }
   },
   "outputs": [],
   "source": [
    "def choose_words(s):\n",
    "    return [w for w in s.split() if len(w) > 2 and w not in fluff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:46.412826Z",
     "start_time": "2018-07-31T05:44:46.185879Z"
    }
   },
   "outputs": [],
   "source": [
    "wp_text_stage4 = [choose_words(sentence) for sentence in wp_text_stage3]\n",
    "print(wp_text_stage4[4000:4010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:46.418218Z",
     "start_time": "2018-07-31T05:44:46.415520Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(wp_text_stage4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the words to common stem -- that is we do not want to consider \"run\", \"runs\", \"running\" as separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:46.500793Z",
     "start_time": "2018-07-31T05:44:46.420086Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"), stemmer.stem(\"run\"), stemmer.stem(\"runs\"), stemmer.stem(\"runner\"))\n",
    "print(stemmer.stem(\"guns\"), stemmer.stem(\"gun\"), stemmer.stem(\"gunned\"), stemmer.stem(\"gunning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:46.568281Z",
     "start_time": "2018-07-31T05:44:46.503765Z"
    }
   },
   "outputs": [],
   "source": [
    "def stem_list(wordlist):\n",
    "    return [stemmer.stem(word) for word in wordlist]\n",
    "for n in range(4000, 4010):\n",
    "    print(wp_text_stage4[n], stem_list(wp_text_stage4[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:49.541024Z",
     "start_time": "2018-07-31T05:44:46.570915Z"
    }
   },
   "outputs": [],
   "source": [
    "wp_text_stage5 = [stem_list(s) for s in wp_text_stage4]\n",
    "print(wp_text_stage5[4000:4010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build a word2vec model with this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:49.546136Z",
     "start_time": "2018-07-31T05:44:49.543335Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 50   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:50.449608Z",
     "start_time": "2018-07-31T05:44:49.547924Z"
    }
   },
   "outputs": [],
   "source": [
    "wp = word2vec.Word2Vec(wp_text_stage5, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:50.464488Z",
     "start_time": "2018-07-31T05:44:50.450952Z"
    }
   },
   "outputs": [],
   "source": [
    "wp.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:50.582044Z",
     "start_time": "2018-07-31T05:44:50.466667Z"
    }
   },
   "outputs": [],
   "source": [
    "wp.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:50.657013Z",
     "start_time": "2018-07-31T05:44:50.585074Z"
    }
   },
   "outputs": [],
   "source": [
    "len(wp.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:50.743203Z",
     "start_time": "2018-07-31T05:44:50.659249Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(list(wp.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:50.810823Z",
     "start_time": "2018-07-31T05:44:50.744665Z"
    }
   },
   "outputs": [],
   "source": [
    "words = [\"chair\",\"car\",\"man\",\"woman\",\"clean\",\"close\",\"cloud\",\"coat\", \"confus\",\"danger\",\"daughter\",\"deal\",\"run\",\"walk\",\"count\",\"father\",\"girl\",\"near\",\"neck\",\"spoke\",\"spoken\",\"stand\",\"show\",\"shown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save this so that we can continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:51.062336Z",
     "start_time": "2018-07-31T05:44:50.814590Z"
    }
   },
   "outputs": [],
   "source": [
    "wp.wv.save_word2vec_format('wp.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:51.070141Z",
     "start_time": "2018-07-31T05:44:51.063962Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([wp[w] for w in wp.wv.vocab if w in words])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let us apply k-means algorithm on the top of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means\n",
    "K-means  is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centroids, one for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:51.287987Z",
     "start_time": "2018-07-31T05:44:51.072628Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 4)\n",
    "km.fit(X)\n",
    "y_kmeans = km.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:51.461375Z",
     "start_time": "2018-07-31T05:44:51.290038Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "lle_data = manifold.LocallyLinearEmbedding(n_neighbors=10, n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T05:44:57.848880Z",
     "start_time": "2018-07-31T05:44:57.617070Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(lle_data[:,0],lle_data[:,1], c =y_kmeans )\n",
    "for i in range(len(words)-3):\n",
    "    plt.annotate(words[i], xy = (lle_data[i][0],lle_data[i][1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words get divided into four clusters  as shown by the four colors, visualize into 2D by LLE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
