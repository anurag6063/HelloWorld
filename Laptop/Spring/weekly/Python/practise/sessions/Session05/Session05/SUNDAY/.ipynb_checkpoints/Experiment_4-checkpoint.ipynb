{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint\n",
    "#### To be done in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is understand a modern Back Propagation Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will be using MNIST database. The MNIST database is a dataset of handwritten digits. It has 60,000 training samples, and 10,000 test samples. Each image is represented by 28 x 28 pixels, each containing a value 0 - 255 with its gray scale value.\n",
    "\n",
    "It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:03.555807Z",
     "start_time": "2018-07-19T04:58:02.980344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuser/.local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/tsuser/.local/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Packages\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset from sklearn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.300346Z",
     "start_time": "2018-07-19T04:58:03.557910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 784) (7000,)\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST datset \n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, Y = mnist.data, mnist.target\n",
    "Y = Y.astype(int)\n",
    "\n",
    "X = X[::10, :]     ## taking the whole data will take a lot of processing time\n",
    "Y = Y[::10]\n",
    "# digits = datasets.load_digits(n_class=10)\n",
    "# # Create our X and y data\n",
    "# X = digits.data\n",
    "# Y = digits.target\n",
    "print(X.shape, Y.shape)\n",
    "num_examples = X.shape[0]      ## training set size\n",
    "nn_input_dim = X.shape[1]      ## input layer dimensionality\n",
    "nn_output_dim = len(np.unique(Y))       ## output layer dimensionality\n",
    "\n",
    "params = {\n",
    "    \"lr\":0.0001,        ## learning_rate\n",
    "    \"max_iter\":500,\n",
    "    \"weight_init\":\"xavier\",\n",
    "    \"h_dimn\":100,     ## hidden_layer_size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero Weight Initialization: This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a solution, it is common to initialize the weights of the neurons to small numbers (random or unique) and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. Instead of using random initializations, it is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.\n",
    "\n",
    "It is worth mentioning that if you do not know which technique should be chosen as weight initilalizaion method, Xaiver is often choosen as a initial try.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.309754Z",
     "start_time": "2018-07-19T04:58:04.305717Z"
    }
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out):\n",
    "    ## using FanAvg variation\n",
    "    n = (fan_in+fan_out)/2\n",
    "    limit = np.sqrt(3.0 * 1 / n)\n",
    "    return np.random.uniform(size = (fan_in, fan_out), low = -limit, high = +limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.379707Z",
     "start_time": "2018-07-19T04:58:04.313431Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_initialization(params):\n",
    "    hdim = params[\"h_dimn\"]\n",
    "    winit = params[\"weight_init\"]\n",
    "    if winit == \"random\":\n",
    "        np.random.seed(0)\n",
    "        W1 = np.random.randn(nn_input_dim, hdim)\n",
    "        b1 = np.random.randn(1, hdim)\n",
    "        W2 = np.random.randn(hdim, nn_output_dim)\n",
    "        b2 = np.random.randn(1, nn_output_dim)\n",
    "    elif winit == \"zeros\":\n",
    "        W1 = np.zeros((nn_input_dim, hdim))\n",
    "        b1 = np.zeros((1, hdim))\n",
    "        W2 = np.zeros((hdim, nn_output_dim))\n",
    "        b2 = np.zeros((1, nn_output_dim))\n",
    "    elif winit == \"xavier\":\n",
    "        W1 = xavier_init(nn_input_dim, hdim)\n",
    "        b1 = xavier_init(1, hdim)\n",
    "        W2 = xavier_init(hdim, nn_output_dim)\n",
    "        b2 = xavier_init(1, nn_output_dim)\n",
    "    elif winit == \"uniform\":\n",
    "        W1 = np.random.uniform(size=(nn_input_dim, hdim), low=-1, high=1)/np.sqrt(nn_input_dim)\n",
    "        b1 = np.random.uniform(size=(1, hdim), low=-1, high=1)\n",
    "        W2 = np.random.uniform(size=(hdim, nn_output_dim), low=-1, high=1)/np.sqrt(hdim)\n",
    "        b2 = np.random.uniform(size=(1, nn_output_dim), low=-1, high=1)\n",
    "    elif winit == \"normal\":\n",
    "        W1 = np.random.normal(loc = 0, scale = 0.5, size = (nn_input_dim, hdim))\n",
    "        b1 = np.random.normal(loc = 0, scale = 0.5, size=(1, hdim))\n",
    "        W2 = np.random.normal(loc = 0, scale = 0.5, size = (hdim, nn_output_dim))\n",
    "        b2 = np.random.normal(loc = 0, scale = 0.5, size=(1, nn_output_dim))\n",
    "    return W1, b1, W2, b2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.487579Z",
     "start_time": "2018-07-19T04:58:04.381942Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.568637Z",
     "start_time": "2018-07-19T04:58:04.489320Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    W1, b1, W2, b2 = weight_initialization(params)\n",
    "    # This is what we return at the end\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.652059Z",
     "start_time": "2018-07-19T04:58:04.574992Z"
    }
   },
   "outputs": [],
   "source": [
    "def feedforward(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    probs = softmax(z2)\n",
    "    return a1, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.716952Z",
     "start_time": "2018-07-19T04:58:04.654051Z"
    }
   },
   "outputs": [],
   "source": [
    "def backpropagation(model, x, y, a1, probs):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    delta3 = probs\n",
    "    delta3[range(y.shape[0]), y] -= 1\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "    dW1 = np.dot(x.T, delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    return dW2, db2, dW1, db1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.819030Z",
     "start_time": "2018-07-19T04:58:04.718633Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_loss(model, x, y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    \n",
    "    # Calculating the cross entropy loss\n",
    "    corect_logprobs = -np.log(probs[range(y.shape[0]), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    \n",
    "    return 1./y.shape[0] * data_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.890239Z",
     "start_time": "2018-07-19T04:58:04.821368Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, x, y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return np.count_nonzero(y==preds)/y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T04:58:04.964566Z",
     "start_time": "2018-07-19T04:58:04.892113Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, Y_train, Y_test, verbose=True):\n",
    "    # Gradient descent. For each batch...\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    for i in range(0, params[\"max_iter\"]):\n",
    "\n",
    "        # Forward propagation\n",
    "        a1, probs = feedforward(model, X_train)\n",
    "\n",
    "        # Backpropagation\n",
    "        dW2, db2, dW1, db1 = backpropagation(model, X_train, Y_train, a1, probs)\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -params[\"lr\"] * dW1\n",
    "        b1 += -params[\"lr\"] * db1\n",
    "        W2 += -params[\"lr\"] * dW2\n",
    "        b2 += -params[\"lr\"] * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        if verbose and i % 50 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model, X_train, Y_train)),\n",
    "                  \", Test accuracy:\", test(model, X_test, Y_test), \"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with different Weight Initializations and evaluate the corresponding test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-19T04:58:03.046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'xavier', 'h_dimn': 100} TestAccuracy= 0.9121428571428571\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4)\n",
    "t = [\"xavier\",\"uniform\",\"normal\",\"zeros\",\"random\"]\n",
    "\n",
    "for i in range(5):\n",
    "    params[\"weight_init\"] = t[i]\n",
    "    model = build_model()\n",
    "    model = train(model, X_train, X_test, Y_train, Y_test, verbose=False)\n",
    "    print(params, \"TestAccuracy=\", test(model,X_test, Y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Selecting Hyperparameters\n",
    "\n",
    "scikit-learn provides a function: GridSearchCV to optimize your neural network's hyper-parameters automatically. We just provide the range or possible value of hyperparameters as the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-19T04:58:03.054Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'activation' : [\"tanh\", \"relu\"],\n",
    "            'learning_rate_init' : [0.0001, 0.001],\n",
    "            'hidden_layer_sizes' : [(300,), (300, 100), (100, 50)],\n",
    "            'solver' : [\"adam\",\"sgd\"]\n",
    "             }\n",
    "clf = MLPClassifier()\n",
    "clf = GridSearchCV(estimator=clf, param_grid=parameters, verbose=2, cv=2)\n",
    "clf.fit(X_train, Y_train)   ## might take about 10 minutes depending on number of total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-19T04:58:03.058Z"
    }
   },
   "outputs": [],
   "source": [
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
