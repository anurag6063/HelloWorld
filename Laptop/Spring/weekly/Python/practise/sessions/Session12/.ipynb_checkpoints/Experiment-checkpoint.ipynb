{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfbdl_SC0WsP"
   },
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint\n",
    "#### To be done in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kv3ppzrN0WsQ"
   },
   "source": [
    "The objective of this experiment is to understand relevance feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prtS4OZr00-W"
   },
   "source": [
    "[link text](https://)The significant part of any Information retrieval system is to make it responsive to user interaction. <br>\n",
    "\n",
    "*   List item|\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n",
    "\n",
    "*   List item\n",
    "\n",
    "\n",
    "Hence our system needs to incorporate the feedback and refine the results adapted to the user input on the fly.<br>\n",
    "Few of the use-cases would be, search engines like Google, Bing and other websites like Youtube <br> which personalize the content for a better user experience.\n",
    "\n",
    "\n",
    "In this experiment we would implement two algorithms for achieving adaptive retrieval based on user input.<br>\n",
    "#### Dataset - Wikipedia documents, a total of 2866 ranging over 10 different categories.\n",
    "\n",
    "1. #### Weighted KNN \n",
    "    - Random list of wikipedia documents (varied category) are displayed to the User. \n",
    "    - The user based on his interest clicks on any document belonging to a particular category. \n",
    "    - We implement a weighted KNN to get a ranking of the relevant documents based on the user input.\n",
    "    - The user then again clicks a document of the relevant documents.\n",
    "    - We update the weight vector based on this feedback to get a new ranking which ensures better recall than before\n",
    "\n",
    "2. #### Rocchio Algorithm\n",
    "    - Random list of documents are displayed.\n",
    "    - The user clicks on a particular document and also decides the relevant and non relevant documents of the displayed ones.\n",
    "    - The initial document clicked is our Query $q$ (initial centroid for relevant documents). <br> Relevant Documents are $D_{r}$, Non relevant ones are $D_{nr}$.\n",
    "    - We use the following formula to converge to a new centroid for the relevant documents.<br> This ensures better precision with every iteration\n",
    "    \n",
    "    $$q_{t+1} = a . q_{t} + b. (\\frac{1}{|D_{r}|} \\sum_{d_{j} \\in D_{r}} d_{j}) - c. (\\frac{1}{|D_{nr}|} \\sum_{d_{k} \\in D_{nr}} d_{k})$$\n",
    "    \n",
    "    - Display the nearest neighbours of the new centroid $q_{t+1}$, which would be more relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_rQLS1N9AJq"
   },
   "outputs": [],
   "source": [
    "!pip3 install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_X-Y2YMr00-a"
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcSJym5D00-m"
   },
   "source": [
    "## Dataset Description ##\n",
    "\n",
    "\n",
    "1. The dataset comprises 2866 Wikipedia articles belonging to a total of 10 categories.\n",
    "2. Each of the article is given a query ID based on it's category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H2l1IX2v00-o"
   },
   "outputs": [],
   "source": [
    "f = open('wikipedia_dataset/categories.list')\n",
    "cats = f.readlines()\n",
    "f = open('wikipedia_dataset/categories_originalids.list')\n",
    "cat = f.readlines()\n",
    "print('The following ' + str(len(cats)) + ' categories', cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVlMqadF00-2"
   },
   "source": [
    "## Parsing the XML data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_auParQ00-4"
   },
   "outputs": [],
   "source": [
    "def parseLog(file, docs, qid):\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler,'lxml')\n",
    "    #print(soup.findAll('text')[0])\n",
    "    txt = soup.findAll('text')[0].text       \n",
    "    for message in soup.findAll('document'):\n",
    "        msg_attrs = dict(message.attrs)\n",
    "        qid.append(int(msg_attrs['cat']))\n",
    "        docs.append(msg_attrs['name'] + ' ' + txt)\n",
    "\n",
    "lis = glob('wikipedia_dataset/texts/*')\n",
    "docs = []\n",
    "qid = []\n",
    "for i in range(len(lis)):\n",
    "    parseLog(lis[i], docs, qid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uC5zJJMy00_A"
   },
   "outputs": [],
   "source": [
    "print('Sample document:', qid[100], docs[100][:100] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TW0-0wt00_I"
   },
   "source": [
    "## Construct feature vector from documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbDoqjzX84YV"
   },
   "source": [
    "Dataset used below present in the \"Datasets\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXWWTbcr00_K"
   },
   "outputs": [],
   "source": [
    "stoplist = open('wikipedia_dataset/stopwords.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxKov96n00_Q"
   },
   "outputs": [],
   "source": [
    "## TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stoplist, min_df=0.001)\n",
    "matrix = vectorizer.fit_transform(docs)\n",
    "doc_vectors = matrix.todense()\n",
    "doc_vectors = np.array(doc_vectors)\n",
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNS3cO2m00_Y"
   },
   "source": [
    "### Read about min_df parameter used above.\n",
    "Vary the parameter to change the feature and see it's effect on the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yv0zIPBu00_a"
   },
   "outputs": [],
   "source": [
    "print('Length of the vocabulary:', len(vocab))\n",
    "print('A sample ' +str(len(vocab)) +' dimensional document vector:', doc_vectors[1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcZAAhVN00_o"
   },
   "outputs": [],
   "source": [
    "print(doc_vectors.shape, len(qid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ks7ZCRnf00_4"
   },
   "source": [
    "### 1. Weighted KNN \n",
    "- Initially display random 20 documents and get user click\n",
    "- Run the KNN Algorithm and chose 100 nearest neighbours\n",
    "- Define a weight matrix to get the ranking of the 100 relavent vectors based on similarity\n",
    "\n",
    "Given $m$ - dimensional query $q$ and document $d_{i}$, the weighted similarity is calculated as follows <br> \n",
    "$q = (q_{1}, q_{2}, \\cdots, q_{m})$ <br>\n",
    "$d_{i} = (d_{1}, d_{2}, \\cdots, c_{m})$ <br>\n",
    "$$Similarity = w^T d'_{i}$$ where ,\n",
    "$$d'_{i} = \\frac{q * d_{i}}{\\parallel q \\parallel \\parallel d_{i} \\parallel}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XElrswMo00_6"
   },
   "outputs": [],
   "source": [
    "## Fit KNN\n",
    "KNN = NearestNeighbors(100, 0.4, metric = 'cosine')\n",
    "KNN.fit(doc_vectors)\n",
    "neighbours = KNN.kneighbors(doc_vectors,return_distance=False)\n",
    "print(neighbours.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d74_Jk9-01AE"
   },
   "outputs": [],
   "source": [
    "# Number of articles displayed at max 100\n",
    "num = 10\n",
    "div = 1 # Change the value to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wsVDV7P01AO"
   },
   "outputs": [],
   "source": [
    "def to_display(doc_ids, init= False):\n",
    "    if init == True:\n",
    "        ix = random.sample(range(0, len(doc_vectors)), num)\n",
    "    else:\n",
    "        ix = doc_ids\n",
    "    \n",
    "    df = pd.DataFrame(columns=['index', 'Article','category'])\n",
    "    df['index'] = ix\n",
    "    df['Article'] = [docs[i][:100] for i in ix]\n",
    "    df['category'] = [cat[qid[i]-1][:-1] for i in ix] \n",
    "    clear_output()\n",
    "    display(HTML(df.to_html()))\n",
    "    return ix\n",
    "\n",
    "def get_user_choice(q):\n",
    "    doc_id = random.choice(q)\n",
    "    return doc_id\n",
    "\n",
    "index = to_display([],init= True)\n",
    "doc_id = input(\"Select the index of the article you would like to read or type 'stop' if you want end the search\")\n",
    "doc_id = int(doc_id)\n",
    "res = []\n",
    "ctr = 0\n",
    "## initialize the weight vector to ones\n",
    "weights = np.ones((doc_vectors.shape[1],1))\n",
    "while (True):   \n",
    "    query_doc = doc_vectors[doc_id].reshape(1, doc_vectors.shape[1])\n",
    "    relevant_docs_ix = neighbours[doc_id]\n",
    "    relevant_docs = doc_vectors[relevant_docs_ix]\n",
    "    ## Calculate Similarity\n",
    "    new_relevance = (query_doc[0] * relevant_docs) / (np.linalg.norm(relevant_docs) * np.linalg.norm(query_doc) )\n",
    "    similarity  = np.matmul(new_relevance , weights)\n",
    "    ## Return the num most ranked doc ids\n",
    "    ranked_docs_ix = relevant_docs_ix[np.argsort(similarity[:,0])]\n",
    "    index = to_display(ranked_docs_ix[:num])\n",
    "    doc_id = input(\"Select the index of the article you would like to read or type 'stop' if you want end the search\")\n",
    "    if doc_id == 'stop':\n",
    "        break    \n",
    "    doc_id = int(doc_id)\n",
    "    weights = weights + doc_vectors[doc_id].reshape(doc_vectors.shape[1], 1)\n",
    "    weights = weights/ np.linalg.norm(weights)\n",
    "    ctr += 1\n",
    "    res_ = input(\"Give the precision count/a number between 1-10 about how satisfied are you with the results\")\n",
    "    res.append(res_)\n",
    "print('Enjoy your Article')\n",
    "\n",
    "plt.plot(res)\n",
    "plt.xlabel(\"number of searches\")\n",
    "plt.ylabel(\"Score given by you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vr-Q32V301AY"
   },
   "source": [
    "### 2. Rocchio Algorithm\n",
    "- Initially random articles are displayed\n",
    "- Select an index value based on your choice\n",
    "- Select all the indices of relevant documents\n",
    "- Update the centroid of relavant vector as shown in the formula above\n",
    "- Display the nearest neighbours of the relavant vector\n",
    "\n",
    "- $alpha$- how close to relavant \n",
    "- $beta$ - how far from non relevant\n",
    "\n",
    "Ideally we would like to have high $alpha$ and low $beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aA-JfYdu01Aa"
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 0.5\n",
    "index = to_display([],init= True)\n",
    "while(True):\n",
    "    doc_id = input(\"Select the index of the article you would like to read or type 'stop' if you want end the search\")\n",
    "    if doc_id == 'stop':\n",
    "        break\n",
    "    print(\"Enter the indices, comma separated for relevant documents\")\n",
    "    R = [int(x) for x in input().split(',')]\n",
    "    NR = [i for i in index if i not in R]\n",
    "    doc_id = int(doc_id)\n",
    "    query_doc = doc_vectors[doc_id].reshape(1, doc_vectors.shape[1])\n",
    "    R_docs = doc_vectors[R]\n",
    "    NR_docs = doc_vectors[NR]\n",
    "\n",
    "    q_new = query_doc + (alpha * np.sum(R_docs, 0)/len(R)) - (beta * np.sum(NR_docs, 0)/len(NR))\n",
    "    print(q_new.shape)\n",
    "    retrieved = KNN.kneighbors(q_new, return_distance=False)\n",
    "    index = to_display(retrieved[0][:num])\n",
    "    \n",
    "print('Enjoy your article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32-00nh001Ai"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Experiment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
