{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML_MCP_LAB_JN04_EX_E.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"eyyWlTUzMEWn","colab_type":"text"},"cell_type":"markdown","source":["# Foundations of Artificial Intelligence and Machine Learning\n","## A Program by IIIT-H and TalentSprint\n","#### To be done in the Lab\n"]},{"metadata":{"id":"M-zxsrWuMEWo","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is to understand Quantization."]},{"metadata":{"id":"QqZUYFdZNSBg","colab_type":"text"},"cell_type":"markdown","source":["Neural network models can take up a lot of space on disk, with the original AlexNet being over 200 MB in float format for example. Almost all of that size is taken up with the weights for the neural connections, since there are often many millions of these in a single model. Because they're all slightly different floating point numbers, simple compression formats like zip don't compress them well.\n","\n","Training neural networks is done by applying many tiny nudges to the weights, and these small increments typically need floating point precision to work. Taking a pre-trained model and running inference is very different. If you think about recognizing an object in a photo you've just taken, the network has to ignore all the noise, lighting changes, and other non-essential differences between it and the training examples it's seen before, and focus on the important similarities instead. This ability means that they seem to treat low-precision calculations as just another source of noise, and still produce accurate results even with numerical formats that hold less information.\n","\n","\n","\n"]},{"metadata":{"id":"wAMdlG04MEWr","colab_type":"text"},"cell_type":"markdown","source":["##### Keywords\n","\n","* Uniform quantization\n","* Non-uniform quantization\n","* K-means clustering\n"]},{"metadata":{"id":"kSbXut-PMEWt","colab_type":"text"},"cell_type":"markdown","source":["##### Expected time to complete the experiment is : 90 min"]},{"metadata":{"id":"a6KW33X-MEWu","colab_type":"text"},"cell_type":"markdown","source":["Once again we do our regular imports."]},{"metadata":{"id":"Mk4-H3VnNSBi","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","np.random.seed(1337)  # for reproducibility\n","from sklearn.cluster import KMeans\n","import torch \n","import torchvision\n","import torch.nn as nn\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-dE4fUykNSBm","colab_type":"text"},"cell_type":"markdown","source":["### Hyperparameters"]},{"metadata":{"id":"FKpyM4zzNSBm","colab_type":"code","colab":{}},"cell_type":"code","source":["num_epochs = 5\n","batch_size = 100\n","learning_rate = 0.001\n","use_reg = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0fWK7vxBMEW4","colab_type":"code","colab":{}},"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mJH3yYJKNSBp","colab_type":"text"},"cell_type":"markdown","source":["### Downloading the MNIST dataset"]},{"metadata":{"id":"3inDH-alNSBq","colab_type":"code","colab":{}},"cell_type":"code","source":["train_dataset = dsets.MNIST(root='../data/',\n","                            train=True, \n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='../data/',\n","                           train=False, \n","                           transform=transforms.ToTensor())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3cIqGefQNSBw","colab_type":"text"},"cell_type":"markdown","source":["### Dataloader"]},{"metadata":{"id":"_3Nxm0bXNSBw","colab_type":"code","colab":{}},"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MrX-dz4fNSB1","colab_type":"text"},"cell_type":"markdown","source":["### Define the network"]},{"metadata":{"id":"DYXjWlJsNSB2","colab_type":"code","colab":{}},"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU())\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        self.fc1 = nn.Linear(7*7*32, 300)\n","        self.fc2 = nn.Linear(300, 10)\n","        \n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ne2DrrXiNSB5","colab_type":"text"},"cell_type":"markdown","source":["<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> "]},{"metadata":{"id":"7ui3ChcWNSB6","colab_type":"code","colab":{}},"cell_type":"code","source":["def reset_model():\n","    net = Net()\n","    net = net.to(device)\n","\n","    # Loss and Optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","    return net,criterion,optimizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EfBSn8rJNSB8","colab_type":"text"},"cell_type":"markdown","source":["### Initializing the model"]},{"metadata":{"id":"7vLzLbbnNSB9","colab_type":"code","colab":{}},"cell_type":"code","source":["net, criterion, optimizer = reset_model()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZyXY7Oq6NSCB","colab_type":"text"},"cell_type":"markdown","source":["### Defining a L1 Regularizer"]},{"metadata":{"id":"zMLxnmDkNSCC","colab_type":"code","colab":{}},"cell_type":"code","source":["def l1_regularizer(net, loss, beta):\n","    l1_crit = nn.L1Loss(size_average=False)\n","    reg_loss = 0\n","    for param in net.parameters():\n","        target = (torch.FloatTensor(param.size()).zero_()).to(device)\n","        reg_loss += l1_crit(param, target)\n","        \n","    loss += beta * reg_loss\n","    return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3NogS9pZNSCF","colab_type":"text"},"cell_type":"markdown","source":["### Training function"]},{"metadata":{"id":"6hUX_GntNSCG","colab_type":"code","colab":{}},"cell_type":"code","source":["# Train the Model\n","\n","def training(net, reset = True):\n","    if reset == True:\n","        net, criterion, optimizer = reset_model()\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","    \n","    net.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        accuracy = []\n","        for i, (images, labels) in enumerate(train_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            temp_labels = labels\n","          \n","\n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","\n","            if use_reg == True :\n","                loss = l1_regularizer(net,loss,beta=0.001)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == temp_labels).sum().item()\n","            accuracy.append(correct/float(batch_size))\n","\n","        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n","    \n","    return net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_5fha93LNSCI","colab_type":"text"},"cell_type":"markdown","source":["### Testing function"]},{"metadata":{"id":"Kj1AAWuBNSCL","colab_type":"code","colab":{}},"cell_type":"code","source":["# Test the Model\n","def testing(net):\n","    net.eval() \n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","       \n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T4WRXM1YNSCP","colab_type":"text"},"cell_type":"markdown","source":["### Training and testing the network"]},{"metadata":{"id":"qSfX2-vFNSCP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"8782c808-52b1-4853-eb20-45ff1773e399","executionInfo":{"status":"ok","timestamp":1536228605793,"user_tz":-330,"elapsed":100662,"user":{"displayName":"Arun Kumar S","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"103047737441181235578"}}},"cell_type":"code","source":["reset = False\n","net = training(net, reset)\n","testing(net)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1, Loss: 545.4819, Accuracy: 0.9528\n","Epoch: 2, Loss: 239.1843, Accuracy: 0.9751\n","Epoch: 3, Loss: 194.6919, Accuracy: 0.9788\n","Epoch: 4, Loss: 172.9960, Accuracy: 0.9804\n","Epoch: 5, Loss: 161.7971, Accuracy: 0.9815\n","Test Accuracy of the network on the 10000 test images: 97.61 %\n"],"name":"stdout"}]},{"metadata":{"id":"b8kc_CbbNSCR","colab_type":"text"},"cell_type":"markdown","source":["### Uniform Quantization\n","\n","The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer representing the closest real number in a linear set of 256 within the range.\n","\n","In the function below we send 8 bits as input which ressembles that the weights of the network should be represented with only 8 bits while storing to disk. In other words we use only 2^8 or 256 clusters. Hence each weight is represented as a 8-bit integer between 0-255.\n","\n","Thus before using the weights during test time they need to be projected into the original weight space by using the following equation:\n","\n","$$\n","W_{i} = min + \\dfrac{max-min}{255}*W_{index}\n","$$"]},{"metadata":{"id":"bkD7PGOCNSCT","colab_type":"code","colab":{}},"cell_type":"code","source":["def uniform_quantize(weight, bits):\n","    print('-------------------------LAYER---------------------------')\n","    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weight))))\n","    n_clusters = 2**bits\n","    \n","    maxim = np.amax(weight)\n","    minim = np.amin(weight)\n","    step= (maxim-minim)/(n_clusters - 1)\n","\n","    clusters=[]\n","\n","    for i in range(0,n_clusters):\n","        clusters.append(minim)\n","        minim+=step\n","\n","    for i in range(0,len(weight)):\n","        dist= (clusters-weight[i])**2     \n","        weight[i]=clusters[np.argmin(dist)]\n","        \n","    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weight))))\n","    \n","    return weight  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"NWEIuI9wNSCU","colab_type":"text"},"cell_type":"markdown","source":["### Uniform Quantization\n","\n","Different number of bits can be used for representing the weights and biases. The exact number of bits to use is a design choice and may depend on the complexity of the task at hand since using too less number of bits can result in poor performance. Here, we use 8 bits for quantizing the weights and 1 bit for the biases."]},{"metadata":{"id":"JpJ1C75qNSCU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":833},"outputId":"4cede503-c67e-4d2b-9c6c-32c8143b1541","executionInfo":{"status":"ok","timestamp":1536228624690,"user_tz":-330,"elapsed":17091,"user":{"displayName":"Arun Kumar S","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"103047737441181235578"}}},"cell_type":"code","source":["for m in net.modules():\n","    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n","        temp_weight = m.weight.data.cpu().numpy()\n","        dims = temp_weight.shape\n","        temp_weight = temp_weight.flatten()\n","        temp_weight = uniform_quantize(temp_weight, 8)\n","        temp_weight=np.reshape(temp_weight,dims)\n","        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n","        \n","        temp_bias = m.bias.data.cpu().numpy()\n","        dims = temp_bias.shape\n","        temp_bias = temp_bias.flatten()\n","        temp_bias = uniform_quantize(temp_bias, 1)\n","        temp_bias = np.reshape(temp_bias,dims)\n","        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 400\n","Number of unique parameters after quantization: 132\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 15\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 2304\n","Number of unique parameters after quantization: 154\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 13\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 4608\n","Number of unique parameters after quantization: 126\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 13\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 468907\n","Number of unique parameters after quantization: 149\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 300\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 3000\n","Number of unique parameters after quantization: 104\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 10\n","Number of unique parameters after quantization: 2\n"],"name":"stdout"}]},{"metadata":{"id":"lQsNHoNyNSCW","colab_type":"text"},"cell_type":"markdown","source":["Now that we have replaced the weight matrix with the approximated weight of the nearest cluster, we can test the network with the modified weights."]},{"metadata":{"id":"3qYOc_42NSCX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8767052a-3618-4346-b3be-8fc30fd8f91e","executionInfo":{"status":"ok","timestamp":1536228627715,"user_tz":-330,"elapsed":2949,"user":{"displayName":"Arun Kumar S","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"103047737441181235578"}}},"cell_type":"code","source":["testing(net)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Test Accuracy of the network on the 10000 test images: 95.36 %\n"],"name":"stdout"}]},{"metadata":{"id":"-sCB4SNmNSCa","colab_type":"text"},"cell_type":"markdown","source":["## Non-uniform quantization\n","\n","We have seen in the previous method that we divide the weight space into equally partitioned cluster heads. However, instead of forcing the cluster heads to be equally spaced it would make more sense to learn them. A common and obvious practice is to learn the weight space as a distribution of cluseter centers using k-means clustering. Here, we define a function to perform k-means to the weight values.\n","\n","$$\n","min\\sum_{i}^{mn}\\sum_{j}^{k}||w_{i}-c_{j}||_{2}^{2}\n","$$"]},{"metadata":{"id":"GUDAcctdNSCb","colab_type":"code","colab":{}},"cell_type":"code","source":["num_clusters = 8\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0,  max_iter=500, precompute_distances='auto', verbose=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r2wWh5lbNSCe","colab_type":"code","colab":{}},"cell_type":"code","source":["def non_uniform_quantize(weights):\n","    print(\"---------------------------Layer--------------------------------\")\n","    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weights))))\n","    weights = np.reshape(weights,[weights.shape[0],1])\n","    print(weights.shape)\n","    kmeans_fit = kmeans.fit(weights)\n","    clusters = kmeans_fit.cluster_centers_\n","    \n","    for i in range(0,len(weights)):\n","        dist= (clusters-weights[i])**2     \n","        weights[i]=clusters[np.argmin(dist)]\n","        \n","    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weights))))\n","    \n","    return weights  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"luzXZINVNSCg","colab_type":"text"},"cell_type":"markdown","source":["We reset the model and train the network since we had earlier done uniform quantization on the weight already."]},{"metadata":{"id":"IYdxVUsvNSCj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"c646d7a7-428b-49a2-8557-7cdf4ee10919","executionInfo":{"status":"ok","timestamp":1536228731635,"user_tz":-330,"elapsed":100004,"user":{"displayName":"Arun Kumar S","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"103047737441181235578"}}},"cell_type":"code","source":["reset = True\n","net = training(net, reset)\n","testing(net)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1, Loss: 567.1708, Accuracy: 0.9527\n","Epoch: 2, Loss: 241.0311, Accuracy: 0.9772\n","Epoch: 3, Loss: 196.0067, Accuracy: 0.9799\n","Epoch: 4, Loss: 173.0234, Accuracy: 0.9812\n","Epoch: 5, Loss: 159.6872, Accuracy: 0.9827\n","Test Accuracy of the network on the 10000 test images: 97.88 %\n"],"name":"stdout"}]},{"metadata":{"id":"WXhYLriUNSCm","colab_type":"text"},"cell_type":"markdown","source":["Uniform quantization on the weights and biases"]},{"metadata":{"id":"ygTmNfWsNSCm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1105},"outputId":"c09861f0-daa4-4cb5-ae1d-23ea04c0348d","executionInfo":{"status":"ok","timestamp":1536228740691,"user_tz":-330,"elapsed":8998,"user":{"displayName":"Arun Kumar S","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"103047737441181235578"}}},"cell_type":"code","source":["for m in net.modules():\n","    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n","        temp_weight = m.weight.data.cpu().numpy()\n","        dims = temp_weight.shape\n","        temp_weight = temp_weight.flatten()\n","        temp_weight = non_uniform_quantize(temp_weight)\n","        temp_weight=np.reshape(temp_weight,dims)\n","        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n","        \n","        temp_bias = m.bias.data.cpu().numpy()\n","        dims = temp_bias.shape\n","        temp_bias = temp_bias.flatten()\n","        temp_bias = non_uniform_quantize(temp_bias)\n","        temp_bias = np.reshape(temp_bias,dims)\n","        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"],"execution_count":19,"outputs":[{"output_type":"stream","text":["---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 400\n","(400, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 2304\n","(2304, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 4608\n","(4608, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 468960\n","(470400, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 300\n","(300, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 3000\n","(3000, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 10\n","(10, 1)\n","Number of unique parameters after quantization: 8\n"],"name":"stdout"}]},{"metadata":{"id":"XAZkRYg9NSCp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"403cda39-f5d7-49ed-c120-da732931c6dd","executionInfo":{"status":"ok","timestamp":1536228743920,"user_tz":-330,"elapsed":3151,"user":{"displayName":"Arun Kumar S","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"103047737441181235578"}}},"cell_type":"code","source":["testing(net)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Test Accuracy of the network on the 10000 test images: 97.71 %\n"],"name":"stdout"}]},{"metadata":{"id":"AZME62IFNSCr","colab_type":"text"},"cell_type":"markdown","source":["### Retraining the network\n","\n","Here we see that 8 clusters are too less in order to maintain the network at the same accuracy since we see almost a 3% drop in performance. One of the solutions is to retrain the network. This helps the other weights to compensate for those weights which on being rounded off to the nearest cluster center have resulted in a drop in performance. Accuracy can be recovered significantly on retraining the network and then non-uniformly quantizing the weights again.\n","\n","#### Excercise"]},{"metadata":{"id":"hDGjonRNNSCt","colab_type":"code","colab":{}},"cell_type":"code","source":["# reset = False\n","# net = training(net, reset)\n","# perform non-uniform quantization\n","# test(net)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A90EeUjONSCv","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"wNL2AuOeNSCx","colab_type":"text"},"cell_type":"markdown","source":["### References\n","\n","1. https://arxiv.org/pdf/1412.6115.pdf"]},{"metadata":{"id":"CPcT0jDuNSCx","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}