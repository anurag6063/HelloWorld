{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nha6plg2TnmQ"
   },
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint\n",
    "#### To be done in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6WV8mIlL8o4"
   },
   "source": [
    "The objective of this experiment is to understand regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2w6zsNW-TnmR"
   },
   "source": [
    "<img src=\"reg1.png\">\n",
    "\n",
    "While our loss function allows us to determine how well (or poorly) our set of parameters (i.e., weight matrix, and bias vector) are performing on a given classification task, the loss function itself does not take into account how the weight matrix “looks”. This brings us to the following questions.  How do we go about choosing a set of parameters that will help ensure our model generalizes well? Or at the very least, lessen the affects of overfitting?\n",
    "\n",
    "The answer is <b>regularization.</b>\n",
    "\n",
    "This notebook is divided into two parts. In the first part we will observe the behaviour of the weight space after adding a regularization term while in the second part, we will use some property of the learned weight space to remove redundant connections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXg62E_pL8o-"
   },
   "source": [
    "##### Keywords\n",
    "\n",
    "* L1 Regularization\n",
    "* Pruning\n",
    "* Batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaHGjdG5L8o_"
   },
   "source": [
    "##### Expected time to complete the experiment is : 90min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COSQ21_9L8o_"
   },
   "source": [
    "Let us begin by our regular import statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46581,
     "status": "ok",
     "timestamp": 1536226743958,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "RKNPMTcvPsbk",
    "outputId": "001830e1-f5dc-4a02-e89f-36f98ce7ee53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (5.1.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.14.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-win_amd64.whl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhxAfWj7TnmS"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-93a52dc60448>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdsets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsun\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSUN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSUNClass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetFolder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCocoCaptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcifar\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCIFAR100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstl10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\lsun.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PILLOW_VERSION'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         raise ImportError(\"The _imaging extension was built for another \"\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8A70o515TnmV"
   },
   "source": [
    "### Hyperparameters to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGlfUnYqTnmV"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "use_reg = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYUS_CLxTnmY"
   },
   "source": [
    "### Downloading the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3829,
     "status": "ok",
     "timestamp": 1536226819020,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "dp8uPwvTTnmY",
    "outputId": "400ada59-800c-4aa9-970b-44bba40353f1"
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='../data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../data',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNnphTJsTnmc"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKzALRgfTnmd"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxFubQ6OTnme"
   },
   "source": [
    "### Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dULykqibTnmf"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc1 = nn.Linear(7*7*32, 300)\n",
    "        self.fc2 = nn.Linear(300, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfLjHmz7Tnmh"
   },
   "source": [
    "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-khA9HNuTnmh"
   },
   "outputs": [],
   "source": [
    "def reset_model():\n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    return net,criterion,optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4d3zIBxTnmk"
   },
   "source": [
    "### L1 Regularization\n",
    "\n",
    "Here, we define a L1 Regularizer and add it to our loss function. The L1 Regularization term basically adds a penalty, equivalent to the absolute value of the magnitude of the weights. This ensures that the magnitude of the weights do not become too high.\n",
    "\n",
    "We have seen in the previous lectures that adding the L1 regularizer ensures sparsity. This is important becuase our ultimate aim is to prune connections from our network. That would mean we should try to make as many weights extremely close to 0 as possible. Thus, adding this penalty term ensures sparsity. The L1 Regularizer is also known as Lass Regression and it can be defined as follows:\n",
    "\n",
    "$$P=\\lambda*\\sum_{i=1}^{n}|\\theta_{i}-0|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN5np0bMTnml"
   },
   "outputs": [],
   "source": [
    "def l1_regularizer(net, loss, beta):\n",
    "    l1_crit = nn.L1Loss(size_average=False)\n",
    "    reg_loss = 0\n",
    "    for param in net.parameters():\n",
    "        target = (torch.FloatTensor(param.size()).zero_()).to(device)\n",
    "        reg_loss += l1_crit(param, target)\n",
    "        \n",
    "    loss += beta * reg_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zXI813PTnmn"
   },
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puVKJQjnTnmo"
   },
   "outputs": [],
   "source": [
    "net, criterion, optimizer = reset_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6D4awmdTnmr"
   },
   "source": [
    "### Defining the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqu5iRI4Tnmr"
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "def training(net, reset = True):\n",
    "    if reset == True:\n",
    "        net, criterion, optimizer = reset_model()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        accuracy = []\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            temp_labels = labels\n",
    "            \n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if use_reg == True :\n",
    "                loss = l1_regularizer(net,loss,beta=0.001)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == temp_labels).sum().item()\n",
    "            accuracy.append(correct/float(batch_size))\n",
    "\n",
    "        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAPJXTA4Tnmt"
   },
   "source": [
    "### Defining the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1M-YBR80Tnmu"
   },
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "def testing(net):\n",
    "    net.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HRbNb4NTnmx"
   },
   "source": [
    "### Training and testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71989,
     "status": "ok",
     "timestamp": 1536227089609,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "YSbJvxVXTnmz",
    "outputId": "52a5d920-c174-4235-81b2-9513ca2ce1c9"
   },
   "outputs": [],
   "source": [
    "reset = True\n",
    "net = training(net, reset)\n",
    "testing(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lPLTqUOTnm3"
   },
   "source": [
    "### Function for plotting the weight distribution\n",
    "\n",
    "We want to plot the weights learnt during training and we define the following function to do so. It is important to note that in order ro achieve smoothness in the behaviour of the plot we use 256 clusters here where the nearest weight values are rounded to. Feel free to experiment with defining lesser number of clusters for the weight distribution approximation. However, the plot may not be as smooth. \n",
    "\n",
    "We plot the cluster values in the x-axis and the frequency of the weights in the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6p7eVeOGTnm3"
   },
   "outputs": [],
   "source": [
    "def weightdistribution(weights):\n",
    "    maxim= np.amax(weights)\n",
    "    print(\"Maximum value of learnt weights: \" + str(maxim))\n",
    "    \n",
    "    minim= np.amin(weights)\n",
    "    print(\"Minimum value of learnt weights: \" + str(minim))\n",
    "    \n",
    "    step= (maxim-minim)/255\n",
    "    freq= np.zeros(256)\n",
    "    steps=[]\n",
    "\n",
    "    for i in range(0,256):\n",
    "        steps.append(minim)\n",
    "        minim+=step\n",
    "    \n",
    "    m = weights.shape[0]\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        e= weights[i]\n",
    "        dist= (steps-e)**2\n",
    "        freq[np.argmin(dist)]+=1\n",
    "    \n",
    "        \n",
    "    plt.plot(steps,list(freq))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16115,
     "status": "ok",
     "timestamp": 1536227334728,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "TxY_VCALTnm6",
    "outputId": "f34d9814-95ba-46ac-be45-b8e36cdda61a"
   },
   "outputs": [],
   "source": [
    "weightdistribution(net.state_dict()['layer2.0.weight'].cpu().numpy().flatten())\n",
    "weightdistribution(net.state_dict()['fc1.weight'].cpu().numpy().flatten())\n",
    "weightdistribution(net.state_dict()['fc2.weight'].cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzseaQfLTnm8"
   },
   "source": [
    "### Defining the pruning function\n",
    "\n",
    "We use a threshold based criteria to remove certain connections in the networks which fall within a threshold value. Hence, we define a function which takes the network as input and a vlaue for threshold and forces those connections to be 0 which fall with the threshold given by the following equation.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " W_{i} = \n",
    " \\begin{cases} \n",
    "      W_{i} & W_{i} < -\\theta \\\\\n",
    "      0 & -\\theta\\leq W_{i}\\leq \\theta \\\\\n",
    "      W_{i} & \\theta < W_{i} \n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We prune both the weights and biases of Convolution layers, Fully Connected layers and the Batch Normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgW4yUPBTnm8"
   },
   "outputs": [],
   "source": [
    "def prune_weight(net, threshold):\n",
    "    \n",
    "    for m in net.modules():\n",
    "        if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n",
    "\n",
    "            temp_weight = m.weight.data.cpu().numpy()\n",
    "            dims = temp_weight.shape\n",
    "            print('WEIGHT ',dims)\n",
    "            print(\"Before pruning------->\")\n",
    "            print(np.count_nonzero(temp_weight))\n",
    "\n",
    "            temp_weight = temp_weight.flatten()\n",
    "\n",
    "            [x1,x2]=((np.where(np.all([[(-1*threshold) < (temp_weight)] , [(temp_weight) < threshold]],axis=0))))\n",
    "            temp_weight[x2] = 0\n",
    "            temp_weight = np.reshape(temp_weight,dims)\n",
    "            print(\"After pruning------->\")\n",
    "            print(np.count_nonzero(temp_weight))\n",
    "            print('-------------------------------------------------------------------------------')\n",
    "            m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n",
    "\n",
    "            temp_bias = m.bias.data.cpu().numpy()\n",
    "            dims = temp_bias.shape\n",
    "            print('BIAS ',dims)\n",
    "            print(\"Before pruning------->\")\n",
    "            print(np.count_nonzero(temp_bias))\n",
    "\n",
    "            temp_bias = temp_bias.flatten()\n",
    "\n",
    "            [x1,x2]=((np.where(np.all([[(-1*threshold) < (temp_bias)] , [(temp_bias) < threshold]],axis=0))))\n",
    "            temp_bias[x2] = 0\n",
    "            temp_bias = np.reshape(temp_bias,dims)\n",
    "            print(\"After pruning------->\")\n",
    "            print(np.count_nonzero(temp_bias))\n",
    "            print('-------------------------------------------------------------------------------')\n",
    "            m.bias.data = (torch.FloatTensor(temp_bias).cuda())\n",
    "\n",
    "# for m in net.modules():\n",
    "#     if isinstance(m,nn.Conv2d):\n",
    "#         print m.bias.data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SNHjrWWTnm-"
   },
   "source": [
    "Here, we check how many connetions have been pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1649
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1536227348958,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "zSqWfhNuTnnA",
    "outputId": "dcbeb676-0ab9-4ead-f5ba-6fd1d82e11c2"
   },
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "prune_weight(net,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnT1VyvOTnnD"
   },
   "source": [
    "Training the network again using the regularizer. The loss is modified as \n",
    "\n",
    "$$\n",
    "loss = loss + regularized_loss\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 97026,
     "status": "ok",
     "timestamp": 1536227454433,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "uCT6MKZITnnD",
    "outputId": "ee11ce32-36bc-48f2-84ce-91150b08bd31"
   },
   "outputs": [],
   "source": [
    "reset = True\n",
    "use_reg = True\n",
    "net = training(net, reset)\n",
    "testing(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B2Ica71fTnnH"
   },
   "source": [
    "<b>Visualizing the weight distributions with the regularized loss</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17507,
     "status": "ok",
     "timestamp": 1536227642781,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "N8RVoDFgTnnH",
    "outputId": "5438f860-80ce-4b77-dbff-6f72adbbb980"
   },
   "outputs": [],
   "source": [
    "weightdistribution(net.state_dict()['layer2.0.weight'].cpu().numpy().flatten())\n",
    "weightdistribution(net.state_dict()['fc1.weight'].cpu().numpy().flatten())\n",
    "weightdistribution(net.state_dict()['fc2.weight'].cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lx0ZAH8jTnnJ"
   },
   "source": [
    "By using the L1 Regularizer, we see that we have forced the network to learn weights within a constrained subspace. Since more number of weights are closer to 0, it is evident that we will be able to make more weights 0 by using the same threshold as before. This, therefore ensures sparsity in every layer of the network.\n",
    "\n",
    "An interesting observation is that the netowrk tells us that it does not require biases for certain layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1649
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3363,
     "status": "ok",
     "timestamp": 1536227646287,
     "user": {
      "displayName": "Arun Kumar S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "103047737441181235578"
     },
     "user_tz": -330
    },
    "id": "S_w7Cpr8TnnK",
    "outputId": "1139d813-df20-4ada-d984-df9b91117cfa"
   },
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "prune_weight(net,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xP6XK9hqTnnM"
   },
   "source": [
    "### References\n",
    "\n",
    "1. http://www.pyimagesearch.com/2016/09/19/understanding-regularization-for-image-classification-and-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCmqGnUSTnnM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AIML_MCP_LAB_JN01_EX_E.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
