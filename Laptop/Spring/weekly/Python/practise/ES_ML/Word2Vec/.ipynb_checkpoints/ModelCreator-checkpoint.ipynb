{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Word2Vec model\n",
    "## Objective: Load given file remove stopwords, create/update a model and save as binary file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the required packages.\n",
    "2. create a class: vocabulary and files to be parsed.\n",
    "3. Create iterator that can read each line and removes stopwords.\n",
    "   Use stemmer to stem english words.\n",
    "   The length of the word should be b/w 2 to 15.\n",
    "   All the words in the file 'stopwords.txt' will be removed.   \n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contails word2vec model\n",
    "import gensim\n",
    "\n",
    "# help in reading files from system\n",
    "import os\n",
    "\n",
    "# regular expression to get words from line.\n",
    "import re\n",
    "\n",
    "# natural language processor\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# pandas to load file\n",
    "import pandas as pd\n",
    "\n",
    "# to get best feature representation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# to get list of files from command line\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser =argparse.ArgumentParser(description='Pass list of file names to be converted in word2Vec')\n",
    "parser =argparse.ArgumentParser(description='Pass list of file names to be converted in word2Vec')\n",
    "\n",
    "parser.add_argument(\"-l\", \"--list\", nargs='+', type=str, dest = 'file', help='Enter list of files')\n",
    "args = parser.parse_args()\n",
    "\n",
    "fileList = args.file\n",
    "files = fileList[0]\n",
    "inputFiles = files.split(' ')\n",
    "print(files.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a english stemmer to convert words into their original form\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# load file which contains words that should not be a part of word2vec model\n",
    "removeWords = pd.read_csv('stopwords.txt').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create class to capture file and its text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsOfFile(object):\n",
    "    def __init__(self, fileNames):\n",
    "        self.fileNames = fileNames\n",
    "        self.vocabulary = set([])        \n",
    "        print('Files being read are: ', fileNames)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for fileName in self.fileNames:\n",
    "            print('Processing file: ', fileName)\n",
    "            \n",
    "            # reading each line of the file\n",
    "            for line in open(fileName, encoding='latin1'):\n",
    "                # get list of all words of length b/w 2 to 15\n",
    "                words = re.findall(r'(\\b[A-Za-z][a-z]{2,15}\\b)', line)\n",
    "                #skip all removeable words. reamining words should be stemmed.\n",
    "                words = [stemmer.stem(word.lower()) for word in words if not word.lower() in removeWords]\n",
    "                # add all the words in set. using generator as its more memory efficient\n",
    "                for word in words:\n",
    "                    self.vocabulary.add(word)\n",
    "                    \n",
    "                # yield the vocabulary. this will let you iterate over list of words just once but in every efficient way.\n",
    "                yield words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file and pass it to the WordsOfFile class. \n",
    "\n",
    "    This will initialize vocabulary and run iterator on it.\n",
    "    We can pass multiple files too. keep the size small of you want to see the model generated after each file. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files being read are:  ['EsContent.txt']\n"
     ]
    }
   ],
   "source": [
    "wordsOfFile = WordsOfFile(inputFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send this words in file to gensim. It will create a vector represntation of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  EsContent.txt\n",
      "Processing file:  EsContent.txt\n",
      "Processing file:  EsContent.txt\n",
      "Processing file:  EsContent.txt\n",
      "Processing file:  EsContent.txt\n",
      "Processing file:  EsContent.txt\n"
     ]
    }
   ],
   "source": [
    "word2VecModel = gensim.models.FastText(wordsOfFile, min_count=100, size=5, workers=4, min_alpha=2.0, max_n=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to the disk in binary format.\n",
    "word2VecModel.save('word2VecModel.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
